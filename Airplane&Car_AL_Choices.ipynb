{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MWwUTInv-oj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, \\\n",
    "    GradientBoostingClassifier\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "import glob\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import glob\n",
    "import platform\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2coy8C7xGqS"
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SvmModel(BaseModel):\n",
    "\n",
    "    model_type = 'Support Vector Machine with linear Kernel'\n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training svm...')\n",
    "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
    "                              class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "\n",
    "class LogModel(BaseModel):\n",
    "\n",
    "    model_type = 'Multinominal Logistic Regression' \n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training multinomial logistic regression')\n",
    "        train_samples = X_train.shape[0]\n",
    "        self.classifier = LogisticRegression(\n",
    "            C=50. / train_samples,\n",
    "            multi_class='multinomial',\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            tol=0.1,\n",
    "            class_weight=c_weight,\n",
    "            )\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
    "                self.test_y_predicted)\n",
    "\n",
    "class RfModel(BaseModel):\n",
    "\n",
    "    model_type = 'Random Forest'\n",
    "    \n",
    "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('training random forest...')\n",
    "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "        self.test_y_predicted = self.classifier.predict(X_test)\n",
    "        self.val_y_predicted = self.classifier.predict(X_val)\n",
    "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEcquMhTzqlC"
   },
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model_object):        \n",
    "        self.accuracies = []\n",
    "        self.model_object = model_object()        \n",
    "\n",
    "    def print_model_type(self):\n",
    "        print (self.model_object.model_type)\n",
    "\n",
    "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
    "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
    "        print ('Val   set:', X_val.shape)\n",
    "        print ('Test  set:', X_test.shape)\n",
    "        t0 = time.time()\n",
    "        (X_train, X_val, X_test, self.val_y_predicted,\n",
    "         self.test_y_predicted) = \\\n",
    "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
    "        self.run_time = time.time() - t0\n",
    "        return (X_train, X_val, X_test)  \n",
    "    # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
    "\n",
    "    # we want accuracy only for the test set\n",
    "\n",
    "    def get_test_accuracy(self, i, y_test):\n",
    "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
    "        self.accuracies.append(classif_rate)               \n",
    "        print('--------------------------------')\n",
    "        print('Iteration:',i)\n",
    "        print('--------------------------------')\n",
    "        print('y-test set:',y_test.shape)\n",
    "        print('Example run in %.3f s' % self.run_time,'\\n')\n",
    "        print(\"Accuracy rate for %f \" % (classif_rate))    \n",
    "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
    "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
    "        print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ni-E8tN7yYX2"
   },
   "outputs": [],
   "source": [
    "class BaseSelectionFunction(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        random_state = check_random_state(0)\n",
    "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++1')\n",
    "#     print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
    "\n",
    "        return selection\n",
    "\n",
    "\n",
    "\n",
    "class EntropySelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
    "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        print(str(np.argsort(e)[::-1]))\n",
    "        return selection\n",
    "      \n",
    "      \n",
    "class MarginSamplingSelection(BaseSelectionFunction):\n",
    "\n",
    "    @staticmethod\n",
    "    def select(probas_val, initial_labeled_samples):\n",
    "        print('++++++++++++++++++++++++++++++++++++++++++++++2')\n",
    "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
    "        values = rev[:, 0] - rev[:, 1]\n",
    "        selection = np.argsort(values)[:initial_labeled_samples]\n",
    "        return selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aRcKj4D0SQJ"
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \n",
    "    def normalize(self, X_train, X_val, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "        return (X_train, X_val, X_test) \n",
    "    \n",
    "    def inverse(self, X_train, X_val, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_val   = self.scaler.inverse_transform(X_val)\n",
    "        X_test  = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_val, X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1GJB6Kb0e54"
   },
   "outputs": [],
   "source": [
    "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
    "                         y_train_full):\n",
    "    random_state = check_random_state(0)\n",
    "    permutation = np.random.choice(trainset_size,\n",
    "                                   initial_labeled_samples,\n",
    "                                   replace=False)\n",
    "    print ()\n",
    "    print ('initial random chosen samples', permutation.shape),\n",
    "#            permutation)\n",
    "    X_train = X_train_full[permutation]\n",
    "    y_train = y_train_full[permutation]\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    bin_count = np.bincount(y_train.astype('int64'))\n",
    "    unique = np.unique(y_train.astype('int64'))\n",
    "    print (\n",
    "        'initial train set:',\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        'unique(labels):',\n",
    "        bin_count,\n",
    "        unique,\n",
    "        )\n",
    "    return (permutation, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRtOTPPvyJGx"
   },
   "outputs": [],
   "source": [
    "class TheAlgorithm(object):\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
    "        self.initial_labeled_samples = initial_labeled_samples\n",
    "        self.model_object = model_object\n",
    "        self.sample_selection_function = selection_function\n",
    "\n",
    "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
    "        \n",
    "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
    "\n",
    "        (permutation, X_train, y_train) = \\\n",
    "            get_k_random_samples(self.initial_labeled_samples,\n",
    "                                 X_train_full, y_train_full)\n",
    "        self.queried = self.initial_labeled_samples\n",
    "        self.samplecount = [self.initial_labeled_samples]\n",
    "\n",
    "        # permutation, X_train, y_train = get_equally_k_random_samples(self.initial_labeled_samples,classes)\n",
    "\n",
    "        # assign the val set the rest of the 'unlabelled' training data\n",
    "\n",
    "        X_val = np.array([])\n",
    "        y_val = np.array([])\n",
    "        X_val = np.copy(X_train_full)\n",
    "        X_val = np.delete(X_val, permutation, axis=0)\n",
    "        y_val = np.copy(y_train_full)\n",
    "        y_val = np.delete(y_val, permutation, axis=0)\n",
    "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
    "        print ()\n",
    "\n",
    "        # normalize data\n",
    "\n",
    "        normalizer = Normalize()\n",
    "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
    "        \n",
    "        self.clf_model = TrainModel(self.model_object)\n",
    "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "        active_iteration = 1\n",
    "        self.clf_model.get_test_accuracy(1, y_test)\n",
    "\n",
    "        # fpfn = self.clf_model.test_y_predicted.ravel() != y_val.ravel()\n",
    "        # print(fpfn)\n",
    "        # self.fpfncount = []\n",
    "        # self.fpfncount.append(fpfn.sum() / y_test.shape[0] * 100)\n",
    "\n",
    "        while self.queried < max_queried:\n",
    "\n",
    "            active_iteration += 1\n",
    "\n",
    "            # get validation probabilities\n",
    "\n",
    "            probas_val = \\\n",
    "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
    "            print ('val predicted:',\n",
    "                   self.clf_model.val_y_predicted.shape,\n",
    "                   self.clf_model.val_y_predicted)\n",
    "            print ('probabilities:', probas_val.shape, '\\n',\n",
    "                   np.argmax(probas_val, axis=1))\n",
    "\n",
    "            # select samples using a selection function\n",
    "\n",
    "            uncertain_samples = \\\n",
    "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
    "\n",
    "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
    " \n",
    "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
    "\n",
    "            # get the uncertain samples from the validation set\n",
    "\n",
    "            print ('trainset before', X_train.shape, y_train.shape)\n",
    "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
    "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
    "            print ('trainset after', X_train.shape, y_train.shape)\n",
    "            self.samplecount.append(X_train.shape[0])\n",
    "\n",
    "            bin_count = np.bincount(y_train.astype('int64'))\n",
    "            unique = np.unique(y_train.astype('int64'))\n",
    "            print (\n",
    "                'updated train set:',\n",
    "                X_train.shape,\n",
    "                y_train.shape,\n",
    "                'unique(labels):',\n",
    "                bin_count,\n",
    "                unique,\n",
    "                )\n",
    "\n",
    "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
    "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
    "            print ('val set:', X_val.shape, y_val.shape)\n",
    "            print ()\n",
    "\n",
    "            # normalize again after creating the 'new' train/test sets\n",
    "            normalizer = Normalize()\n",
    "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
    "\n",
    "            self.queried += self.initial_labeled_samples\n",
    "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
    "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
    "\n",
    "        print ('final active learning accuracies',\n",
    "               self.clf_model.accuracies)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areas(lengths,choice_str,model):\n",
    "    sum = 0.\n",
    "    i = 0\n",
    "    k=10\n",
    "    #print(lengths,ks)\n",
    "    for length in lengths:\n",
    "        lengthhere = lengths[i]\n",
    "        if i == 0:\n",
    "            sum = sum + lengthhere*k/2\n",
    "        else:\n",
    "            sum = sum + (lengthhere+lengthhere)*k/2\n",
    "        i += 1\n",
    "    #print(\"The whole area is : 50,000\")\n",
    "    print(\"Airplane & car \"+choice_str+\" \"+model+\": The area below this line is: %.2f\"%sum)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdataset(choice):\n",
    "    if os.path.exists('bottleneck_features_test.npy') and choice == \"Pretrained\":\n",
    "        choice ='Pretrained_repeat'\n",
    "    print(\"Geting data now\")\n",
    "    print(\"The data you chose is \"+choice)\n",
    "    classifications=['airplane','car']\n",
    "    train_data_dir = 'rawdata/train/'\n",
    "    validation_data_dir = 'rawdata/test/'\n",
    "    \n",
    "    nb_train_samples = trainset_size\n",
    "    nb_validation_samples = testset_size\n",
    "\n",
    "    df_train = []\n",
    "    y_train_full=[]\n",
    "    images = os.listdir(train_data_dir)\n",
    "    for classification in classifications:\n",
    "        imagePaths = [f for f in glob.glob(train_data_dir+classification+'/'+'*.jpg')]\n",
    "        #here will take one min\n",
    "        for img in imagePaths:\n",
    "            img_data = load_img(img ,target_size=(150, 150))\n",
    "            img_data_array = img_to_array(img_data)\n",
    "            #print(img_data_array.shape)#img_data_array is (150,150,3)\n",
    "            if choice == 'Raw':\n",
    "                #will get (150*150*3)\n",
    "                img_data_flat = img_data_array.reshape(img_data_array.shape[0]*img_data_array.shape[1]*img_data_array.shape[2])\n",
    "                #print(img_data_flat.shape)\n",
    "            if choice == 'HOG' :\n",
    "                #will get (150,150)\n",
    "                img_data_flat=(img_data_array[:, :, 0] * 0.2989 + img_data_array[:, :, 1] * 0.5870 + img_data_array[:, :, 2] * 0.1140)/255.0\n",
    "                #print(img_data_flat.shape)\n",
    "            if choice == 'Pretrained' or choice == 'Pretrained_repeat':\n",
    "                #will get is (150,150,3)\n",
    "                img_data_flat= img_data_array\n",
    "                #print(img_data_flat.shape)\n",
    "            df_train.append(img_data_flat)\n",
    "            #print(classification)\n",
    "            if classification == \"airplane\":\n",
    "                y_train_full.append(0)\n",
    "            if classification == \"car\":\n",
    "                y_train_full.append(1)\n",
    "    #print(y_train_full)\n",
    "            \n",
    "\n",
    "    df_val = []\n",
    "    y_test = []\n",
    "    images = os.listdir(validation_data_dir)\n",
    "    for classification in classifications:\n",
    "        imagePaths = [f for f in glob.glob(validation_data_dir+classification+'/'+'*.jpg')]\n",
    "        for img in imagePaths:\n",
    "            img_data = load_img(img ,target_size=(150, 150))\n",
    "            img_data_array = img_to_array(img_data)\n",
    "            if choice == 'Raw':\n",
    "                #will get (150*150*3)\n",
    "                img_data_flat = img_data_array.reshape(img_data_array.shape[0]*img_data_array.shape[1]*img_data_array.shape[2])\n",
    "                #print(img_data_flat.shape)\n",
    "            if choice == 'HOG' :\n",
    "                #will get (150,150)\n",
    "                img_data_flat=(img_data_array[:, :, 0] * 0.2989 + img_data_array[:, :, 1] * 0.5870 + img_data_array[:, :, 2] * 0.1140)/255.0\n",
    "                #print(img_data_flat.shape)\n",
    "            if choice == 'Pretrained' or choice == 'Pretrained_repeat':\n",
    "                #will get is (150,150,3)\n",
    "                img_data_flat= img_data_array\n",
    "                #print(img_data_flat.shape)\n",
    "            df_val.append(img_data_flat)\n",
    "            if classification == \"airplane\":\n",
    "                    y_test.append(0)\n",
    "            if classification == \"car\":\n",
    "                    y_test.append(1)\n",
    "            #print(y_train_full)\n",
    "\n",
    "    #This will run for 10second\n",
    "    X_train_full = np.concatenate([arr[np.newaxis] for arr in df_train]).astype('float32')\n",
    "    X_test = np.concatenate([arr[np.newaxis] for arr in df_val]).astype('float32')\n",
    "\n",
    "    #将list 转成np，否则会有报错\n",
    "    y_train_full=np.array(y_train_full)\n",
    "    y_test=np.array(y_test)\n",
    "\n",
    "    if choice =='HOG':\n",
    "        X_test_new=[]\n",
    "        X_train_new=[]\n",
    "        for num in range(0,trainset_size):\n",
    "            fd = hog(X_train_full[num], orientations=12,block_norm='L1', pixels_per_cell=[8, 8], cells_per_block=[4, 4], visualize=False,transform_sqrt=False\n",
    "                    )\n",
    "            X_train_new.append(fd)\n",
    "\n",
    "        for num in range(0,testset_size):\n",
    "            fd = hog(X_test[num], orientations=12,block_norm='L1', pixels_per_cell=[8, 8], cells_per_block=[4, 4], visualize=False,\n",
    "                             transform_sqrt=True)\n",
    "            X_test_new.append(fd)\n",
    "\n",
    "        X_train_full=np.array(X_train_new)\n",
    "        X_test=np.array(X_test_new)\n",
    "\n",
    "    if choice == 'Pretrained':\n",
    "        model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "        last_layer = Model(inputs=model.input, outputs=model.get_layer('block5_pool').output)\n",
    "\n",
    "        start = time.clock()\n",
    "        print(\"Process data though the pretrained mdoel(train set), it will last for about 8 mins\")\n",
    "        features = last_layer.predict(X_train_full)\n",
    "        elapsed = (time.clock() - start)\n",
    "        print(\"(should about 8 mins)Time used:\",elapsed)\n",
    "\n",
    "        start = time.clock()\n",
    "        print(\"Process data though the pretrained mdoel(test set), it will last for about 2 mins\")\n",
    "        features_test = last_layer.predict(X_test)\n",
    "        elapsed = (time.clock() - start)\n",
    "        print(\"(should about 100second)Time used:\",elapsed)\n",
    "\n",
    "        X_train_full = features.reshape(trainset_size,-1)\n",
    "        X_test=features_test.reshape(testset_size,-1)\n",
    "\n",
    "        np.save('bottleneck_features_train.npy',X_train_full)\n",
    "        np.save('bottleneck_features_test.npy',X_test)\n",
    "\n",
    "    if choice == 'Pretrained' or choice == 'Pretrained_repeat':\n",
    "        X_train_full=np.load('bottleneck_features_train.npy')\n",
    "        X_test=np.load('bottleneck_features_test.npy')\n",
    "    #print(y_train_full)\n",
    "        \n",
    "        \n",
    "    print(\"Got all data now, here is the shape of the data\")\n",
    "    print(\"X_train_full shape: \"+str(X_train_full.shape))\n",
    "    print(\"y_train_full shape: \"+str(y_train_full.shape))\n",
    "    print(\"X_test shape: \"+str(X_test.shape))\n",
    "    print(\"y_test shape: \"+str(y_test.shape))\n",
    "        \n",
    "    return (X_train_full,y_train_full,X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'500''1294'\n",
    "max_queried = 500\n",
    "Ks = [10]\n",
    "\n",
    "#True means will save, False means will not save \n",
    "save_file=True\n",
    "\n",
    "#'Raw''HOG''Pretrained'\n",
    "choices=['HOG']\n",
    "\n",
    "#'RfModel''LogModel''SvmModel'\n",
    "models = [LogModel] \n",
    "\n",
    "#'RandomSelection''MarginSamplingSelection''EntropySelection'\n",
    "selection_functions = [RandomSelection, MarginSamplingSelection,EntropySelection] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_size = 1295\n",
    "testset_size = 400\n",
    "\n",
    "choices_str=choices\n",
    "\n",
    "Ks_str = ['10'] \n",
    "\n",
    "models_str=[]\n",
    "for model in models:\n",
    "    models_str.append( model.__name__) \n",
    "\n",
    "selection_functions_str=[]\n",
    "for selection_function in selection_functions:\n",
    "    selection_functions_str.append( selection_function.__name__) \n",
    "\n",
    "stopped_at = -1 \n",
    "#the data should be put in two dir rowdata/train/ and rowdata/test/\n",
    "#data should be named as A.num.jpg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping at: 500\n",
      "Geting data now\n",
      "The data you chose is HOG\n",
      "Got all data now, here is the shape of the data\n",
      "X_train_full shape: (1295, 43200)\n",
      "y_train_full shape: (1295,)\n",
      "X_test shape: (400, 43200)\n",
      "y_test shape: (400,)\n",
      "Count = 1, using model = LogModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 43200) (10,) unique(labels): [3 7] [0 1]\n",
      "val set: (1285, 43200) (1285,) (10,)\n",
      "\n",
      "Train set: (10, 43200) y: (10,)\n",
      "Val   set: (1285, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.150 s \n",
      "\n",
      "Accuracy rate for 76.750000 \n",
      "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.81       200\n",
      "           1       0.94      0.57      0.71       200\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       400\n",
      "   macro avg       0.82      0.77      0.76       400\n",
      "weighted avg       0.82      0.77      0.76       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [ 86 114]]\n",
      "--------------------------------\n",
      "val predicted: (1285,) [0 0 0 ... 1 1 0]\n",
      "probabilities: (1285, 2) \n",
      " [0 0 0 ... 1 1 0]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (10, 43200) (10,)\n",
      "trainset after (20, 43200) (20,)\n",
      "updated train set: (20, 43200) (20,) unique(labels): [ 7 13] [0 1]\n",
      "val set: (1275, 43200) (1275,)\n",
      "\n",
      "Train set: (20, 43200) y: (20,)\n",
      "Val   set: (1275, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.259 s \n",
      "\n",
      "Accuracy rate for 84.750000 \n",
      "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.86       200\n",
      "           1       0.97      0.72      0.83       200\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       400\n",
      "   macro avg       0.87      0.85      0.84       400\n",
      "weighted avg       0.87      0.85      0.84       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [ 56 144]]\n",
      "--------------------------------\n",
      "val predicted: (1275,) [0 0 0 ... 1 1 0]\n",
      "probabilities: (1275, 2) \n",
      " [0 0 0 ... 1 1 0]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (20, 43200) (20,)\n",
      "trainset after (30, 43200) (30,)\n",
      "updated train set: (30, 43200) (30,) unique(labels): [14 16] [0 1]\n",
      "val set: (1265, 43200) (1265,)\n",
      "\n",
      "Train set: (30, 43200) y: (30,)\n",
      "Val   set: (1265, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.245 s \n",
      "\n",
      "Accuracy rate for 88.500000 \n",
      "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87       200\n",
      "           1       0.83      0.97      0.89       200\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       400\n",
      "   macro avg       0.90      0.89      0.88       400\n",
      "weighted avg       0.90      0.89      0.88       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[160  40]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1265,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1265, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (30, 43200) (30,)\n",
      "trainset after (40, 43200) (40,)\n",
      "updated train set: (40, 43200) (40,) unique(labels): [17 23] [0 1]\n",
      "val set: (1255, 43200) (1255,)\n",
      "\n",
      "Train set: (40, 43200) y: (40,)\n",
      "Val   set: (1255, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.378 s \n",
      "\n",
      "Accuracy rate for 94.250000 \n",
      "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       200\n",
      "           1       0.97      0.92      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [ 17 183]]\n",
      "--------------------------------\n",
      "val predicted: (1255,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1255, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (40, 43200) (40,)\n",
      "trainset after (50, 43200) (50,)\n",
      "updated train set: (50, 43200) (50,) unique(labels): [21 29] [0 1]\n",
      "val set: (1245, 43200) (1245,)\n",
      "\n",
      "Train set: (50, 43200) y: (50,)\n",
      "Val   set: (1245, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.392 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       200\n",
      "           1       0.98      0.93      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.96      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [ 14 186]]\n",
      "--------------------------------\n",
      "val predicted: (1245,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1245, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (50, 43200) (50,)\n",
      "trainset after (60, 43200) (60,)\n",
      "updated train set: (60, 43200) (60,) unique(labels): [30 30] [0 1]\n",
      "val set: (1235, 43200) (1235,)\n",
      "\n",
      "Train set: (60, 43200) y: (60,)\n",
      "Val   set: (1235, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.454 s \n",
      "\n",
      "Accuracy rate for 95.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       200\n",
      "           1       0.97      0.94      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [ 12 188]]\n",
      "--------------------------------\n",
      "val predicted: (1235,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1235, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (60, 43200) (60,)\n",
      "trainset after (70, 43200) (70,)\n",
      "updated train set: (70, 43200) (70,) unique(labels): [33 37] [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val set: (1225, 43200) (1225,)\n",
      "\n",
      "Train set: (70, 43200) y: (70,)\n",
      "Val   set: (1225, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.498 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       200\n",
      "           1       0.95      0.98      0.97       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.97      0.96      0.96       400\n",
      "weighted avg       0.97      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1225,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1225, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (70, 43200) (70,)\n",
      "trainset after (80, 43200) (80,)\n",
      "updated train set: (80, 43200) (80,) unique(labels): [35 45] [0 1]\n",
      "val set: (1215, 43200) (1215,)\n",
      "\n",
      "Train set: (80, 43200) y: (80,)\n",
      "Val   set: (1215, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.552 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       200\n",
      "           1       0.95      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1215,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1215, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (80, 43200) (80,)\n",
      "trainset after (90, 43200) (90,)\n",
      "updated train set: (90, 43200) (90,) unique(labels): [36 54] [0 1]\n",
      "val set: (1205, 43200) (1205,)\n",
      "\n",
      "Train set: (90, 43200) y: (90,)\n",
      "Val   set: (1205, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.546 s \n",
      "\n",
      "Accuracy rate for 94.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95       200\n",
      "           1       0.92      0.97      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[184  16]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1205,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1205, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (90, 43200) (90,)\n",
      "trainset after (100, 43200) (100,)\n",
      "updated train set: (100, 43200) (100,) unique(labels): [43 57] [0 1]\n",
      "val set: (1195, 43200) (1195,)\n",
      "\n",
      "Train set: (100, 43200) y: (100,)\n",
      "Val   set: (1195, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.603 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95       200\n",
      "           1       0.94      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[188  12]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1195,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1195, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (100, 43200) (100,)\n",
      "trainset after (110, 43200) (110,)\n",
      "updated train set: (110, 43200) (110,) unique(labels): [47 63] [0 1]\n",
      "val set: (1185, 43200) (1185,)\n",
      "\n",
      "Train set: (110, 43200) y: (110,)\n",
      "Val   set: (1185, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.678 s \n",
      "\n",
      "Accuracy rate for 94.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.94      0.96      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[187  13]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (1185,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1185, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (110, 43200) (110,)\n",
      "trainset after (120, 43200) (120,)\n",
      "updated train set: (120, 43200) (120,) unique(labels): [49 71] [0 1]\n",
      "val set: (1175, 43200) (1175,)\n",
      "\n",
      "Train set: (120, 43200) y: (120,)\n",
      "Val   set: (1175, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.710 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1175,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1175, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (120, 43200) (120,)\n",
      "trainset after (130, 43200) (130,)\n",
      "updated train set: (130, 43200) (130,) unique(labels): [56 74] [0 1]\n",
      "val set: (1165, 43200) (1165,)\n",
      "\n",
      "Train set: (130, 43200) y: (130,)\n",
      "Val   set: (1165, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.738 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1165,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1165, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (130, 43200) (130,)\n",
      "trainset after (140, 43200) (140,)\n",
      "updated train set: (140, 43200) (140,) unique(labels): [60 80] [0 1]\n",
      "val set: (1155, 43200) (1155,)\n",
      "\n",
      "Train set: (140, 43200) y: (140,)\n",
      "Val   set: (1155, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.785 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1155,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1155, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (140, 43200) (140,)\n",
      "trainset after (150, 43200) (150,)\n",
      "updated train set: (150, 43200) (150,) unique(labels): [65 85] [0 1]\n",
      "val set: (1145, 43200) (1145,)\n",
      "\n",
      "Train set: (150, 43200) y: (150,)\n",
      "Val   set: (1145, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.742 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       200\n",
      "           1       0.95      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (1145,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1145, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (150, 43200) (150,)\n",
      "trainset after (160, 43200) (160,)\n",
      "updated train set: (160, 43200) (160,) unique(labels): [71 89] [0 1]\n",
      "val set: (1135, 43200) (1135,)\n",
      "\n",
      "Train set: (160, 43200) y: (160,)\n",
      "Val   set: (1135, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.888 s \n",
      "\n",
      "Accuracy rate for 95.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1135,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1135, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (160, 43200) (160,)\n",
      "trainset after (170, 43200) (170,)\n",
      "updated train set: (170, 43200) (170,) unique(labels): [75 95] [0 1]\n",
      "val set: (1125, 43200) (1125,)\n",
      "\n",
      "Train set: (170, 43200) y: (170,)\n",
      "Val   set: (1125, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.889 s \n",
      "\n",
      "Accuracy rate for 95.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.95      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1125,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1125, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (170, 43200) (170,)\n",
      "trainset after (180, 43200) (180,)\n",
      "updated train set: (180, 43200) (180,) unique(labels): [82 98] [0 1]\n",
      "val set: (1115, 43200) (1115,)\n",
      "\n",
      "Train set: (180, 43200) y: (180,)\n",
      "Val   set: (1115, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.127 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (1115,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1115, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (180, 43200) (180,)\n",
      "trainset after (190, 43200) (190,)\n",
      "updated train set: (190, 43200) (190,) unique(labels): [ 84 106] [0 1]\n",
      "val set: (1105, 43200) (1105,)\n",
      "\n",
      "Train set: (190, 43200) y: (190,)\n",
      "Val   set: (1105, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.044 s \n",
      "\n",
      "Accuracy rate for 95.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.94      0.96      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[188  12]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1105,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1105, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (190, 43200) (190,)\n",
      "trainset after (200, 43200) (200,)\n",
      "updated train set: (200, 43200) (200,) unique(labels): [ 86 114] [0 1]\n",
      "val set: (1095, 43200) (1095,)\n",
      "\n",
      "Train set: (200, 43200) y: (200,)\n",
      "Val   set: (1095, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.192 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.95      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1095,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1095, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (200, 43200) (200,)\n",
      "trainset after (210, 43200) (210,)\n",
      "updated train set: (210, 43200) (210,) unique(labels): [ 89 121] [0 1]\n",
      "val set: (1085, 43200) (1085,)\n",
      "\n",
      "Train set: (210, 43200) y: (210,)\n",
      "Val   set: (1085, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 21\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.202 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1085,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1085, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (210, 43200) (210,)\n",
      "trainset after (220, 43200) (220,)\n",
      "updated train set: (220, 43200) (220,) unique(labels): [ 94 126] [0 1]\n",
      "val set: (1075, 43200) (1075,)\n",
      "\n",
      "Train set: (220, 43200) y: (220,)\n",
      "Val   set: (1075, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 22\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.226 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1075,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1075, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (220, 43200) (220,)\n",
      "trainset after (230, 43200) (230,)\n",
      "updated train set: (230, 43200) (230,) unique(labels): [ 99 131] [0 1]\n",
      "val set: (1065, 43200) (1065,)\n",
      "\n",
      "Train set: (230, 43200) y: (230,)\n",
      "Val   set: (1065, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 23\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.176 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       200\n",
      "           1       0.93      0.97      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[186  14]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1065,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1065, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (230, 43200) (230,)\n",
      "trainset after (240, 43200) (240,)\n",
      "updated train set: (240, 43200) (240,) unique(labels): [102 138] [0 1]\n",
      "val set: (1055, 43200) (1055,)\n",
      "\n",
      "Train set: (240, 43200) y: (240,)\n",
      "Val   set: (1055, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 24\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.122 s \n",
      "\n",
      "Accuracy rate for 94.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.94      0.96      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[187  13]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (1055,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1055, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (240, 43200) (240,)\n",
      "trainset after (250, 43200) (250,)\n",
      "updated train set: (250, 43200) (250,) unique(labels): [104 146] [0 1]\n",
      "val set: (1045, 43200) (1045,)\n",
      "\n",
      "Train set: (250, 43200) y: (250,)\n",
      "Val   set: (1045, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 25\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.170 s \n",
      "\n",
      "Accuracy rate for 94.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95       200\n",
      "           1       0.93      0.96      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[186  14]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1045,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1045, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (250, 43200) (250,)\n",
      "trainset after (260, 43200) (260,)\n",
      "updated train set: (260, 43200) (260,) unique(labels): [107 153] [0 1]\n",
      "val set: (1035, 43200) (1035,)\n",
      "\n",
      "Train set: (260, 43200) y: (260,)\n",
      "Val   set: (1035, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 26\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.361 s \n",
      "\n",
      "Accuracy rate for 94.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       200\n",
      "           1       0.92      0.97      0.95       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.95      0.95      0.94       400\n",
      "weighted avg       0.95      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[184  16]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1035,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1035, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (260, 43200) (260,)\n",
      "trainset after (270, 43200) (270,)\n",
      "updated train set: (270, 43200) (270,) unique(labels): [110 160] [0 1]\n",
      "val set: (1025, 43200) (1025,)\n",
      "\n",
      "Train set: (270, 43200) y: (270,)\n",
      "Val   set: (1025, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 27\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.410 s \n",
      "\n",
      "Accuracy rate for 93.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       200\n",
      "           1       0.91      0.96      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[182  18]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1025,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1025, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (270, 43200) (270,)\n",
      "trainset after (280, 43200) (280,)\n",
      "updated train set: (280, 43200) (280,) unique(labels): [113 167] [0 1]\n",
      "val set: (1015, 43200) (1015,)\n",
      "\n",
      "Train set: (280, 43200) y: (280,)\n",
      "Val   set: (1015, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 28\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.288 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.94      0.96      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[187  13]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1015,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1015, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (280, 43200) (280,)\n",
      "trainset after (290, 43200) (290,)\n",
      "updated train set: (290, 43200) (290,) unique(labels): [116 174] [0 1]\n",
      "val set: (1005, 43200) (1005,)\n",
      "\n",
      "Train set: (290, 43200) y: (290,)\n",
      "Val   set: (1005, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 29\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.514 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (1005,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1005, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (290, 43200) (290,)\n",
      "trainset after (300, 43200) (300,)\n",
      "updated train set: (300, 43200) (300,) unique(labels): [119 181] [0 1]\n",
      "val set: (995, 43200) (995,)\n",
      "\n",
      "Train set: (300, 43200) y: (300,)\n",
      "Val   set: (995, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 30\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.562 s \n",
      "\n",
      "Accuracy rate for 93.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (995,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "probabilities: (995, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (300, 43200) (300,)\n",
      "trainset after (310, 43200) (310,)\n",
      "updated train set: (310, 43200) (310,) unique(labels): [124 186] [0 1]\n",
      "val set: (985, 43200) (985,)\n",
      "\n",
      "Train set: (310, 43200) y: (310,)\n",
      "Val   set: (985, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 31\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.419 s \n",
      "\n",
      "Accuracy rate for 93.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93       200\n",
      "           1       0.92      0.95      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.93       400\n",
      "weighted avg       0.94      0.94      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[183  17]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (985,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "probabilities: (985, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (310, 43200) (310,)\n",
      "trainset after (320, 43200) (320,)\n",
      "updated train set: (320, 43200) (320,) unique(labels): [127 193] [0 1]\n",
      "val set: (975, 43200) (975,)\n",
      "\n",
      "Train set: (320, 43200) y: (320,)\n",
      "Val   set: (975, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 32\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.460 s \n",
      "\n",
      "Accuracy rate for 93.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (975,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "probabilities: (975, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (320, 43200) (320,)\n",
      "trainset after (330, 43200) (330,)\n",
      "updated train set: (330, 43200) (330,) unique(labels): [131 199] [0 1]\n",
      "val set: (965, 43200) (965,)\n",
      "\n",
      "Train set: (330, 43200) y: (330,)\n",
      "Val   set: (965, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 33\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.701 s \n",
      "\n",
      "Accuracy rate for 92.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       200\n",
      "           1       0.90      0.95      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.92       400\n",
      "weighted avg       0.93      0.93      0.92       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[179  21]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (965,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (965, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (330, 43200) (330,)\n",
      "trainset after (340, 43200) (340,)\n",
      "updated train set: (340, 43200) (340,) unique(labels): [136 204] [0 1]\n",
      "val set: (955, 43200) (955,)\n",
      "\n",
      "Train set: (340, 43200) y: (340,)\n",
      "Val   set: (955, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 34\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.535 s \n",
      "\n",
      "Accuracy rate for 93.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       200\n",
      "           1       0.91      0.96      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[182  18]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (955,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "probabilities: (955, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (340, 43200) (340,)\n",
      "trainset after (350, 43200) (350,)\n",
      "updated train set: (350, 43200) (350,) unique(labels): [139 211] [0 1]\n",
      "val set: (945, 43200) (945,)\n",
      "\n",
      "Train set: (350, 43200) y: (350,)\n",
      "Val   set: (945, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 35\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.583 s \n",
      "\n",
      "Accuracy rate for 93.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.93       400\n",
      "weighted avg       0.94      0.94      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (945,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (945, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (350, 43200) (350,)\n",
      "trainset after (360, 43200) (360,)\n",
      "updated train set: (360, 43200) (360,) unique(labels): [146 214] [0 1]\n",
      "val set: (935, 43200) (935,)\n",
      "\n",
      "Train set: (360, 43200) y: (360,)\n",
      "Val   set: (935, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 36\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.620 s \n",
      "\n",
      "Accuracy rate for 93.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       200\n",
      "           1       0.92      0.95      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[184  16]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (935,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (935, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (360, 43200) (360,)\n",
      "trainset after (370, 43200) (370,)\n",
      "updated train set: (370, 43200) (370,) unique(labels): [150 220] [0 1]\n",
      "val set: (925, 43200) (925,)\n",
      "\n",
      "Train set: (370, 43200) y: (370,)\n",
      "Val   set: (925, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 37\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.665 s \n",
      "\n",
      "Accuracy rate for 93.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       200\n",
      "           1       0.92      0.95      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[184  16]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (925,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (925, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (370, 43200) (370,)\n",
      "trainset after (380, 43200) (380,)\n",
      "updated train set: (380, 43200) (380,) unique(labels): [154 226] [0 1]\n",
      "val set: (915, 43200) (915,)\n",
      "\n",
      "Train set: (380, 43200) y: (380,)\n",
      "Val   set: (915, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 38\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.706 s \n",
      "\n",
      "Accuracy rate for 92.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       200\n",
      "           1       0.91      0.95      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "val predicted: (915,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (915, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (380, 43200) (380,)\n",
      "trainset after (390, 43200) (390,)\n",
      "updated train set: (390, 43200) (390,) unique(labels): [159 231] [0 1]\n",
      "val set: (905, 43200) (905,)\n",
      "\n",
      "Train set: (390, 43200) y: (390,)\n",
      "Val   set: (905, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 39\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.969 s \n",
      "\n",
      "Accuracy rate for 93.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (905,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (905, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (390, 43200) (390,)\n",
      "trainset after (400, 43200) (400,)\n",
      "updated train set: (400, 43200) (400,) unique(labels): [165 235] [0 1]\n",
      "val set: (895, 43200) (895,)\n",
      "\n",
      "Train set: (400, 43200) y: (400,)\n",
      "Val   set: (895, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 40\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.802 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (895,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "probabilities: (895, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (400, 43200) (400,)\n",
      "trainset after (410, 43200) (410,)\n",
      "updated train set: (410, 43200) (410,) unique(labels): [170 240] [0 1]\n",
      "val set: (885, 43200) (885,)\n",
      "\n",
      "Train set: (410, 43200) y: (410,)\n",
      "Val   set: (885, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 41\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.851 s \n",
      "\n",
      "Accuracy rate for 92.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       200\n",
      "           1       0.90      0.95      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.92       400\n",
      "weighted avg       0.93      0.93      0.92       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "val predicted: (885,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (885, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (410, 43200) (410,)\n",
      "trainset after (420, 43200) (420,)\n",
      "updated train set: (420, 43200) (420,) unique(labels): [172 248] [0 1]\n",
      "val set: (875, 43200) (875,)\n",
      "\n",
      "Train set: (420, 43200) y: (420,)\n",
      "Val   set: (875, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 42\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.877 s \n",
      "\n",
      "Accuracy rate for 92.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93       200\n",
      "           1       0.91      0.95      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (875,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (875, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (420, 43200) (420,)\n",
      "trainset after (430, 43200) (430,)\n",
      "updated train set: (430, 43200) (430,) unique(labels): [174 256] [0 1]\n",
      "val set: (865, 43200) (865,)\n",
      "\n",
      "Train set: (430, 43200) y: (430,)\n",
      "Val   set: (865, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 43\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.926 s \n",
      "\n",
      "Accuracy rate for 93.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.93       400\n",
      "weighted avg       0.94      0.94      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[182  18]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (865,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (865, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (430, 43200) (430,)\n",
      "trainset after (440, 43200) (440,)\n",
      "updated train set: (440, 43200) (440,) unique(labels): [179 261] [0 1]\n",
      "val set: (855, 43200) (855,)\n",
      "\n",
      "Train set: (440, 43200) y: (440,)\n",
      "Val   set: (855, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 44\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.978 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (855,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "probabilities: (855, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (440, 43200) (440,)\n",
      "trainset after (450, 43200) (450,)\n",
      "updated train set: (450, 43200) (450,) unique(labels): [184 266] [0 1]\n",
      "val set: (845, 43200) (845,)\n",
      "\n",
      "Train set: (450, 43200) y: (450,)\n",
      "Val   set: (845, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 45\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.005 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (845,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (845, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (450, 43200) (450,)\n",
      "trainset after (460, 43200) (460,)\n",
      "updated train set: (460, 43200) (460,) unique(labels): [191 269] [0 1]\n",
      "val set: (835, 43200) (835,)\n",
      "\n",
      "Train set: (460, 43200) y: (460,)\n",
      "Val   set: (835, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 46\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.330 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (835,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (835, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (460, 43200) (460,)\n",
      "trainset after (470, 43200) (470,)\n",
      "updated train set: (470, 43200) (470,) unique(labels): [194 276] [0 1]\n",
      "val set: (825, 43200) (825,)\n",
      "\n",
      "Train set: (470, 43200) y: (470,)\n",
      "Val   set: (825, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 47\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.103 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  20]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (825,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (825, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (470, 43200) (470,)\n",
      "trainset after (480, 43200) (480,)\n",
      "updated train set: (480, 43200) (480,) unique(labels): [198 282] [0 1]\n",
      "val set: (815, 43200) (815,)\n",
      "\n",
      "Train set: (480, 43200) y: (480,)\n",
      "Val   set: (815, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 48\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.131 s \n",
      "\n",
      "Accuracy rate for 92.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.90      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[179  21]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (815,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "probabilities: (815, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (480, 43200) (480,)\n",
      "trainset after (490, 43200) (490,)\n",
      "updated train set: (490, 43200) (490,) unique(labels): [200 290] [0 1]\n",
      "val set: (805, 43200) (805,)\n",
      "\n",
      "Train set: (490, 43200) y: (490,)\n",
      "Val   set: (805, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 49\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.184 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93       200\n",
      "           1       0.91      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[181  19]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (805,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (805, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++1\n",
      "trainset before (490, 43200) (490,)\n",
      "trainset after (500, 43200) (500,)\n",
      "updated train set: (500, 43200) (500,) unique(labels): [204 296] [0 1]\n",
      "val set: (795, 43200) (795,)\n",
      "\n",
      "Train set: (500, 43200) y: (500,)\n",
      "Val   set: (795, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 50\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.228 s \n",
      "\n",
      "Accuracy rate for 92.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93       200\n",
      "           1       0.90      0.96      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.93      0.93      0.93       400\n",
      "weighted avg       0.93      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[179  21]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "final active learning accuracies [76.75, 84.75, 88.5, 94.25, 95.5, 95.75, 96.5, 96.25, 94.75, 95.5, 94.75, 96.25, 95.5, 95.5, 95.5, 95.25, 95.75, 96.0, 95.25, 95.5, 95.0, 95.0, 95.0, 94.75, 94.75, 94.5, 93.75, 95.0, 93.25, 93.0, 93.5, 93.0, 92.5, 93.75, 93.5, 93.75, 93.75, 92.75, 93.0, 93.25, 92.5, 92.75, 93.5, 93.25, 93.25, 93.25, 93.25, 92.75, 93.25, 92.75]\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 2, using model = LogModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 43200) (10,) unique(labels): [6 4] [0 1]\n",
      "val set: (1285, 43200) (1285,) (10,)\n",
      "\n",
      "Train set: (10, 43200) y: (10,)\n",
      "Val   set: (1285, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.153 s \n",
      "\n",
      "Accuracy rate for 64.250000 \n",
      "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74       200\n",
      "           1       1.00      0.28      0.44       200\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       400\n",
      "   macro avg       0.79      0.64      0.59       400\n",
      "weighted avg       0.79      0.64      0.59       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[200   0]\n",
      " [143  57]]\n",
      "--------------------------------\n",
      "val predicted: (1285,) [0 0 0 ... 0 1 0]\n",
      "probabilities: (1285, 2) \n",
      " [0 0 0 ... 0 1 0]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (10, 43200) (10,)\n",
      "trainset after (20, 43200) (20,)\n",
      "updated train set: (20, 43200) (20,) unique(labels): [ 6 14] [0 1]\n",
      "val set: (1275, 43200) (1275,)\n",
      "\n",
      "Train set: (20, 43200) y: (20,)\n",
      "Val   set: (1275, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.177 s \n",
      "\n",
      "Accuracy rate for 83.250000 \n",
      "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.99      0.86       200\n",
      "           1       0.99      0.68      0.80       200\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       400\n",
      "   macro avg       0.87      0.83      0.83       400\n",
      "weighted avg       0.87      0.83      0.83       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [ 65 135]]\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val predicted: (1275,) [0 0 0 ... 0 1 1]\n",
      "probabilities: (1275, 2) \n",
      " [0 0 0 ... 0 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (20, 43200) (20,)\n",
      "trainset after (30, 43200) (30,)\n",
      "updated train set: (30, 43200) (30,) unique(labels): [ 6 24] [0 1]\n",
      "val set: (1265, 43200) (1265,)\n",
      "\n",
      "Train set: (30, 43200) y: (30,)\n",
      "Val   set: (1265, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.361 s \n",
      "\n",
      "Accuracy rate for 72.500000 \n",
      "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.45      0.62       200\n",
      "           1       0.65      1.00      0.78       200\n",
      "\n",
      "   micro avg       0.72      0.72      0.73       400\n",
      "   macro avg       0.82      0.72      0.70       400\n",
      "weighted avg       0.82      0.72      0.70       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 90 110]\n",
      " [  0 200]]\n",
      "--------------------------------\n",
      "val predicted: (1265,) [0 1 0 ... 1 1 1]\n",
      "probabilities: (1265, 2) \n",
      " [0 1 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (30, 43200) (30,)\n",
      "trainset after (40, 43200) (40,)\n",
      "updated train set: (40, 43200) (40,) unique(labels): [15 25] [0 1]\n",
      "val set: (1255, 43200) (1255,)\n",
      "\n",
      "Train set: (40, 43200) y: (40,)\n",
      "Val   set: (1255, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.393 s \n",
      "\n",
      "Accuracy rate for 98.250000 \n",
      "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       200\n",
      "           1       0.99      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1255,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1255, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (40, 43200) (40,)\n",
      "trainset after (50, 43200) (50,)\n",
      "updated train set: (50, 43200) (50,) unique(labels): [21 29] [0 1]\n",
      "val set: (1245, 43200) (1245,)\n",
      "\n",
      "Train set: (50, 43200) y: (50,)\n",
      "Val   set: (1245, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.440 s \n",
      "\n",
      "Accuracy rate for 97.750000 \n",
      "Classification report for classifier LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       200\n",
      "           1       0.98      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[197   3]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1245,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1245, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (50, 43200) (50,)\n",
      "trainset after (60, 43200) (60,)\n",
      "updated train set: (60, 43200) (60,) unique(labels): [24 36] [0 1]\n",
      "val set: (1235, 43200) (1235,)\n",
      "\n",
      "Train set: (60, 43200) y: (60,)\n",
      "Val   set: (1235, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.467 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       200\n",
      "           1       0.97      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1235,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1235, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (60, 43200) (60,)\n",
      "trainset after (70, 43200) (70,)\n",
      "updated train set: (70, 43200) (70,) unique(labels): [27 43] [0 1]\n",
      "val set: (1225, 43200) (1225,)\n",
      "\n",
      "Train set: (70, 43200) y: (70,)\n",
      "Val   set: (1225, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.548 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       200\n",
      "           1       0.96      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1225,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1225, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (70, 43200) (70,)\n",
      "trainset after (80, 43200) (80,)\n",
      "updated train set: (80, 43200) (80,) unique(labels): [30 50] [0 1]\n",
      "val set: (1215, 43200) (1215,)\n",
      "\n",
      "Train set: (80, 43200) y: (80,)\n",
      "Val   set: (1215, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.567 s \n",
      "\n",
      "Accuracy rate for 97.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1215,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1215, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (80, 43200) (80,)\n",
      "trainset after (90, 43200) (90,)\n",
      "updated train set: (90, 43200) (90,) unique(labels): [30 60] [0 1]\n",
      "val set: (1205, 43200) (1205,)\n",
      "\n",
      "Train set: (90, 43200) y: (90,)\n",
      "Val   set: (1205, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.689 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       200\n",
      "           1       0.95      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1205,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1205, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (90, 43200) (90,)\n",
      "trainset after (100, 43200) (100,)\n",
      "updated train set: (100, 43200) (100,) unique(labels): [34 66] [0 1]\n",
      "val set: (1195, 43200) (1195,)\n",
      "\n",
      "Train set: (100, 43200) y: (100,)\n",
      "Val   set: (1195, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.736 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       200\n",
      "           1       0.99      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1195,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1195, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (100, 43200) (100,)\n",
      "trainset after (110, 43200) (110,)\n",
      "updated train set: (110, 43200) (110,) unique(labels): [34 76] [0 1]\n",
      "val set: (1185, 43200) (1185,)\n",
      "\n",
      "Train set: (110, 43200) y: (110,)\n",
      "Val   set: (1185, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.791 s \n",
      "\n",
      "Accuracy rate for 97.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1185,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1185, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (110, 43200) (110,)\n",
      "trainset after (120, 43200) (120,)\n",
      "updated train set: (120, 43200) (120,) unique(labels): [38 82] [0 1]\n",
      "val set: (1175, 43200) (1175,)\n",
      "\n",
      "Train set: (120, 43200) y: (120,)\n",
      "Val   set: (1175, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.841 s \n",
      "\n",
      "Accuracy rate for 99.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       200\n",
      "           1       0.99      0.99      0.99       200\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[197   3]\n",
      " [  1 199]]\n",
      "--------------------------------\n",
      "val predicted: (1175,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1175, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (120, 43200) (120,)\n",
      "trainset after (130, 43200) (130,)\n",
      "updated train set: (130, 43200) (130,) unique(labels): [40 90] [0 1]\n",
      "val set: (1165, 43200) (1165,)\n",
      "\n",
      "Train set: (130, 43200) y: (130,)\n",
      "Val   set: (1165, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.825 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1165,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1165, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (130, 43200) (130,)\n",
      "trainset after (140, 43200) (140,)\n",
      "updated train set: (140, 43200) (140,) unique(labels): [43 97] [0 1]\n",
      "val set: (1155, 43200) (1155,)\n",
      "\n",
      "Train set: (140, 43200) y: (140,)\n",
      "Val   set: (1155, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.960 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       200\n",
      "           1       0.97      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1155,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1155, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (140, 43200) (140,)\n",
      "trainset after (150, 43200) (150,)\n",
      "updated train set: (150, 43200) (150,) unique(labels): [ 46 104] [0 1]\n",
      "val set: (1145, 43200) (1145,)\n",
      "\n",
      "Train set: (150, 43200) y: (150,)\n",
      "Val   set: (1145, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.924 s \n",
      "\n",
      "Accuracy rate for 98.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[197   3]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (1145,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1145, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (150, 43200) (150,)\n",
      "trainset after (160, 43200) (160,)\n",
      "updated train set: (160, 43200) (160,) unique(labels): [ 48 112] [0 1]\n",
      "val set: (1135, 43200) (1135,)\n",
      "\n",
      "Train set: (160, 43200) y: (160,)\n",
      "Val   set: (1135, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.984 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1135,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1135, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (160, 43200) (160,)\n",
      "trainset after (170, 43200) (170,)\n",
      "updated train set: (170, 43200) (170,) unique(labels): [ 54 116] [0 1]\n",
      "val set: (1125, 43200) (1125,)\n",
      "\n",
      "Train set: (170, 43200) y: (170,)\n",
      "Val   set: (1125, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.033 s \n",
      "\n",
      "Accuracy rate for 98.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (1125,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1125, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (170, 43200) (170,)\n",
      "trainset after (180, 43200) (180,)\n",
      "updated train set: (180, 43200) (180,) unique(labels): [ 56 124] [0 1]\n",
      "val set: (1115, 43200) (1115,)\n",
      "\n",
      "Train set: (180, 43200) y: (180,)\n",
      "Val   set: (1115, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.085 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (1115,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1115, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (180, 43200) (180,)\n",
      "trainset after (190, 43200) (190,)\n",
      "updated train set: (190, 43200) (190,) unique(labels): [ 57 133] [0 1]\n",
      "val set: (1105, 43200) (1105,)\n",
      "\n",
      "Train set: (190, 43200) y: (190,)\n",
      "Val   set: (1105, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.263 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1105,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1105, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (190, 43200) (190,)\n",
      "trainset after (200, 43200) (200,)\n",
      "updated train set: (200, 43200) (200,) unique(labels): [ 61 139] [0 1]\n",
      "val set: (1095, 43200) (1095,)\n",
      "\n",
      "Train set: (200, 43200) y: (200,)\n",
      "Val   set: (1095, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.320 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1095,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1095, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (200, 43200) (200,)\n",
      "trainset after (210, 43200) (210,)\n",
      "updated train set: (210, 43200) (210,) unique(labels): [ 62 148] [0 1]\n",
      "val set: (1085, 43200) (1085,)\n",
      "\n",
      "Train set: (210, 43200) y: (210,)\n",
      "Val   set: (1085, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 21\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.377 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       200\n",
      "           1       0.97      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1085,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1085, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (210, 43200) (210,)\n",
      "trainset after (220, 43200) (220,)\n",
      "updated train set: (220, 43200) (220,) unique(labels): [ 62 158] [0 1]\n",
      "val set: (1075, 43200) (1075,)\n",
      "\n",
      "Train set: (220, 43200) y: (220,)\n",
      "Val   set: (1075, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 22\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.438 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1075,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1075, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (220, 43200) (220,)\n",
      "trainset after (230, 43200) (230,)\n",
      "updated train set: (230, 43200) (230,) unique(labels): [ 66 164] [0 1]\n",
      "val set: (1065, 43200) (1065,)\n",
      "\n",
      "Train set: (230, 43200) y: (230,)\n",
      "Val   set: (1065, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 23\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.356 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1065,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1065, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (230, 43200) (230,)\n",
      "trainset after (240, 43200) (240,)\n",
      "updated train set: (240, 43200) (240,) unique(labels): [ 66 174] [0 1]\n",
      "val set: (1055, 43200) (1055,)\n",
      "\n",
      "Train set: (240, 43200) y: (240,)\n",
      "Val   set: (1055, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 24\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.275 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       200\n",
      "           1       0.97      0.98      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1055,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1055, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (240, 43200) (240,)\n",
      "trainset after (250, 43200) (250,)\n",
      "updated train set: (250, 43200) (250,) unique(labels): [ 71 179] [0 1]\n",
      "val set: (1045, 43200) (1045,)\n",
      "\n",
      "Train set: (250, 43200) y: (250,)\n",
      "Val   set: (1045, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 25\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.464 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1045,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1045, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (250, 43200) (250,)\n",
      "trainset after (260, 43200) (260,)\n",
      "updated train set: (260, 43200) (260,) unique(labels): [ 75 185] [0 1]\n",
      "val set: (1035, 43200) (1035,)\n",
      "\n",
      "Train set: (260, 43200) y: (260,)\n",
      "Val   set: (1035, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 26\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.364 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       200\n",
      "           1       0.97      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1035,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1035, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (260, 43200) (260,)\n",
      "trainset after (270, 43200) (270,)\n",
      "updated train set: (270, 43200) (270,) unique(labels): [ 77 193] [0 1]\n",
      "val set: (1025, 43200) (1025,)\n",
      "\n",
      "Train set: (270, 43200) y: (270,)\n",
      "Val   set: (1025, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 27\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.652 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1025,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1025, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (270, 43200) (270,)\n",
      "trainset after (280, 43200) (280,)\n",
      "updated train set: (280, 43200) (280,) unique(labels): [ 77 203] [0 1]\n",
      "val set: (1015, 43200) (1015,)\n",
      "\n",
      "Train set: (280, 43200) y: (280,)\n",
      "Val   set: (1015, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 28\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.647 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1015,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1015, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (280, 43200) (280,)\n",
      "trainset after (290, 43200) (290,)\n",
      "updated train set: (290, 43200) (290,) unique(labels): [ 77 213] [0 1]\n",
      "val set: (1005, 43200) (1005,)\n",
      "\n",
      "Train set: (290, 43200) y: (290,)\n",
      "Val   set: (1005, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 29\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.342 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1005,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1005, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (290, 43200) (290,)\n",
      "trainset after (300, 43200) (300,)\n",
      "updated train set: (300, 43200) (300,) unique(labels): [ 77 223] [0 1]\n",
      "val set: (995, 43200) (995,)\n",
      "\n",
      "Train set: (300, 43200) y: (300,)\n",
      "Val   set: (995, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 30\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.549 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (995,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (995, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (300, 43200) (300,)\n",
      "trainset after (310, 43200) (310,)\n",
      "updated train set: (310, 43200) (310,) unique(labels): [ 78 232] [0 1]\n",
      "val set: (985, 43200) (985,)\n",
      "\n",
      "Train set: (310, 43200) y: (310,)\n",
      "Val   set: (985, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 31\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.603 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       200\n",
      "           1       0.96      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (985,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (985, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (310, 43200) (310,)\n",
      "trainset after (320, 43200) (320,)\n",
      "updated train set: (320, 43200) (320,) unique(labels): [ 82 238] [0 1]\n",
      "val set: (975, 43200) (975,)\n",
      "\n",
      "Train set: (320, 43200) y: (320,)\n",
      "Val   set: (975, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 32\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.040 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       200\n",
      "           1       0.97      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (975,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (975, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (320, 43200) (320,)\n",
      "trainset after (330, 43200) (330,)\n",
      "updated train set: (330, 43200) (330,) unique(labels): [ 84 246] [0 1]\n",
      "val set: (965, 43200) (965,)\n",
      "\n",
      "Train set: (330, 43200) y: (330,)\n",
      "Val   set: (965, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 33\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.896 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (965,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (965, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (330, 43200) (330,)\n",
      "trainset after (340, 43200) (340,)\n",
      "updated train set: (340, 43200) (340,) unique(labels): [ 86 254] [0 1]\n",
      "val set: (955, 43200) (955,)\n",
      "\n",
      "Train set: (340, 43200) y: (340,)\n",
      "Val   set: (955, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 34\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.168 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (955,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (955, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (340, 43200) (340,)\n",
      "trainset after (350, 43200) (350,)\n",
      "updated train set: (350, 43200) (350,) unique(labels): [ 87 263] [0 1]\n",
      "val set: (945, 43200) (945,)\n",
      "\n",
      "Train set: (350, 43200) y: (350,)\n",
      "Val   set: (945, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 35\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.008 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (945,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (945, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (350, 43200) (350,)\n",
      "trainset after (360, 43200) (360,)\n",
      "updated train set: (360, 43200) (360,) unique(labels): [ 88 272] [0 1]\n",
      "val set: (935, 43200) (935,)\n",
      "\n",
      "Train set: (360, 43200) y: (360,)\n",
      "Val   set: (935, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 36\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.083 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (935,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (935, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (360, 43200) (360,)\n",
      "trainset after (370, 43200) (370,)\n",
      "updated train set: (370, 43200) (370,) unique(labels): [ 88 282] [0 1]\n",
      "val set: (925, 43200) (925,)\n",
      "\n",
      "Train set: (370, 43200) y: (370,)\n",
      "Val   set: (925, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 37\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.401 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (925,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (925, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (370, 43200) (370,)\n",
      "trainset after (380, 43200) (380,)\n",
      "updated train set: (380, 43200) (380,) unique(labels): [ 91 289] [0 1]\n",
      "val set: (915, 43200) (915,)\n",
      "\n",
      "Train set: (380, 43200) y: (380,)\n",
      "Val   set: (915, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 38\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.172 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       200\n",
      "           1       0.97      0.95      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (915,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (915, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (380, 43200) (380,)\n",
      "trainset after (390, 43200) (390,)\n",
      "updated train set: (390, 43200) (390,) unique(labels): [ 97 293] [0 1]\n",
      "val set: (905, 43200) (905,)\n",
      "\n",
      "Train set: (390, 43200) y: (390,)\n",
      "Val   set: (905, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 39\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.225 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       200\n",
      "           1       0.96      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.97      0.96      0.96       400\n",
      "weighted avg       0.97      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (905,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (905, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (390, 43200) (390,)\n",
      "trainset after (400, 43200) (400,)\n",
      "updated train set: (400, 43200) (400,) unique(labels): [101 299] [0 1]\n",
      "val set: (895, 43200) (895,)\n",
      "\n",
      "Train set: (400, 43200) y: (400,)\n",
      "Val   set: (895, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 40\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.569 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (895,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "probabilities: (895, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (400, 43200) (400,)\n",
      "trainset after (410, 43200) (410,)\n",
      "updated train set: (410, 43200) (410,) unique(labels): [107 303] [0 1]\n",
      "val set: (885, 43200) (885,)\n",
      "\n",
      "Train set: (410, 43200) y: (410,)\n",
      "Val   set: (885, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 41\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.341 s \n",
      "\n",
      "Accuracy rate for 95.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.95      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (885,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (885, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (410, 43200) (410,)\n",
      "trainset after (420, 43200) (420,)\n",
      "updated train set: (420, 43200) (420,) unique(labels): [111 309] [0 1]\n",
      "val set: (875, 43200) (875,)\n",
      "\n",
      "Train set: (420, 43200) y: (420,)\n",
      "Val   set: (875, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 42\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.877 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       200\n",
      "           1       0.97      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.97      0.96      0.96       400\n",
      "weighted avg       0.97      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (875,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (875, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (420, 43200) (420,)\n",
      "trainset after (430, 43200) (430,)\n",
      "updated train set: (430, 43200) (430,) unique(labels): [111 319] [0 1]\n",
      "val set: (865, 43200) (865,)\n",
      "\n",
      "Train set: (430, 43200) y: (430,)\n",
      "Val   set: (865, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 43\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.195 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       200\n",
      "           1       0.97      0.95      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (865,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (865, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (430, 43200) (430,)\n",
      "trainset after (440, 43200) (440,)\n",
      "updated train set: (440, 43200) (440,) unique(labels): [111 329] [0 1]\n",
      "val set: (855, 43200) (855,)\n",
      "\n",
      "Train set: (440, 43200) y: (440,)\n",
      "Val   set: (855, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 44\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.235 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       200\n",
      "           1       0.97      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.97      0.96      0.96       400\n",
      "weighted avg       0.97      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (855,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "probabilities: (855, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (440, 43200) (440,)\n",
      "trainset after (450, 43200) (450,)\n",
      "updated train set: (450, 43200) (450,) unique(labels): [113 337] [0 1]\n",
      "val set: (845, 43200) (845,)\n",
      "\n",
      "Train set: (450, 43200) y: (450,)\n",
      "Val   set: (845, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 45\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.276 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (845,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (845, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (450, 43200) (450,)\n",
      "trainset after (460, 43200) (460,)\n",
      "updated train set: (460, 43200) (460,) unique(labels): [115 345] [0 1]\n",
      "val set: (835, 43200) (835,)\n",
      "\n",
      "Train set: (460, 43200) y: (460,)\n",
      "Val   set: (835, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 46\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.623 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (835,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (835, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (460, 43200) (460,)\n",
      "trainset after (470, 43200) (470,)\n",
      "updated train set: (470, 43200) (470,) unique(labels): [118 352] [0 1]\n",
      "val set: (825, 43200) (825,)\n",
      "\n",
      "Train set: (470, 43200) y: (470,)\n",
      "Val   set: (825, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 47\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.658 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (825,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (825, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (470, 43200) (470,)\n",
      "trainset after (480, 43200) (480,)\n",
      "updated train set: (480, 43200) (480,) unique(labels): [120 360] [0 1]\n",
      "val set: (815, 43200) (815,)\n",
      "\n",
      "Train set: (480, 43200) y: (480,)\n",
      "Val   set: (815, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 48\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.448 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       200\n",
      "           1       0.96      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "val predicted: (815,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "probabilities: (815, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n",
      "trainset before (480, 43200) (480,)\n",
      "trainset after (490, 43200) (490,)\n",
      "updated train set: (490, 43200) (490,) unique(labels): [122 368] [0 1]\n",
      "val set: (805, 43200) (805,)\n",
      "\n",
      "Train set: (490, 43200) y: (490,)\n",
      "Val   set: (805, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 49\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.803 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.94      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [ 11 189]]\n",
      "--------------------------------\n",
      "val predicted: (805,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (805, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (490, 43200) (490,)\n",
      "trainset after (500, 43200) (500,)\n",
      "updated train set: (500, 43200) (500,) unique(labels): [126 374] [0 1]\n",
      "val set: (795, 43200) (795,)\n",
      "\n",
      "Train set: (500, 43200) y: (500,)\n",
      "Val   set: (795, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 50\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.527 s \n",
      "\n",
      "Accuracy rate for 95.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "final active learning accuracies [64.25, 83.25, 72.5, 98.25, 97.75, 97.5, 96.75, 97.75, 96.0, 98.0, 97.75, 99.0, 98.0, 97.5, 98.5, 97.5, 98.25, 98.0, 98.0, 97.25, 97.5, 97.0, 97.25, 97.25, 97.5, 97.5, 97.0, 97.5, 97.0, 96.75, 96.25, 97.5, 97.25, 97.25, 96.5, 97.0, 96.25, 96.25, 96.5, 96.0, 95.75, 96.5, 96.25, 96.5, 95.5, 95.0, 96.0, 95.5, 95.0, 95.25]\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "Count = 3, using model = LogModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
      "\n",
      "initial random chosen samples (10,)\n",
      "initial train set: (10, 43200) (10,) unique(labels): [5 5] [0 1]\n",
      "val set: (1285, 43200) (1285,) (10,)\n",
      "\n",
      "Train set: (10, 43200) y: (10,)\n",
      "Val   set: (1285, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 1\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.121 s \n",
      "\n",
      "Accuracy rate for 68.750000 \n",
      "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       200\n",
      "           1       0.77      0.53      0.63       200\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       400\n",
      "   macro avg       0.71      0.69      0.68       400\n",
      "weighted avg       0.71      0.69      0.68       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[169  31]\n",
      " [ 94 106]]\n",
      "--------------------------------\n",
      "val predicted: (1285,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1285, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[731 972 920 ... 399   5 432]\n",
      "trainset before (10, 43200) (10,)\n",
      "trainset after (20, 43200) (20,)\n",
      "updated train set: (20, 43200) (20,) unique(labels): [ 6 14] [0 1]\n",
      "val set: (1275, 43200) (1275,)\n",
      "\n",
      "Train set: (20, 43200) y: (20,)\n",
      "Val   set: (1275, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 2\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.169 s \n",
      "\n",
      "Accuracy rate for 79.000000 \n",
      "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.58      0.74       200\n",
      "           1       0.71      0.99      0.83       200\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       400\n",
      "   macro avg       0.85      0.79      0.78       400\n",
      "weighted avg       0.85      0.79      0.78       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[117  83]\n",
      " [  1 199]]\n",
      "--------------------------------\n",
      "val predicted: (1275,) [0 1 0 ... 1 1 1]\n",
      "probabilities: (1275, 2) \n",
      " [0 1 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 483  208  414 ... 1076  902 1267]\n",
      "trainset before (20, 43200) (20,)\n",
      "trainset after (30, 43200) (30,)\n",
      "updated train set: (30, 43200) (30,) unique(labels): [16 14] [0 1]\n",
      "val set: (1265, 43200) (1265,)\n",
      "\n",
      "Train set: (30, 43200) y: (30,)\n",
      "Val   set: (1265, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 3\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.188 s \n",
      "\n",
      "Accuracy rate for 93.250000 \n",
      "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       200\n",
      "           1       0.99      0.88      0.93       200\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       400\n",
      "   macro avg       0.94      0.93      0.93       400\n",
      "weighted avg       0.94      0.93      0.93       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [ 25 175]]\n",
      "--------------------------------\n",
      "val predicted: (1265,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1265, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 750 1160 1235 ...  294  119  392]\n",
      "trainset before (30, 43200) (30,)\n",
      "trainset after (40, 43200) (40,)\n",
      "updated train set: (40, 43200) (40,) unique(labels): [17 23] [0 1]\n",
      "val set: (1255, 43200) (1255,)\n",
      "\n",
      "Train set: (40, 43200) y: (40,)\n",
      "Val   set: (1255, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 4\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.358 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1255,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1255, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[997 537  98 ...  59 469  89]\n",
      "trainset before (40, 43200) (40,)\n",
      "trainset after (50, 43200) (50,)\n",
      "updated train set: (50, 43200) (50,) unique(labels): [21 29] [0 1]\n",
      "val set: (1245, 43200) (1245,)\n",
      "\n",
      "Train set: (50, 43200) y: (50,)\n",
      "Val   set: (1245, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 5\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.359 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       200\n",
      "           1       0.99      0.95      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "val predicted: (1245,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1245, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 645 1226 1059 ...  466  118   89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (50, 43200) (50,)\n",
      "trainset after (60, 43200) (60,)\n",
      "updated train set: (60, 43200) (60,) unique(labels): [23 37] [0 1]\n",
      "val set: (1235, 43200) (1235,)\n",
      "\n",
      "Train set: (60, 43200) y: (60,)\n",
      "Val   set: (1235, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 6\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.486 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1235,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1235, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 517 1105  222 ...   58   11   88]\n",
      "trainset before (60, 43200) (60,)\n",
      "trainset after (70, 43200) (70,)\n",
      "updated train set: (70, 43200) (70,) unique(labels): [27 43] [0 1]\n",
      "val set: (1225, 43200) (1225,)\n",
      "\n",
      "Train set: (70, 43200) y: (70,)\n",
      "Val   set: (1225, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 7\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.458 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       200\n",
      "           1       0.95      0.98      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (1225,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1225, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[217 972 484 ...  11  42  88]\n",
      "trainset before (70, 43200) (70,)\n",
      "trainset after (80, 43200) (80,)\n",
      "updated train set: (80, 43200) (80,) unique(labels): [33 47] [0 1]\n",
      "val set: (1215, 43200) (1215,)\n",
      "\n",
      "Train set: (80, 43200) y: (80,)\n",
      "Val   set: (1215, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 8\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.598 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       200\n",
      "           1       0.95      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (1215,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1215, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[580 961 894 ... 567  11  87]\n",
      "trainset before (80, 43200) (80,)\n",
      "trainset after (90, 43200) (90,)\n",
      "updated train set: (90, 43200) (90,) unique(labels): [33 57] [0 1]\n",
      "val set: (1205, 43200) (1205,)\n",
      "\n",
      "Train set: (90, 43200) y: (90,)\n",
      "Val   set: (1205, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 9\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.606 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       200\n",
      "           1       0.95      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1205,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1205, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 801  924 1187 ...   11   41   87]\n",
      "trainset before (90, 43200) (90,)\n",
      "trainset after (100, 43200) (100,)\n",
      "updated train set: (100, 43200) (100,) unique(labels): [34 66] [0 1]\n",
      "val set: (1195, 43200) (1195,)\n",
      "\n",
      "Train set: (100, 43200) y: (100,)\n",
      "Val   set: (1195, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 10\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.660 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       200\n",
      "           1       0.93      0.98      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.96      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[186  14]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1195,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1195, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 985 1029  591 ...   11   86   41]\n",
      "trainset before (100, 43200) (100,)\n",
      "trainset after (110, 43200) (110,)\n",
      "updated train set: (110, 43200) (110,) unique(labels): [37 73] [0 1]\n",
      "val set: (1185, 43200) (1185,)\n",
      "\n",
      "Train set: (110, 43200) y: (110,)\n",
      "Val   set: (1185, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 11\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.730 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       200\n",
      "           1       0.96      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1185,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1185, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 519 1048  541 ...   11   41   86]\n",
      "trainset before (110, 43200) (110,)\n",
      "trainset after (120, 43200) (120,)\n",
      "updated train set: (120, 43200) (120,) unique(labels): [39 81] [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val set: (1175, 43200) (1175,)\n",
      "\n",
      "Train set: (120, 43200) y: (120,)\n",
      "Val   set: (1175, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 12\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.692 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1175,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1175, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[122 978 400 ...  86  11  41]\n",
      "trainset before (120, 43200) (120,)\n",
      "trainset after (130, 43200) (130,)\n",
      "updated train set: (130, 43200) (130,) unique(labels): [42 88] [0 1]\n",
      "val set: (1165, 43200) (1165,)\n",
      "\n",
      "Train set: (130, 43200) y: (130,)\n",
      "Val   set: (1165, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 13\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.743 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1165,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1165, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[1070  805 1030 ...   11   41   86]\n",
      "trainset before (130, 43200) (130,)\n",
      "trainset after (140, 43200) (140,)\n",
      "updated train set: (140, 43200) (140,) unique(labels): [43 97] [0 1]\n",
      "val set: (1155, 43200) (1155,)\n",
      "\n",
      "Train set: (140, 43200) y: (140,)\n",
      "Val   set: (1155, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 14\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.955 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       200\n",
      "           1       0.95      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1155,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1155, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 76 475 649 ... 279  86  41]\n",
      "trainset before (140, 43200) (140,)\n",
      "trainset after (150, 43200) (150,)\n",
      "updated train set: (150, 43200) (150,) unique(labels): [ 50 100] [0 1]\n",
      "val set: (1145, 43200) (1145,)\n",
      "\n",
      "Train set: (150, 43200) y: (150,)\n",
      "Val   set: (1145, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 15\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.015 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1145,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1145, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[997 706 843 ...  11  41  85]\n",
      "trainset before (150, 43200) (150,)\n",
      "trainset after (160, 43200) (160,)\n",
      "updated train set: (160, 43200) (160,) unique(labels): [ 52 108] [0 1]\n",
      "val set: (1135, 43200) (1135,)\n",
      "\n",
      "Train set: (160, 43200) y: (160,)\n",
      "Val   set: (1135, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 16\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.077 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1135,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1135, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[606 597 529 ...  79  41  85]\n",
      "trainset before (160, 43200) (160,)\n",
      "trainset after (170, 43200) (170,)\n",
      "updated train set: (170, 43200) (170,) unique(labels): [ 52 118] [0 1]\n",
      "val set: (1125, 43200) (1125,)\n",
      "\n",
      "Train set: (170, 43200) y: (170,)\n",
      "Val   set: (1125, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 17\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.142 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       200\n",
      "           1       0.96      0.98      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  4 196]]\n",
      "--------------------------------\n",
      "val predicted: (1125,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1125, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[168 761 985 ... 273  85  41]\n",
      "trainset before (170, 43200) (170,)\n",
      "trainset after (180, 43200) (180,)\n",
      "updated train set: (180, 43200) (180,) unique(labels): [ 55 125] [0 1]\n",
      "val set: (1115, 43200) (1115,)\n",
      "\n",
      "Train set: (180, 43200) y: (180,)\n",
      "Val   set: (1115, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 18\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 0.983 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1115,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1115, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 265 1048  532 ...  271   41   84]\n",
      "trainset before (180, 43200) (180,)\n",
      "trainset after (190, 43200) (190,)\n",
      "updated train set: (190, 43200) (190,) unique(labels): [ 58 132] [0 1]\n",
      "val set: (1105, 43200) (1105,)\n",
      "\n",
      "Train set: (190, 43200) y: (190,)\n",
      "Val   set: (1105, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 19\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.252 s \n",
      "\n",
      "Accuracy rate for 97.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1105,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1105, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[981 546 871 ...  56 119  84]\n",
      "trainset before (190, 43200) (190,)\n",
      "trainset after (200, 43200) (200,)\n",
      "updated train set: (200, 43200) (200,) unique(labels): [ 59 141] [0 1]\n",
      "val set: (1095, 43200) (1095,)\n",
      "\n",
      "Train set: (200, 43200) y: (200,)\n",
      "Val   set: (1095, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 20\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.078 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1095,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1095, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[1068  802  714 ...  119  269   84]\n",
      "trainset before (200, 43200) (200,)\n",
      "trainset after (210, 43200) (210,)\n",
      "updated train set: (210, 43200) (210,) unique(labels): [ 61 149] [0 1]\n",
      "val set: (1085, 43200) (1085,)\n",
      "\n",
      "Train set: (210, 43200) y: (210,)\n",
      "Val   set: (1085, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 21\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.390 s \n",
      "\n",
      "Accuracy rate for 96.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1085,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1085, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[938 583 866 ...  78 268  84]\n",
      "trainset before (210, 43200) (210,)\n",
      "trainset after (220, 43200) (220,)\n",
      "updated train set: (220, 43200) (220,) unique(labels): [ 63 157] [0 1]\n",
      "val set: (1075, 43200) (1075,)\n",
      "\n",
      "Train set: (220, 43200) y: (220,)\n",
      "Val   set: (1075, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 22\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.671 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       200\n",
      "           1       0.99      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[198   2]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1075,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1075, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[836 674 403 ... 118  83  41]\n",
      "trainset before (220, 43200) (220,)\n",
      "trainset after (230, 43200) (230,)\n",
      "updated train set: (230, 43200) (230,) unique(labels): [ 65 165] [0 1]\n",
      "val set: (1065, 43200) (1065,)\n",
      "\n",
      "Train set: (230, 43200) y: (230,)\n",
      "Val   set: (1065, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 23\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.654 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1065,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1065, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[659 858 673 ... 118  83  41]\n",
      "trainset before (230, 43200) (230,)\n",
      "trainset after (240, 43200) (240,)\n",
      "updated train set: (240, 43200) (240,) unique(labels): [ 66 174] [0 1]\n",
      "val set: (1055, 43200) (1055,)\n",
      "\n",
      "Train set: (240, 43200) y: (240,)\n",
      "Val   set: (1055, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 24\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.411 s \n",
      "\n",
      "Accuracy rate for 97.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       200\n",
      "           1       0.98      0.97      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1055,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1055, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ 65 772 208 ...  83  41 118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (240, 43200) (240,)\n",
      "trainset after (250, 43200) (250,)\n",
      "updated train set: (250, 43200) (250,) unique(labels): [ 71 179] [0 1]\n",
      "val set: (1045, 43200) (1045,)\n",
      "\n",
      "Train set: (250, 43200) y: (250,)\n",
      "Val   set: (1045, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 25\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.643 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  5 195]]\n",
      "--------------------------------\n",
      "val predicted: (1045,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1045, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[270 709 536 ... 117  41  82]\n",
      "trainset before (250, 43200) (250,)\n",
      "trainset after (260, 43200) (260,)\n",
      "updated train set: (260, 43200) (260,) unique(labels): [ 76 184] [0 1]\n",
      "val set: (1035, 43200) (1035,)\n",
      "\n",
      "Train set: (260, 43200) y: (260,)\n",
      "Val   set: (1035, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 26\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.869 s \n",
      "\n",
      "Accuracy rate for 97.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       200\n",
      "           1       0.98      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.98      0.97      0.97       400\n",
      "weighted avg       0.98      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196   4]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (1035,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1035, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[697 300 466 ...  11  82 116]\n",
      "trainset before (260, 43200) (260,)\n",
      "trainset after (270, 43200) (270,)\n",
      "updated train set: (270, 43200) (270,) unique(labels): [ 77 193] [0 1]\n",
      "val set: (1025, 43200) (1025,)\n",
      "\n",
      "Train set: (270, 43200) y: (270,)\n",
      "Val   set: (1025, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 27\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.836 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1025,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1025, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[610 500 718 ...  41  82 116]\n",
      "trainset before (270, 43200) (270,)\n",
      "trainset after (280, 43200) (280,)\n",
      "updated train set: (280, 43200) (280,) unique(labels): [ 78 202] [0 1]\n",
      "val set: (1015, 43200) (1015,)\n",
      "\n",
      "Train set: (280, 43200) y: (280,)\n",
      "Val   set: (1015, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 28\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.963 s \n",
      "\n",
      "Accuracy rate for 98.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       200\n",
      "           1       0.98      0.98      0.98       200\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       400\n",
      "   macro avg       0.98      0.98      0.98       400\n",
      "weighted avg       0.98      0.98      0.98       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (1015,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1015, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[347 866 456 ...  41  82 116]\n",
      "trainset before (280, 43200) (280,)\n",
      "trainset after (290, 43200) (290,)\n",
      "updated train set: (290, 43200) (290,) unique(labels): [ 80 210] [0 1]\n",
      "val set: (1005, 43200) (1005,)\n",
      "\n",
      "Train set: (290, 43200) y: (290,)\n",
      "Val   set: (1005, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 29\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.720 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (1005,) [0 0 0 ... 1 1 1]\n",
      "probabilities: (1005, 2) \n",
      " [0 0 0 ... 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[106 906 836 ...  11  82 116]\n",
      "trainset before (290, 43200) (290,)\n",
      "trainset after (300, 43200) (300,)\n",
      "updated train set: (300, 43200) (300,) unique(labels): [ 83 217] [0 1]\n",
      "val set: (995, 43200) (995,)\n",
      "\n",
      "Train set: (300, 43200) y: (300,)\n",
      "Val   set: (995, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 30\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.031 s \n",
      "\n",
      "Accuracy rate for 97.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       200\n",
      "           1       0.96      0.98      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  3 197]]\n",
      "--------------------------------\n",
      "val predicted: (995,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (995, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[686 754 294 481  56 969 861 642 587 524 974 752 819 692 540 554 289 373\n",
      " 887 939 491 492 581 710 937 854 509 227 573 525 814 944 516 240 603 966\n",
      " 530 247 630 486 988 690 878 730 580 265 617 599 831 985 346 447 793 598\n",
      " 620 614  53 584 765 164 958 504 820 744 547 855 922 824 735 394 510 965\n",
      " 677 667 501 713 652 427 672 805 750 107 533 947 898 778 942 282 521 477\n",
      " 588 328 904 625 545 440 770 586 991 764 291 542 479 864 173 789 949 801\n",
      " 157 821 767 800 811 670 319 651 655 467 862 701 248 361 561 699 941 825\n",
      " 327 832 860 500 994 837 980 388 101 810 918 763 681 929 812 636 901 520\n",
      " 605 702 295 900 564 474 549 727 740 153 983 972 850 940 746 412 301 612\n",
      " 685 582 606 395 781 615  44 957 613 513 208 876 333 719 556 192 252 894\n",
      " 662 933 938 952 487 953 484 631  71 109 758 777 718 970 932 971 539 469\n",
      " 310 903 756 943 254 331 663 569 747   8 238 579 186 848 741 560 382 843\n",
      " 839 517 209 502 954 566 604 494 589 526 466 849 773 935 534 120 523 223\n",
      " 595 275 597 132 664 905 769 782  13 429 489 671 106 609   5 694 623 512\n",
      " 853 706 865 678 396 260 870 318 892 535 674 537 453 557 577 948 691 802\n",
      " 791 830 703 406 352 734 411 698 728 354 553 968 645 211 881 299 619 771\n",
      "  20 732   6  27 695 872 532 493 585 451 787 137 660 976 717 856 627 578\n",
      " 470 558 370 592 196 780 480 882 823 761  29 611 836 688 875 762 232 518\n",
      " 930 928 465 945  62 673 724 403 100 982 956 457 709 891 827 729 852 315\n",
      " 883 432  43 742 496 472 169 920 326 450 643 707 725 402 736 360 714 463\n",
      " 347 869 990 924 264 482 418 840 962 668 708 653 906 833 204 300 693 552\n",
      " 600 214 546 680 775 356 365 548 649 298 987 217  86 909 323 529 841 381\n",
      " 234 559 170 607 133 273 263 125 506 616 230 641 721 857 658 798 507 538\n",
      " 923 927 570 926 287 471 822 866 393 925  26 624 508 884 456 656 146 433\n",
      " 858 437 235 829 955 745 213 689 572 796 733  73 626 863 749 911  76 233\n",
      " 766 497 650 815 934 536 567 541 551 185 575 608 847 229 543 978 171  69\n",
      " 743 343 362 410 441 274 189 158 977 303 640 684 355 462 590 975 116 478\n",
      " 902 511 799 897  85 565 459 803 648 910  47 992 239 792 129 835 753 618\n",
      " 461 144 405 720 963  48 731 159 531 278 711 914 888 458 134 269 845 737\n",
      "  41  18 460 241 716  24 102 634 973 449  65 270 774 218 885  84 424 739\n",
      " 788 259 629 602 931 622  93 436  33 445 419  68  52 255  82 392 808  64\n",
      " 908 407 464 138 936 899 425 877 804  91 364  87 574 330 221 222 290 398\n",
      " 621 423 386 351  21 871 628 583 444 112  42 779 375 452 544 842 350 321\n",
      " 723  95 700 844 193   0 760 809 657 288 654 568 880 473 297 485 859 984\n",
      " 794 527 199 168 951 895 446 726  19 591 163 366  92   1 175  17 219 224\n",
      " 338 946 632 818 964 772 384 555 635  66 498 325 205 675 438 334 610 313\n",
      " 786 868 161 795 950 915 131 751 738 576  30 277 682 571 834 475 669 886\n",
      " 296  14 251 647 188 431 666 426 210 272 959 123 162 344 704 921 184 563\n",
      " 322 377 167 115 960 380 851 166 519 644 912 415 638  11 136 807 363 596\n",
      " 216 874  49 397 243  94 784  67 246 442 250 476 401 292 118 817 967 712\n",
      " 593  57   2 633 376 879  32 907 197 499  61  88  60 683 226 317 128 528\n",
      " 399 206 249 715 182 505 759 372 919 339 867  23  35 594 262 562 122 385\n",
      " 665 637 404 454 271 194 455 659 253 154 357 314 896 358 104 443 150 307\n",
      " 434 468 139 293 390 755   9 143 428  90 785 503 306 913 550 601 514 748\n",
      " 309 981  70 422 190 148  55 989 639 215 816 191 687 225 522 697 889 110\n",
      " 268 198 172 245 279 379 149 661 757 105 421 435 416 448 722 179  36 806\n",
      " 341  45 413 378 768 160 152 174 979 790  46 873 490 212 387 155   3 281\n",
      " 302 312 646 400 838 244 127  96 828 183 126 705 776 261 348 383 335 286\n",
      " 488 893 916 181 371 304 439  78 414  28   4 256 156  74 337 369 151 283\n",
      " 145 961 993 228  22 676  99 220 797  37 986 332 430 389 679 342 367  59\n",
      " 280 285 130 324 826 165 846 202 142 417 813  58  16 236 267  12  77 180\n",
      "  97 121 329  34 368  38  80 353 783 231 917  72 359 177 237 187 320 890\n",
      " 515 284   7  89 696 483 266  79 207  39 117 242 349 495 124 257 311  15\n",
      " 103 276 178 113 141  50 203 345 374  31  51 408 391 147 111 420  63 305\n",
      "  54 108 176 135 201 140 336 340 316  75 258 409  25 119 195 308  83 200\n",
      "  98  40  10 114  81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (300, 43200) (300,)\n",
      "trainset after (310, 43200) (310,)\n",
      "updated train set: (310, 43200) (310,) unique(labels): [ 85 225] [0 1]\n",
      "val set: (985, 43200) (985,)\n",
      "\n",
      "Train set: (310, 43200) y: (310,)\n",
      "Val   set: (985, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 31\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.095 s \n",
      "\n",
      "Accuracy rate for 96.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       200\n",
      "           1       0.96      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.97      0.96      0.96       400\n",
      "weighted avg       0.97      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (985,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (985, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[371 846 536 569 811 806 598 895 488 685 928 288 762 737 550 483 344 152\n",
      " 964 930 878 823 163 517 594 817 612 975 246 829 556 666 576 803 920 580\n",
      " 728 625 610 649 577 770 293 847 609 541 526 593 745 757 723 489 290 239\n",
      " 521 501 692 812 601 785 804 883 973 855 683 498 529 475 792 932 484 889\n",
      " 477 392 957 949 185 445 615 960 733 237 797 583  44 156 940 131 816 506\n",
      " 560 956 281 264 984 739 909 661 938 324 656 325 848 386 172 891 557 543\n",
      " 510 507 247 923 326 793 924 824 467 438  53  13 913 106 978 781 664 513\n",
      " 750 726 425 711 552  99 531 935 742  29 108 867 226 853 299 962 308 573\n",
      " 740 380 578 478 582 981 562   8 841 703 831 470 465 530 317 755 599 105\n",
      " 518 222 748 207 933 934 945 608 535 939 756 892 695 394 657 607 802 538\n",
      " 774 707 620 948 970 921 404  70 966 679 253 869 499 688 195 681 553 773\n",
      " 861 813 464 191 545 696 590 794 410 497 455 842 637 931 251 509 772 872\n",
      " 272 493 168 961  48 761 850 725 427 743 706 885 575 832 694 514 231 675\n",
      " 713 667 822 967 897 769 259 819 352 592 430 684 409 505 345 626 654 600\n",
      " 721 947 710 136 549 100 835 844 840 491 169 359 779 341 461 490 699 360\n",
      " 298 532 274 350 668 184 849 208 472 342 313 874 587 658 544 393 548 665\n",
      " 533 606  85  26 936 672 584 391 845 929 863 602 119 565  27 700 944 825\n",
      "  43 469 753 542 639 316 574 902 449 729 368 734 814 714 722  20 754 647\n",
      " 358 646 277 486  64 671 645 674 759 946 718 554 321 687 614 618 463 563\n",
      " 210 720 702 622 329 631 852 953 790 331 854 911 468 922 481 919 448 717\n",
      " 856 170 783 435 528 866 839 727   6 815 795 262 273 636 128 263 900 915\n",
      " 828 212 977 873 611 765 917 363 460 791 709 959 561 927 916 712 269 896\n",
      "   5 254 157 228 232 619 451 925 145 980 763 522 403 868 515 581 968 373\n",
      " 954 234 691 894 534 400  47  33 233 585 566 520 401 701 479 875 158 827\n",
      " 586 213 353 971  75 348  91 439 860 882 857  61 686 503 629 886 796 570\n",
      " 630 537 450  18 539 766 905 752 767 431 457 240 101  67 216 476 132 764\n",
      " 217 643   1 876 943 296 459 276 926 568 390 972 555 408 203 124 604 744\n",
      " 595 648 258 143  63 508 624 354 893 982 800 221 301 788 642 196 571 662\n",
      " 416 379 494 220 442 652 732 504 133 746 328 644  72 704 447 423  68 906\n",
      " 650 525 621 758 871 396 603 579 735  19 111 682 551 115 821 429 456 888\n",
      "  52 192 286 375  17 941 297 362 807  65 974 382 421 833 890 613 965 130\n",
      " 736   0 417 914 597 471 443 444 780 137 121 349 149 188 292 836 942 731\n",
      " 268 434 738 205 319 786 167 724 405 364 287  14 311 918 678 810  21 295\n",
      " 271 458 384 879 162 730 837 690 963 229 635 572 776 218 799 899 323 117\n",
      " 870 523  92 617 127 865  94 616 540 436  90 898 473  24 204  30 623  81\n",
      " 663 294 215 238  87 693 160 516 223 660  49  42 527 474 462 651 784 877\n",
      " 397 209 440 834 955 632 399 103 502  41 653 937 858 901 242  93 559 291\n",
      " 669 588 187 633 198 950 320 676 638 903  11  86 424 482 826 174 801 605\n",
      " 627  84 355 862 426 225 641 135 912 193 361 250 771 422 655 454 161 716\n",
      " 395 628 374 376 697 564 248 166   2 547  35 558 178 778 183 496 719 787\n",
      " 356 402 859 165 336 851 332 252  66 315 249 441 245  83 768 567 809 378\n",
      " 500 420  55 432 715 138 698  89 705 670 289 546 114 591 122 951 747  56\n",
      " 677 511 267 452 589 159 446 260 958 843 305 751 820 495 388 904 519 453\n",
      "  32  69 153 808 383 377 433 880 884 109 524 224 976 596 741 680 910 659\n",
      " 419 144 126 634   9 979  57 214 887 142 190 782 346 370 798 640 211 244\n",
      " 181 983  60 151 760 413  95 310 243 197 270 173  37 337 312 777 708 414\n",
      " 148 398 367 969 261 339  36 466 180 749 864 189 154 369 280 227 385 335\n",
      " 219 485   3  45 300  59 838  77 278 179 255 285 412 304 125 302 182 155\n",
      " 322  23 150 147 141 327 340   4 437 381 171 775 387  98 282 512 104  28\n",
      " 201 333 236 952 365 673 830  88  58  22  16 411 428 186   7 689 789 415\n",
      " 881 176 164 487  79 307 366 279  12 330  46 123 372  73 907 202 818 805\n",
      " 256 129 908  76 284 120 116  71  96 357 230 266  34 318 492 351 235  38\n",
      " 480 283 206 177 112 102 406 140 275 110 389  39 343 265  78 134  31  50\n",
      " 347 241  51  54 407 418 309 175 314  62 303 146 139 338 107 200 334  74\n",
      " 306  25 118 257 194  15 199  82  97  10  40  80 113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (310, 43200) (310,)\n",
      "trainset after (320, 43200) (320,)\n",
      "updated train set: (320, 43200) (320,) unique(labels): [ 86 234] [0 1]\n",
      "val set: (975, 43200) (975,)\n",
      "\n",
      "Train set: (320, 43200) y: (320,)\n",
      "Val   set: (975, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 32\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.132 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       200\n",
      "           1       0.96      0.97      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (975,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (975, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[918 731 925 815 656 838 686 482 604 913 766 391 779 538 590 717 547 576\n",
      " 326 661 519 607 524 910 152 246 809 705 965 756 504 805 325 281 874 954\n",
      " 264 791 733 846 697  13 226 466 589 572 288 172 939 707 816 444 869 499\n",
      " 496 290 937 734 722 511 578 920 610 739 808 573 644 487 751 106 821 947\n",
      " 787 476 239 620 804 317 344 880 463 605  70 844 596 222 839 843 163 515\n",
      " 858 744 540 247 385 764 679 559  99 299 464 806 797 974 923 720 553 721\n",
      " 928 763 495 938 968  44 786 379 554 474 935 767 131 963 659 491 882 437\n",
      " 914 727 775 557 674 755 516 678 477 594 105 508  53 737   8 579 569 651\n",
      " 535 529 670 666 823 748 946 505  29 911 298 424 574 527 952 796 602 393\n",
      " 957 324 603 156 930 549 934 662 237 448 438 750 901 136 259 798 924 528\n",
      " 950 632 469 860 652 601 834 253 195 232 700 899 689 649 520 308 541 108\n",
      " 489 231 753 168 293 971 832 207 409 345 833 922 704 331 512 329 903 641\n",
      " 653 960 615 350 626 471 784 210 660 359 951 360  85 488 542 688 119 272\n",
      " 595 408 921 663 533 854 788 827 640 274 358 503 483 550 269 497  20 462\n",
      " 883 588 836 723 581 403 742 736 586 824 191 814 518 621 392 185 642 712\n",
      " 773 399 876 863 693 706 857 708 749 811  48 634 970 546 613 690 286 714\n",
      " 747 507 363 526 852 956 638 917 571 719 777 728 570  61 454 681 400 892\n",
      " 682 450 715 847 912 352 552 143  27 460 480 545 532 905 580 873 390 478\n",
      " 919 716 297 562 631 738  26 667 768 100 694 539 866 929 865 485 597 759\n",
      " 599 273 467 468 341 606 696 746 885 234 701 967 864 790 271 583  68 676\n",
      " 531 933 949 828 831 593  47 378 577 458 558 887 761 961 726 426 906 434\n",
      " 820 502 609 170 915 263 591 669 429 625 228 459 817 184 890  64 916 536\n",
      "   5 730 614 389 904 313 859 456 523 551 128 837 926 565 447 841 169 958\n",
      "  75 936 422 501 213 402 560 233 685 944  63 845 368 617 758 943 470 884\n",
      " 124 703 301 757 639 132 785 879 637 494 513 896 886  24 677 530  72 158\n",
      " 807 782 695 321 262 353 909 563  91 780   1 167 254 196 316 840 598 877\n",
      " 188 258 729  21 645 643 862 825 548 319 627 372 276  41 435 506 294 342\n",
      " 612 895 457 647 673 624 711 800  14 760  33 819 212 813 657 962 725 240\n",
      " 932 296 442 268 752 789 848 907 500 851   6 795 724 354 101 430 972 867\n",
      " 881 203 348 953 908  43 439  81 407 680 111 608 145 568 664 415 567 534\n",
      " 433 216 853 383  52 793 449 770 137 648 446 616 121 732 492 443 157 628\n",
      " 277  67  19 792 204 794 395 473 208 115 889 566 328 441 575 420 778  87\n",
      " 364 544 217 475 229 289 774 215 521 655 826 870 940 394  90 740 472 803\n",
      " 927 698 582 251 955 537 633 893 931 829 650 238 850 455 130 525 311 481\n",
      " 691 250 636 349 362 600 888 398 205 964 718  17 381 440 198 561 966 315\n",
      " 332 818 765 404 762 630 103  42 149  86 453 658 248 223 133   2 622 221\n",
      " 902 323 295 684 218 117 416 713 452 421   0 220 356 735 425  65 451 665\n",
      " 945 461 891 165 187 856  92  18 174 687 772 336 514 623 267 142 431 611\n",
      " 646 396 423 374 126 776 418  84 812 270  94 153  11 361  69 252 412 522\n",
      " 619 192 493 842 699 584  83 710 618 671 122 305  56 556 861 377  89  32\n",
      " 564 287 428 555 138  93 692 209 868 745 114 166 244 249 355 162  59 855\n",
      "  35 900 672 370 959  95 401 387 875 878 225 291 781 587 498 373 654  49\n",
      " 973 849 709  66 178 941 337 193  30 419 339 159 702 320 197 969 280 771\n",
      " 148  45 675 376 413 894 161 635 397 517 465 292  60 830 183 871 260  23\n",
      " 310 741 278 629 160 382 754 171 127 802 135 509 835 180 375 173 543 243\n",
      " 144 224 190 335   9 585 285  55  37 245 109 801 743 592 380 242  57 948\n",
      " 432 307 261 445 411  22 304 214  73 410 333 300 436 386 151 125 484  77\n",
      " 150 769   3  98 384 181 346 369 211 302 189 822 367 810  46   4 312 219\n",
      "  36 683 872 942 155 322 154 365 327 898  28 282 179 255 147 897 141  88\n",
      " 230 182 202  79 357 279 104 201 414 486   7 427 116 120 227 176 330 371\n",
      " 510 164  16 102 283 783 340 123  34 235 668 351 129 799 318 284  12 236\n",
      "  76 479 206 266  96 177 110  50  38 366 275 186 388  71 343  58 309 490\n",
      " 265 241 112  31 140 417  39 256  51 405  78 303  62 347 134 107 338 139\n",
      "  15 314 200 334 146  74  54 175 257 194 118 306 406  25 199  82  97  10\n",
      "  40  80 113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (320, 43200) (320,)\n",
      "trainset after (330, 43200) (330,)\n",
      "updated train set: (330, 43200) (330,) unique(labels): [ 86 244] [0 1]\n",
      "val set: (965, 43200) (965,)\n",
      "\n",
      "Train set: (330, 43200) y: (330,)\n",
      "Val   set: (965, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 33\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.100 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.96      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[195   5]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (965,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "probabilities: (965, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[152 929 288 546 675 903 264 325 713 658 618 751 839 503 495 608 953 537\n",
      " 293 799 605 696 247 815 466 514 518 701 734  99 246 344 920 486 804 156\n",
      " 575 944 589 786  65 571 498 476 649 588 424 553 290 539 572 955 391 593\n",
      " 837 791 556 862 810 317 937 603 739 774 523 940 464  44 667 746 595 326\n",
      " 558 851 663 876 793 642 541 281 758 913 947 684 748 477 693 914 832 792\n",
      " 729 403 474 718 527 875 602 548 743 924 906 936 191 770 222 639 239 163\n",
      " 762 629 671 510 659 925 847 226  48 444   8 552 828 577 390 827 676 490\n",
      " 573 350 958 515 761 817 856 469 818 638 534 299 504 942 519 393 867 172\n",
      " 664  70 507 448 951 131 482 715  13 237 601 909 106 964 918 502 308 779\n",
      " 911  29 210 673 437 759 253  53 656 783 907 873 830 262 710 579 750 700\n",
      " 100 426  27 379 723  20 685 717 207 409 324 487 928 208 496 946 802 704\n",
      " 568 231 800 915 108 728 745 904 385 272 526 408 892 578 744  26 833  85\n",
      " 724 619 960 636 517 731 756 716 885 596 463 660 912 889 585 360 532 185\n",
      " 782 686 195 454 630 624 632 494 857 747 462 488 269 834 651 358 657 259\n",
      " 359 640 894 538 719 395 689 961 212 549 298 467 168 612 229 471 869 732\n",
      " 561 363 512 447 368 950 234 679 905 480 781 468 803 899 613 690  43 157\n",
      " 594 840 845 908 540 754 600 250 941 858 276 274 313 826 456 132 878 682\n",
      " 136 752 879 329 407 457 524 611 392 564 316 263 528 582 105 919 345 184\n",
      " 615 806 763 570 545 301 331 742 737 768 587 551 933 544 785 707  18 699\n",
      " 286 228 866 352 506 557 119 841 569 124 870 880 733 708 511 898 853 753\n",
      " 434 233  75 137 795 927 530 169 836 450 741 438 529 934 472 525 772 399\n",
      "  63 962  64 647 531 277 354 703 429 598 691 430  68 697 550 415 712  61\n",
      " 522 916   6 484 711 780 896 831 897 372 722 203 811 321 341 674 562 726\n",
      " 218  91 101 666 475 145 735 692 592 383 353 135 607 580 755  93 809 872\n",
      " 939 251  14 625 232 838 220 604 567 883 273 460 287 453 821 576 547 459\n",
      " 559 650 133 635  72  21 819 670 470 435 623 143 348 294 654 852 855 859\n",
      " 499 478   5 814 769 643 389 130 500 481 398 433 501 381 678 813 491 295\n",
      " 850 166 721 254 702 888 825 400 945 158 493 877 505 720  24 801 167 374\n",
      " 709 948 614 923  33 128 520 846  30 449 628 323 645 881 597 900  11 620\n",
      " 790 574 581 387 637 240  47 760 757 727 590  42 644 952 661 115 822 204\n",
      " 404 767 922 606 844 364 402 170 694 901  81 921  52 451 648 902 216  17\n",
      " 957 261 533 765 431 882 215 209  87 874 789 473 416 565 142 271 860 910\n",
      "  67 808 425 513 336 823 926 609 297 149 458 421 161 631 886 420 221 373\n",
      " 159 440 956 777  95 622 442 193 798 954 361 362 242 812 655 807 111 375\n",
      " 248 446 776  86 428 439 634 378  90 688 917 289 687 394 296 455 117  92\n",
      " 560  83 349 621 422 217 566 328 355 843 943 771 725 884 320 820 319 126\n",
      " 536   9 173 188   1 714 617 784 730 706  49 535 788 773 583 677 258 153\n",
      " 187 213 223 854  19 342 521  41 641 871 315 849 196 205 443  35 610 109\n",
      " 291 356 775  66 165 736 861 662  55 121 432 586 160 599 930 268 835 332\n",
      "  84 122 893 787 267 935 441 418  94 492   0 555 705 270 211 931 681 963\n",
      " 452 245  60 652 401   3 949 151 365 190 178 103 508 311 653 626 280 864\n",
      " 829 127 162 396 198 683 171 863 633 238 114 668 646 461 627 382 370  69\n",
      " 423 419 848 302 669 516 304 174 411 192 305 340 292 497 183 842 868 895\n",
      " 138 554 584  59 542 333 445 377 672 410 243  56  89 749 255 740  37 225\n",
      " 252 887 197 824 465  77 176   2 219 563 959 543 616 764 380  45 376 938\n",
      " 150 339  36 144 766 244 412 698 181 738 816 591 346  79 164 214  32   4\n",
      " 371 189 367 285 796 386 695 148  23 483 680 249 310 312 413 509 224  16\n",
      "  12 266  57 337 154 335 147 322 797 890 330 182 397 485 180 427 891 778\n",
      " 369 278  98 865 155 230 932 805  46  58  73  28 260  22 120 794 307  51\n",
      " 125 327   7 665 384 201 300 236 104  34 179 436 366 200 388 279 282  78\n",
      " 275 123 489 284  76 102 414 141  88 146 186 479  71 347 357 318 227 110\n",
      " 235 265 116  38 140 177 417 112 134 129  96 175 202  31 351  50 303 343\n",
      " 309  39 283 314  62 139 256  54 338 406 405 241 206  15  10  97 306 334\n",
      " 107  25 118  74  82 194 257 199  40 113  80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (330, 43200) (330,)\n",
      "trainset after (340, 43200) (340,)\n",
      "updated train set: (340, 43200) (340,) unique(labels): [ 90 250] [0 1]\n",
      "val set: (955, 43200) (955,)\n",
      "\n",
      "Train set: (340, 43200) y: (340,)\n",
      "Val   set: (955, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 34\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.273 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.95      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (955,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (955, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[499 506 927 472 533 809 796 743 514 162 686 570 637 945 519 766 535 613\n",
      " 494 726 669 556 584 696 511 598 486 785 831 731 287 600 566 246 926 934\n",
      " 515 709 721 314 868 694 837 829 603 460 583 386 843 498 710 634 791  29\n",
      " 491 553 854 778 644 597 225 824 340 470  70 795 482 865   8 754  53 131\n",
      " 462 548 458 689 420 679 590 290 245 537  44  85 510 742 895 522 463 588\n",
      " 930 937 783 646 563 595 653 221 296 322 388 106 677 567 905 337 720 771\n",
      " 738 948 784 951 751 440 503 658 753 859 325 832 346 387 270 171 279 321\n",
      " 954 867 916 884 735 886 492 897 523 547 551 589 572 651 399 500 794 839\n",
      " 574 919 305 236 940 433 802 568 459 513 194 483 911 818 896 716 631 736\n",
      " 484 825 701 530 425 902 724 775 467 774 819 573 258 473 575 661 906 543\n",
      " 750 544 374 715 465 478 740 209 232 298 507 633 105 762 155 614 295 900\n",
      " 773 596 693 541 375 909 524 822 672 238 645 904 708 534 915 807 943 737\n",
      " 813 230 444 792 430  99 108 443 729 389 703 364 536 861 941 936 845 190\n",
      " 820 678 668 877 252 888 274  43 745 359 231 654 848 405 903 667 826 450\n",
      " 828 398 748 697  20 618 723 627 253 261 657  47 381 206 801 858 577 476\n",
      " 143 434 810 211 652 100 923 932 907 931 660 395 495 685 692 169  65 571\n",
      " 580 456 582 474 862 755 760 341 528 355 540 910 849 793 262 683 128 207\n",
      " 520 665 707 480 624 348 327 872 204 490 833 725 385 847 733 545 606 913\n",
      " 890 607 564 881 599 798 746 157 119 705 183 187 871 271 354 870 950 671\n",
      " 502 464 817  64 619 711 714 642 912 695 800 521 546 452 947 518 772 894\n",
      " 704 294 136 184 602 610 167 593 630 591 404 682 316 318 852 227 608 747\n",
      " 739 257 803 823 449  61 777 525 552 469 496 422 250 851 734 501 542 744\n",
      "   1  13  48 272 360  21 891 892 635 426 675 850 684 565 269 111 526 761\n",
      " 267 620 203  27 412 239 356 625 471 527 769 145 358 313 917  92 875 914\n",
      " 855 396 391 929 585 918 718 344 700 379 864 212 455 559 103 217 429  91\n",
      "   5 664 924 806  19 649  63  30 446 938 830 310 576 284 587 749 431 216\n",
      " 842 442 604 508 208 899 411 757 215  68  26 233 670 529 214 466 617 713\n",
      " 249 901 838 403 952 293  52 690 557 764 497 880 417 836 224 776 767 811\n",
      " 805 532 944 632 454 435 898 156 390 780 228 219 942 638 531 124 222 623\n",
      "  41 292 368  87 468 308 561 418 643 370 869 202  42 516  49 804 164 191\n",
      " 137 489 266 132 790 149 889 554 126 640 844 302 438 158  90 639 152   6\n",
      " 719 173 161 447  18 655 117 569 946  86 275 866  14 394 765 814  93 439\n",
      " 350 291 727 841 779 197 605 101 237  32 345  81  33 933  11 352 609 251\n",
      " 436 487 722 437 687 477  24 130 135 509 893 636 935 338 759 874 351  75\n",
      " 815 592 328 453 752 562 445 320 787 324 133 349 717  72 782 612 925 656\n",
      " 195 220 421 626 878 392 241 165 373 781 581   0 248   2 289  83 712 827\n",
      " 876 168 706 121 615 560 451  37 285 377 127 317  67 109 419 115 641 244\n",
      " 621 142 286 400 648 835 674 427 647 393 594 457 539 908 196 763 265 383\n",
      " 210 953  95 550 428 335 243 223 138 166 151 601 278 680 840 416 939 863\n",
      " 182 406 148 332  69  57 357 812 448 247 853 834 371 578 331 681  84 424\n",
      " 616 702 159 699 873 409 397 662 365 268 799  59 488 650 517 122 260 768\n",
      "  77  89 555 663 301  35 698 414 949 628  17 921 629  55   9 369 676 153\n",
      " 860 189 144 312 666 538 732 920 885 213 846 160 821 114 333 288 816 728\n",
      " 504 378 493 283 579 186 382 336 688 441 512 549  56 741 307 879 611 192\n",
      " 887  60 789  66 172 154 415 259 141 558 299 673 366 163 432 242 179  45\n",
      " 622 200   4 147 150 218  16 461 372 756 116 170 788 125 479 691 408 586\n",
      " 178 188 376  94 304 928 342 309 758 857  73 407 856  98 808  23 380 297\n",
      " 505  36 329 280 273 281 177 180 326 123  88 410 254   3 175 276 323 315\n",
      " 730  12 353 883 319 201 264 363 277  22 129 361 181 234 306  50  79 120\n",
      "  58 922 140  34  38 112 423   7 176 362 226 770  62  46  96  76 797 339\n",
      " 347 229 282 185 481 367 102 401 235 659  31 263 384  71 104  51  28 139\n",
      " 240 205 134 475 882  39 786 110 334 199  54 485 255  78 107 311 300 330\n",
      " 413 174 343 118 256  74  97  25  15 146 303 402 193  82 198 113  10  80\n",
      "  40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (340, 43200) (340,)\n",
      "trainset after (350, 43200) (350,)\n",
      "updated train set: (350, 43200) (350,) unique(labels): [ 91 259] [0 1]\n",
      "val set: (945, 43200) (945,)\n",
      "\n",
      "Train set: (350, 43200) y: (350,)\n",
      "Val   set: (945, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 35\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.210 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193   7]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (945,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (945, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[663 822 935 386 631 245 784 746 845 561 230 577 680 788 607 584 582 578\n",
      " 286 704 510 594 759 778 592 321 514 688 933  53 469 545 627 461 729 776\n",
      " 725 235 419 432 734 398 859 105 542 560 131 730 917 828  99 628 709 834\n",
      " 547 564 920 313 941 489 652 166 205 896 591 879 850 492 459 938 517 193\n",
      " 655 836 597 484 557  44 439 500 640 924 720 927  85 744 683 106 189 339\n",
      " 856 777 529 224 767 403 320 388 471 736 638 506 893 732 815 507 480 875\n",
      " 944 541 747 108 820 907 902 768 562 645 531 921 550 723 278 755  29 571\n",
      " 702 497 771 900 888 699 715 340  70 830 647 671 811 304 237 289 481 518\n",
      " 353 648 816 490 385 887 731 566 886 608 568 639 877 336 380 910 266 790\n",
      " 294 819 895 852 589 809 926 244 424 930 182 466 252 523 710 673 714 567\n",
      "   8 269 404 295 271 521 374 170 509 441 787 881 128 814 528 590 537 804\n",
      " 474 813 488 525 429 476 922 462 534 458 535 433 906 519 703 697 687 764\n",
      " 345 220 666 636 701 808 651 574 794 810 898 443 249 558 273 858 530 691\n",
      " 823 748 905 208 625 229 355 817 659 753 256 665 618 931 672 297 914  20\n",
      " 326 442 690 143 496 565 883  21 739 840 718 482 785 363 661 698 596 894\n",
      " 863 766 457  48 499 903 358 478 451 824 538 885 576 251 868 600 799 257\n",
      "  13 384 503 664 569 891 684 155 833 882 937 293 397 524 324 167 654 601\n",
      " 621 839 559 801 260 897 515 472 728 520 602 100 463 892 227 354 604 678\n",
      " 121 540  47 454  26 717 853 793 504 705 593 940 727 612 168 889 738 901\n",
      " 359 743 539 861 261 862  27 464 449 516 583 741 455 694 841 283 686 765\n",
      " 394 270 130 872 274 421 617 546 231 136 757 689 733 646  61 677 614 919\n",
      " 387 493 750 679 908 585 513 156  64 719 928 183 769 821 712 770 904 662\n",
      " 762 842 553 619 119 555 149 658 395 915 410 347 425 522 402 268 866 707\n",
      " 309 210 468  14 849 393 843 606 740 626 909 786 536 695 445 708 579 890\n",
      "  43 211 838 213  65  42 624 798 669 548 465 795 494 498 203 758 713  92\n",
      " 369  72 526 760 290 378 145 317 448 587 157 315 598 436 581 495 215 348\n",
      " 453 485 470 201 932 226 570 629 721 613 643 343 206  33 855 871 430 754\n",
      " 218 248 827 603 880 344 676 111 551 312 232 390 737 437 681 700 103 137\n",
      " 792  75 357 186 163 207 214 942 774 444 563 291 487 194 588 775  68 884\n",
      " 285 925 527   5   0  93 835  91  19 796 124 857 367 711  83 164 331   1\n",
      " 511 802 417 632  69 327  41 428 797 238 742 934 411 586 165 221 633 649\n",
      " 860 301 611  87 752 209 151 899  49 923 630 650  86   6 554 135  35 467\n",
      " 450 556 288  32  17 806 837 434  30 634 246 216 265  52 399 138 595 844\n",
      " 101 350 161 351  18 512 319 435 240 152 599 413 446 223 426  63 117 533\n",
      " 829 832 202 316 292 373 356  11 825 337 936 158 452 635 641 196 475 637\n",
      " 756 236 438 572 780 250 391 418 706 420 127 911 783 675 323 745 190 456\n",
      " 773 772 805 644 831 389 109 693 782 605  37 674 912 867 376 126 761  67\n",
      " 865 610 864 264  89 407 716 133 427 623 423 415 692 696 406 142 382 846\n",
      " 122 826 916 115 818  66  81 396 219 159 505  45  90 300 620 869 132 609\n",
      " 803 791 615 349 282 185 307 371 447 322 247 544  55 195 508 549 416 876\n",
      " 573 187 726 341 735 287 486  24 172 575 311 144 657 242   2   3 682 171\n",
      " 377 943 668 372 656 543 642 812 781 670 243 114 191 491 177 334  77 939\n",
      "  56 440 330 851  23 328 284 212 178  95  94 580  60 552 501 722 622  59\n",
      " 365 277 306 854 364 929 308 148   9 153 370 241   4 123 332 222 267 381\n",
      " 878 120 749 685 368 188 660 379 408 477 405  98 318  28 275 200 431 217\n",
      "  57 807 918 724 335 667 150 325 870 502 258 279 460 154 160 375 296  36\n",
      " 751 259 162 848 913 298 532  88 181 102  12 174 616 125 392 176 414 253\n",
      "  79 346 280  84  73 847  96 199 272 116 228  46 800  22 422  16 276 104\n",
      " 360 110  38 141  51 873 874 112 383 281 180 169 653 362 352 409 233 361\n",
      "   7 366 263 303 129 314 179 763 412  58 140 789 310 234  34 184 225 473\n",
      "  71 175 198 134 305  31 479 204  78 483  76 146  50 147 400 779 299 338\n",
      " 262  39  62 239 254  54 302 173 107 333 139 342  97 192 197 401 329  10\n",
      " 118  25  82 255  74  15 113  40  80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (350, 43200) (350,)\n",
      "trainset after (360, 43200) (360,)\n",
      "updated train set: (360, 43200) (360,) unique(labels): [ 93 267] [0 1]\n",
      "val set: (935, 43200) (935,)\n",
      "\n",
      "Train set: (360, 43200) y: (360,)\n",
      "Val   set: (935, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 36\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.064 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (935,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (935, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[579 683 558 591 504 575 205 915 396 604 715 589 574 648 753  53 417 699\n",
      " 675 561 459 529 539 527 285 457 847 850 235 727 108 467 437 908 765 430\n",
      " 772 771 515 749 338 739 810 319 741 543  70 487 555 401 512 678 704 422\n",
      " 808 841 887 105 545 588 643 220 884 622 548 911 893 536 490 294 924 922\n",
      " 469 928 581  29 859 877 523 778 516 335 288 535 594 495 482 770 826 540\n",
      " 379 720 918 155 564 726  85 559 625 934 277 879 641 352 615 224 718 804\n",
      " 251 494 507 244 886 634 761 486 792 508 571 106 725 505 809 913 488 250\n",
      " 636  99 373 170 820 891 666 912 419 586 303 731 758 870 312 517 565 762\n",
      " 624 354 898 901 868 632 131 193 526 587   8 667 385 697 710 464 478 693\n",
      " 402 803 182 781 682 344 256 230 647 501 849 605 917 889 472 705 580 698\n",
      " 931 268 655 593 888  44 692 844 882 651 644 853 568 738 866 813 474 386\n",
      " 661 712 447 635 787 427 686 822 873 528 657 292 208 498 439 441 729 921\n",
      " 802 461 100 320 143 456 831 709 742 248 700 885 892 713 786 353 462 293\n",
      " 601 760 479 255 815 533 513 282 664  92 521 732 724 734 272 689 843 828\n",
      " 455 269 362 883 696 722 237 339 794 480 265 578 736 694 806 930 812 833\n",
      " 906 514 878 707 780 910 905 671 582 872 476 668 440  47 556 311 598 764\n",
      " 562 616 618 919  61 573 259 229 566 733 532 189 783 874 626 183 267 557\n",
      " 384 642 714 546 431 168 203 747 518 136 690 863 852 723  21  48 797 325\n",
      " 825 563 156 597 538 660 167 260 497 157 599 347 650 511 684 346 273 145\n",
      " 610 614  43 357 674 449 316 816 408 897 227 876 128 583 103  20 520 609\n",
      " 685 470 296 896 659 446 551 392 460 857 600 576 549 932 894 834 201   5\n",
      " 735 383 832 372 492 702 895 368 491 124 590 111 463 672 519 791 270 428\n",
      " 862 560  64 210 679 323 881 923 623 426 703 166 748 522 835 232 854   0\n",
      " 342 206 400 830 537 544 138 681   1 496 194 763 673 502 611 214 807 630\n",
      " 534 788 119 493 101 827 451 393 785 453 756  65 186 130  27 851 468  13\n",
      "  69 231 840 584 654 819 899 658  75 608  42  72 744 190 240  18  33 759\n",
      " 900 880 473 287 751 927 245  90   6 388 314  14 413 218 213 395 149 121\n",
      " 553 483 790 752 226 207 221 172 646 132 423 926 846 308  87  26 358 209\n",
      " 628 137 322 603 343 821 728 202  83 552 871 452 848 606 695  91 640 706\n",
      " 525 824 485  49 356 444 621 211 433 443 777 223 801 336 567 315 127  30\n",
      " 135  63  68 238 435 450 769 925 117 554  32 247 754 814  52 246 164  19\n",
      " 466 676 856 243 366 306 637 161 629 215 114 795  24 902 524  81 249 434\n",
      " 774  67 716 639 465 290 418 133 421 432 596  86 264 645 349  93 409 875\n",
      " 318 436 890 860 617 779  41 737 656 914 115 740 670 585 627 397 799 289\n",
      " 415 122 350 377 789 798 509 291 633 442 595 776 391 691 855 389 701 348\n",
      " 300 165 445   9  60 569   2 708 126 592 768 163 550 196  17 510 602 326\n",
      " 858 284 818 216 454 746 665 669  11 394 448 503 151 933 416 305 547 755\n",
      " 310 414 836 796 375 767 620 484 542 405 506  94 152 572  55 766 652 663\n",
      " 631 541 837 916 363 424 612 907 242 187 425 411 283 219 845 153  35 142\n",
      " 750 236 800 784 176  89 387 607 195 181 109 531 286 711 295 274 688 331\n",
      "  37 903  95 159  23 263 144 212 613 811  45 842   3 177  84 638 499 662\n",
      " 869 340 721 404 329 677 355 823 222 158 920 307 653 687 367 570 838 364\n",
      " 745 867 406 376 829 148 500 530  77 321 717 909  57 299 730 743 489 929\n",
      "  88 266 371 188 619 333  36 861  56   4 817  59 281 805 258 475 438 185\n",
      " 381 330 191 719 160 370 278 458 178 257 775 199 276 241 390 171 252 680\n",
      " 380 334 141 125 429 577  66 403  73  28 369 150 120 154 116 412 217 420\n",
      " 839 378 407 104  96 179 793 361 275 225 279 351  22 169 317 327   7 297\n",
      " 180 174  98 302 904 262 360 200  71 864 865  12 129 757 773 123 234 313\n",
      " 271 147  46 110 324 359 374 365 280  58  79 184 782 162 477 112  38 345\n",
      " 649 261 304  62 140  76 102  50 204 481  51 175 233 228 253  16 398 198\n",
      " 410  78 134 382 471 239 309 146 107  54  34 337 173  31 341 139 328  39\n",
      "  25 332 298 192  74 254 301  97 118  15 399  82 197  10 113  80  40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (360, 43200) (360,)\n",
      "trainset after (370, 43200) (370,)\n",
      "updated train set: (370, 43200) (370,) unique(labels): [ 95 275] [0 1]\n",
      "val set: (925, 43200) (925,)\n",
      "\n",
      "Train set: (370, 43200) y: (370,)\n",
      "Val   set: (925, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 37\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.919 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (925,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (925, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[744 457 334 902 690 337 287 756 465  29 524 488 318 415 284 583 899 706\n",
      " 875 884 557 575 908 718 537 587  53 914 670 762 542 918 428 545 540 493\n",
      " 667 838 384 722 717 783 170 455 640 817 635 420 106 155 526 108 509 378\n",
      " 243 701 607 582 570 555 532 219 311 732 485 476 749 633 643 502 819 730\n",
      " 372 795 763 841 876 832 276 772 912 319  70 740 626 234 801 385 711 460\n",
      " 799 351 616 727 794 870 472 425  85 878 882 761 467 512 247 800 302 753\n",
      " 486 889 513 921 505 536 552 695 861 533 560 658 492 911 904 639 835 131\n",
      " 715   8 857 903 189 924 811 689 677 496 793 484 293 660 470 343 520 868\n",
      " 480 834 499 880 804 703 399 864 785 514 249 223 859 687 193 561 892 205\n",
      " 774 628 720 850 716 877 617 400 559 797 581 105 345  43 477 649  44 567\n",
      " 636 207 530 624 236 771 504 462 500 250 574 752 713 873 696 683 813 439\n",
      " 564 627 523 292 591 729 338 674 383 691 100 725 647 474 816 267 887 361\n",
      " 228 907 529  47 778 518 685 788 653 136 920 680 255 610 659 352 586 597\n",
      " 435 822 454  99 516 515 209 840 896 709 417 167 844 869 879 230 733 888\n",
      " 535 694 553 271 803 558 806 580 478 183  18 458 872 854  92 883 608 322\n",
      " 664   1 700 592 310 663 684 203 724 356 291 353  13 145 823 602 489  20\n",
      " 656 495 675 777 590 259 704 751 824 913 603 897 614 594  48 755 769 453\n",
      " 438 466 584 182 525 738 490 341 119 508 258 698 666 671 445 863 534 459\n",
      " 548 723 601 168 705 576 562 782 468 845 511 483 874 848 272 157 693 541\n",
      " 681 231 651 810 577 429 867 865  61 510 543  64 742 398 779 269 229 281\n",
      " 517 143 295 156 315  65 239 437  27 688 268 573 719 491 461 885 531 714\n",
      "  75 376 371 394 615 237 321 634 807 618 665 451 124 101 421 909 554   5\n",
      " 217 346 798  30 324 264 254 843 596 406 901 642 201 161 213 128 622 519\n",
      " 646  21  42 676 225 286 726 739 444 578 391 307 737 792 426 395 317 226\n",
      " 130   0 447 449 890   6 673 818 166 571  72 853 652 891 569 448  83 750\n",
      " 825  86  19 206 335 917  26 367 747 606 546 886 382 551 463 424 631  91\n",
      " 650 826 686 637 754 735 805 342 365  87 765 111 922 593 871 387 837 431\n",
      "  81 103  14 481 194 866  33 781 314 707 699 662 522 556 220 728 831 450\n",
      " 355 600 149 464 165 494 638 613 563 760 244 117 862 133 549 407 127 847\n",
      " 419 821 697 620 413 218 471 632 588 392 202 266 196 305 163 893 208 222\n",
      " 790 906 589  90 839  67 214  63 288 289  24 621 741 411 186 550 521  41\n",
      " 212 357  49 745 215 682 849 609 851 786 432  93 390 347 430 388 655 246\n",
      " 544 815 441 313 433 115 135 842 692 121 579 598 827  17 679 138 210 916\n",
      " 248 137 506 242 846 374 181 668 599 114 190 767 132 442 503 290 585 625\n",
      " 619 757 789 661 881 768 898  11  68 482  52 329 172 416 780 568 159 743\n",
      "  69 731 412 434 915 812 828  32 905 262 565 349 370 153 776  55 440 283\n",
      " 151 443 612 644 348 393   9 263 770 758 702 423 325 389 142 595 164 414\n",
      " 273 446 191 791 152 787 814 604 759 109 221 629 894 195 282 452 501 539\n",
      " 657 122 375 833 422 235 241  60 623 211 746 566   3 126 328 820 187 403\n",
      " 354 678 775 648 836 630 507 369 409 538 802 299   2 386  57 900 809 158\n",
      "  56 404 176  89 528 339 304 309 611 380 368 497 473  94  59 366 306 919\n",
      "  23 160 796 144 910  37 808 721 547 285 605  45 330 645  95 858   4 734\n",
      "  35  77 654  84 708  36 527 216 171 487 185 240 363 923 179 188 245 294\n",
      " 712 456 298 332 410 710 852 275 436 257 177 736 402 401 141 860 377 829\n",
      " 280 147 379 256 333 498 316 669 125 199 362 148  66 154  58 162 274 178\n",
      " 150 277 766 265 572 296 427 320 830 672  22 251 104  73 129   7  46 323\n",
      " 261 748 360 418 373 326 116  88  16 174 895 312  12 784  28 169  96 855\n",
      " 184 405  76 359 270 110  71 252 123 358 350 233  50 641 180  79 200 227\n",
      " 232 278  98 773 475 856 764 279 224 140 364 120 344  38 301 112 173 204\n",
      "  34 260  31 381  51  62 469 102 396 303 336 134 308  39 175 408  78 238\n",
      " 479 139 198  54 146 340 297 107  25 327  74 331 192 300 397 118  82  97\n",
      " 253  10  15 197  80 113  40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (370, 43200) (370,)\n",
      "trainset after (380, 43200) (380,)\n",
      "updated train set: (380, 43200) (380,) unique(labels): [ 99 281] [0 1]\n",
      "val set: (915, 43200) (915,)\n",
      "\n",
      "Train set: (380, 43200) y: (380,)\n",
      "Val   set: (915, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 38\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.975 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (915,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (915, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[474  52 564 539 661 551 283 890 908 503 875 753 518 531 774 832 577 683\n",
      " 536 699 451 898  69 725 829 470 866 688 411 431 741 534 569 520 629 754\n",
      " 105 507 901 486 784 713 802 808 790 610 289 482 733 715 499 275 710 765\n",
      " 554 620 664 731 530 517 424 810 720 479 218 581 848 786 868 222 154 763\n",
      " 555 711 694 722 744 506 752 893 831 671 568 309 634 633 880 841 904 487\n",
      " 869 493 637 709 246 553 788 657 652 526 823 850 300 902 745 654 861 860\n",
      " 549 878 472 552 514 704 697 496 621 914 576 490 601 480  98 546  84 349\n",
      " 622 641 575 596 380 871 248 689 395 233 627 433 591 449 339 512 317 894\n",
      " 677 863 130 804 795 825 723 867 455 561 466 206 887 585 316 653  20 574\n",
      " 228 644 478 434 584 718 794 586 883 464 580 678 911 588 674 779 192 604\n",
      " 505 791 608 862 797 873 668 852 743 768 542 291 527 870 807 457 242 461\n",
      " 815 169 227 266 235 488 769 687 717 529 696 368 416 450 865 906 597 630\n",
      " 792 462 814 525 679 682 643 708 845 435 441 611 855 858 595 647  64 826\n",
      " 618 835 859 693 762 726 864 773 548 556 812 785 347 166 813 681 200 897\n",
      " 454 468   8 910 822 716 107 834 519 453 706  60 650 381 702 660 270  99\n",
      " 379 675 537 483 547 669 528 421 357 425 747 570 836 489  27 460 565 445\n",
      " 396 612 104 684 188 879 155 854 459 280 498 572 204 881 590 249 510 776\n",
      " 761 628 508  43 367 874 578 485 388 770 523 707 563 567 798 892 443 413\n",
      " 135 558 477 374 142 839 245 402  41 394 801  46 167 524 856 535 735 907\n",
      " 659 665 208 698 594 809 691 334 670  67  91 587 645  42 230 631 118   6\n",
      " 123 387 165 471 583 263 777 494 737 828 352 742  74 625 817 500 640 632\n",
      " 739 342 338 212 182  47 600 721 509 728 760 502 636 658 667 833 515 219\n",
      " 803 320 602 888 504 877 876  82 447 224 238 882 437 193 772 267 258 348\n",
      " 465 322 903 646 372 225 268 513 114 853 692 181 615 816 796 783   5 616\n",
      "  92 540 305 341 216 732 127 293 290 730 484  26 448 202  21  32 724 100\n",
      "  63 912 254 257  29 144  13 899 164 789 544 571 417 614 746 214 337 253\n",
      " 209 308  71 475  24 896  40 857 690 495 456 756 440 229  62 428 361  18\n",
      " 686 429 751 844 905 806 132 370  85 872 236 712 446 838 819 557 592  90\n",
      " 344 889 748 550 285 759 436 391 390 522 160 736 438 840 655 598 185  86\n",
      " 734 511 895 265 609 695 582 271 476 781 545 403 827 573 313 426  19 589\n",
      " 383 811 409 420 129  51 234 719 680 842 603 444 805 673 422 359  17 126\n",
      " 624 195 771 613 700 136  68  11 156 767 319 758   1 619 311 562  66 766\n",
      " 607 662 543 837 286 782 217 439 750  54 442 780 113 213  89 201 900 141\n",
      " 516 162  31 148 134 533 749 131 617 287 407 430 606 651 247 221 378 288\n",
      " 656 685 579 343 116 151 163 137 284 282 110  59 676 175 312  93 382 365\n",
      " 205 672 649 538 599 366 412 363 727 818 559 605 362 521 481  14 626 884\n",
      "  55 793 120  34 913 830 497 389 458 353 642 638 315 843 323 243 891 501\n",
      " 281 415 351 623 121   0 787 327 800  58 738 885 150   3 729  94 207 909\n",
      " 452 211 593 419 386 274 189 799 560 778 427 701 307   9 158 180  80 418\n",
      " 272 364  48 303 332  23 345 262 261 384   4  88 491 648 532 467 335 432\n",
      " 541 847 159 398 170 410 326 849  35 775 152 350  44 194 240 304 220 190\n",
      " 102 703 297 376 171 256 184 153 371 663 820  83 125 210 824 399  45 178\n",
      " 714 143 157 186 328 302 239 149 187 408 851 314 666 146 108 492 639 241\n",
      " 405 821 705 173 292 264  36 168  76 740 103 177 356 423 757 147 764  57\n",
      " 255 244  12 331   2 215 250 469  28 400 755 273  72 358  65 566 385 198\n",
      " 179 330 161 223 296  87 321 397 294 124 318 199 375 406 354  56 369 279\n",
      " 886 324 635 122  22   7 176 401  78 473 299 846 232 373 414  70 260  37\n",
      " 115 109  50 276  75 140  16 360  97 310 128 340 101 183  95 269 346 355\n",
      " 277 226 259 463 377 111 251 237 119 404 174  38  49 231 278 392  30  77\n",
      " 139  33 145 203 295 333 172 301 133 306  61 106 191 325 329  53  15 336\n",
      " 197 117  25  73 298  96 393 196 138  81  10  79 252 112  39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (380, 43200) (380,)\n",
      "trainset after (390, 43200) (390,)\n",
      "updated train set: (390, 43200) (390,) unique(labels): [101 289] [0 1]\n",
      "val set: (905, 43200) (905,)\n",
      "\n",
      "Train set: (390, 43200) y: (390,)\n",
      "Val   set: (905, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 39\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.003 s \n",
      "\n",
      "Accuracy rate for 97.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       200\n",
      "           1       0.97      0.97      0.97       200\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[194   6]\n",
      " [  6 194]]\n",
      "--------------------------------\n",
      "val predicted: (905,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (905, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[527 755 627  97 802 314 800 422 626 409 449 570 884 705 603 824 796 541\n",
      " 707 516 895 733 530 431 645 669 479 782 315 703 372 379 476 786 745 656\n",
      " 889 532  68  83 725 414 221 104 691 526 757 901 503 496 622 393 766 746\n",
      " 717 736 794 274 103 307 680 547 853 855 776 686 840 620 584 187 514 715\n",
      " 594 702 858 522 544 562 513 168 484 245 821 737 696 613 332 470 227 663\n",
      " 630 429 217 298  62 455 574 378 247 646 232 475 817 688 569 504 778 885\n",
      " 493 615 670 611  27 165 870 394 459 867 134 685 723 468 700 549 844 780\n",
      " 815 681 366 502 744 701 226 477 205 842 153 453 229 129 487 567 860 262\n",
      " 614 490 777 865 879 561 577 578 548 510 710 823 555 623 674 790 483 546\n",
      " 689 154  43 875 439 289 203 783 904 495 872 892 712 462 833 180 568 893\n",
      " 694 234 166 771 851 191 789 452 474 806 856 752 675 857 563 676 241   8\n",
      " 787 863 846 537 883 597 377 861 590 714 106 581 207 601 448 827 837 164\n",
      " 640 634 267 818 589 866 525 900 799 784 754 248 347 660 847 552 871 433\n",
      " 423  13 671 339 826  46 888 647 460 279 768 520 593 523  42 760  63 852\n",
      " 508 337 266 579  70 466 636 718 621 805 464 761 486 698 346 143  20 122\n",
      " 709 673  47 653 345 542 859 690 604 270 560 659 141 438 500 638 521 192\n",
      " 269 194 501 355 265 432 662  26 666 573 506 735 451 469 235 739 480  59\n",
      " 419  66 683 264  98 588 435 708 237 557 880 679 898  21 458 862 605 199\n",
      " 704 637  90 287 499 652 801 288 208 571 639 828 519 392 311 447 524 550\n",
      " 765 159  18 515 256 340 643 128 411 807 303 386 317 215 661  99 762 306\n",
      " 320 481 485 650 244 223 854 868 684 848 253 539 667 624 657 820 228  29\n",
      " 509  24 699 565 531 809  73   6 370 117  41 257 804 793 400 678 727 580\n",
      " 558 350 595 482 385  81 407 445  89 291 126 213 388 713 869 505 365 389\n",
      " 608 682 201 625 181 633 325 850 874 441 405 335 472 566 147 420 831 724\n",
      " 873 545 716 163   1 890 336 728 734 731 629 381 252 443 564 798 535 729\n",
      " 457 720 795 242 463 318 284 583 602 112  32 418 135 618 781 543 218 651\n",
      "  40 591 119 894 491 410 808 281  17 376 200 497 155 775 310 511 538 368\n",
      " 246 726 533 454 140 845 161 115 897 836 551 748 210  19 125 814 738 849\n",
      " 427 773 184 211 384 507 607 887 825 283 131  61 609 285 425 722  34 492\n",
      " 788 224 518 576 642  14  91  65 512  85 401 216  84 902 763 204 886 876\n",
      " 600 342 743 357   0   5  11 434 764 881 494 753 179 654 220 113 359 648\n",
      " 759 361 313 444 587 436  79 426 740 672 575 136 212 864 101 424 109 830\n",
      " 498 540 169 408 351 596 834  58  51 582 396 742 774 349 260 309 233 403\n",
      " 415 416 157  48 440 321 692 364 649 891  67 387 612 261 811 797 437  53\n",
      " 572   3 772 792 599 534 149 150 301 341 803 162 188 585 553 687  44 822\n",
      " 286 362 619 750 456 896 711 635 360 769  64 556 219 316 174 363  23 130\n",
      "  54 616  88 413  31 206 617 333 282 428 677 473 446 442 380 280 348 189\n",
      " 668 832   9 528 156 382 606 369  92 751 903 810 730 148 185 665 417 133\n",
      "   4  57 300  87 295 829 489 120 586 819 741 374 324 779 193 271 610 899\n",
      " 343 641 664 478 305 158 238  75 841 877 294 882 719 183  82 172  93 785\n",
      " 302 209 124 770  35 529 239 816 758 107 631 644 292 273 598 693 554 151\n",
      " 655 450 330 592 352 791 312 397 721 465 142  55 706   2  28 398 186 697\n",
      " 328 263 290  56 240 406 658 255 326 536 383 322 167 430 517 243 488  22\n",
      " 278 214 835 160 329 222 404 843 319 354 146 177 176  71 632   7  36 145\n",
      " 175 254 178 170  77 152 373 144 358 198 272 395  45 813 108 100 749 231\n",
      "  86 123 249 421  69 559 275 767 197 732  12 225 812 118 356 139 375 839\n",
      " 127 399  96 371 367 182 102 695 114  50 250 121  74 259 402 412  16 878\n",
      "  37 344 756  49  94 276 297 171 173 628 353 268 277 308 838 467 338 230\n",
      "  38 138  33 471 236 461 390  76 258 304 202 196 110 299 132 747 293  30\n",
      " 105 334 331 327  52 190  60 296 323  95 391  25 195  10 137  72  15  80\n",
      " 116  39  78 111 251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (390, 43200) (390,)\n",
      "trainset after (400, 43200) (400,)\n",
      "updated train set: (400, 43200) (400,) unique(labels): [105 295] [0 1]\n",
      "val set: (895, 43200) (895,)\n",
      "\n",
      "Train set: (400, 43200) y: (400,)\n",
      "Val   set: (895, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 40\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.061 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (895,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "probabilities: (895, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[472 492 220 684 377  68 475 814 510 598 376 700 874 638 522 758 788 738\n",
      " 464 525 608 445 411 216 569 273 649 673 512 565 392 710 557 737 233 857\n",
      " 499 617 739 696 498 708 774 527 623 370 879   8 167 811 544 536 391 850\n",
      " 471 313 306 662 152 885 695 832 768 894 128 518 364 813 449 848 698 778\n",
      " 718 875 729 882  20 479 458 455 589 599 539 240 297 830 615 855 264 345\n",
      " 225 489 606 679 862 542 845 749 681 506 596 563 786 564 509  13 779 891\n",
      " 827 730 579 246 639 716 693 843 861 231 808 521 770 180 286 851 841 425\n",
      " 707 860 640 483 703 480 689 163 288 427 667 682 435 486 883 823 656  27\n",
      " 103 204 541 705 869 694 500 668 653 687 678  83 448 627 769 451 190 610\n",
      " 775 460 491 558 747 782 842 663 133 556 532 836 504 776 669 562 244 726\n",
      "  26 330 745 807 844 633 763 165 543 186 519 674 878 153 817 429 466 791\n",
      " 772 666 550 206 630 573 516 781 847 865 834 618 102 473 691 752 465 266\n",
      " 753 584 105 853 572 375 890 462  47 576 592 664 837  63 805 629 164 278\n",
      " 760 560 873 456 616 228 545 728 335 511 416 547 702 795  62 555 268 502\n",
      " 585 583 683 444 318 419  18 849 856 353 261 810 797 650 796  42 816  46\n",
      " 701 496 363 482 574 588 537 672 643 676 343 711  97 645 470 226 644 202\n",
      " 481 428 265 852 227 247 846 520 443  43 590 476 732 870 568 337 754 552\n",
      " 121 344 692 677 505 864 252 501 794 654 495  59 818 655 222 792 142 478\n",
      " 785 798  29 619  73   5 517 450 408 116 652 434 383 487 214 200 659 453\n",
      " 310 773 412 447 632  66 338 477 198 741 859 757 287 497 646 636 515 566\n",
      " 316 888 243 626 236 609 884 390 838 528 269 840 559 210 158 454 217 553\n",
      " 600 333 140  91 441 578 334 256 437 302 179  98 804   6 212 387 379 439\n",
      "  21 863 622 613 671 720 255 207 384 706  17 368  90 799  81 675 315 125\n",
      "  41 660 821 415 530 892 134 727 575 417 526 234 880  70 858 717 431 722\n",
      " 631 468 604 323 709 348 815 780 724  89 127 459 582 603 721  24 715 713\n",
      " 535 154 290 826 767 130 736  85 398 540 597 503 790 538 283 602 493  32\n",
      " 534 245 787  19 193 162 223 282   1 423 839 366 507 305 835 765 886 561\n",
      "  61 697 733  84 308 386 546 199 191 571  67 641 756 146 168 887 430 680\n",
      " 355 215 405 112 263 746  14 514 635 533 866 620 488 647 731 178  65 361\n",
      " 704 820 359 570 421 433 422 438 241 594 685 139 403 854 614 382 357 751\n",
      " 607 129 595 280 586 822 440 339 761 580 284 108 824 591 420  40 508 211\n",
      " 399 436 114 309 124 118 219 719 665 285 755 312 877 156 385 407   3  79\n",
      " 300 789 577 490 374 149 494 183 801 529 360 661 876 270 203 812 871 251\n",
      " 173 187 349 232 548 567 784 744 378  11 432 340 764  88 160 452 410 111\n",
      " 347 424 723 670  53 100  64  31 642 612 867  48 281 601 406 218 766 881\n",
      " 800 404 279 819 551 735  51 413 442 793 182 581 358 658 469 523 372 734\n",
      " 259 743 637 135 132 260  58 806  92 161 611 414 394 346 605 889 831 628\n",
      " 809  34 119 367  93 762 157 893 362 148 872 380 524 686 624 331 205 184\n",
      " 771 750 446 169 777   0 657 474   9 188 593 209  54 651  23 549 176   4\n",
      " 322 712 155 314 254 150 301 147 587  82  87 304 319  57 106 208 783 699\n",
      " 513 291 341 833 401  44 648  28 690 825 237 485 396 192 381 144 484  75\n",
      " 714  56 688 171 350 185 123 262 531  22 293 311 239 253  55 418 145 299\n",
      "  77 426 294 151 213 759  35 238 175 326 166 328 625 393 141 272 554 122\n",
      " 802 221 324 277 101 803 461  45 395   2 742 402 634 320 371 248 352  36\n",
      " 159 327 196 365 725 242  69 107 174 271 289 224 138  86 356 230 828 354\n",
      " 868 143 274 258 197  71  50  12 369 463   7 317 740 181 177 126 467 621\n",
      "  16 373  74  96 397 296 829 748  49 409 120 276 400 307 249 117 257  94\n",
      " 170 457 113 342  76 351  37 267 229  99 275 235  38 388  30 109 336 137\n",
      " 303 298 332 292  33 131  60 201 172 329 195 321 104  52  25 325 389  72\n",
      " 295 189 136  95  15  10 115 194  80  39  78 250 110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (400, 43200) (400,)\n",
      "trainset after (410, 43200) (410,)\n",
      "updated train set: (410, 43200) (410,) unique(labels): [108 302] [0 1]\n",
      "val set: (885, 43200) (885,)\n",
      "\n",
      "Train set: (410, 43200) y: (410,)\n",
      "Val   set: (885, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 41\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.089 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (885,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (885, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[550 691 875 641 374 729 749 869 368 709 505 779 838 408 847 493 840 558\n",
      " 761 864 872 562 515 717 802 468 686 654 520 881 474 102 850 648 701 475\n",
      " 511 481 665 442 855 503 736 670 615 873 549 537 589 759 422 432 782 242\n",
      " 328 721 311  82 448 529 865 446 630 518 469 536 598 341 566 452 535 687\n",
      " 680 388 884 203 166 609 500 478 832 824 534 625 185 295 484 833 699 730\n",
      " 582 659 304 671 101 577 673 525 104 607 557 719 645 661 769 698 622 664\n",
      " 720 684 836 845 600 728 321 820 822 743 619 556 837 635 300 831 852 744\n",
      " 548 765 492 841 285 551 631 512 754 461 561 271 205 689 666 245 441   8\n",
      " 738 565 467 455 827 362 685 127  62 851 763 817 373 591 453 486 366 702\n",
      " 389 740 788 585 777 226 215 440 694 163 602 772 463 692 880 842 206  19\n",
      " 658 826 796 229  46 696 675 799 426 223 660 655  63 471 238 387 610 224\n",
      " 201 651 266 532 656 668 264 569 567 459 678 457 804 592 425 652 682 807\n",
      " 843 431 213 132 608 509 496 416 760 543 436 490 581 813 839 601 538 707\n",
      " 830 621 284  41 424 751 859 623 489 477 540 530 767 197 868 333 473 860\n",
      " 628 555 723 798  18 766 674 214 646 179 244 849  27 778 510 351 444 346\n",
      "  29 776 286 190 335 308 786 632 263 877  72 647 343 189 568 232 795 162\n",
      " 209 806 808  13 828 576 141 546 462 745 513 384  26 262 139  89 276 748\n",
      " 514 250 559 152 234 494 504 785 787 414 332 178 151 115 878  20  40 572\n",
      " 498 579 438 773 700 578 642 835 161 853 846 313 482 770 451 259 783 583\n",
      " 523 413 159  17 683 281 564 792 298 220  42 801 638 507 336 706 854 342\n",
      " 552 267  66 693 669  47 521 254 445  69 405 153  83 519 715 863 874 253\n",
      "  80 508 314 834  59 216  97 157 644 241 225 495 553 570  21 663 192 618\n",
      " 491 231 288 249 711 624 450   5 805 199 243  43 708 611 499 545 614 575\n",
      " 594 218 612 307 211 396 573 129 221 418   1 531 303  96 164 434 380 120\n",
      " 636 456 756 697  65 747 866 596 447  11 781 758 126 848 811   6 563 361\n",
      " 825 483 476 472 533  53 167 138 383 465 713 310 688 771 381 402 372 497\n",
      " 278  67 527 667 526 528 316 124 501 412 123 355 789 599 812  90 637 870\n",
      " 752 435 742  14 595 829 657 816 261  84 871 376  32 420 882 605 358 876\n",
      " 404 764 724 737 554 620 257 155 202 437 110 571 590 364 428 177 883 379\n",
      " 208  24 735 409  61 606 810   4 522 757 419 133 331 186 722 430  78 633\n",
      " 347 338 634 790 603 502 317 487 449 378 856 345 695 712 111 360 861 400\n",
      " 672 410 280 800 282  34 107 732 814 584 329 306 337 207 587 753 204 653\n",
      " 134 580 187 439 309 588 791 809   0 239 407 148 417 627 403   3 113 755\n",
      "  48 485 147 734 718 639 131 427 705 649  51 395 844 154 727 574 704 803\n",
      " 710 182  57  88 365 746 629 443 217 867 604 517 172 382 377 230 183 191\n",
      " 392 268 676 146 357 488 541  31  64  44   2 181 429  54  99 725 149 741\n",
      " 312 539 398 279 277 210 156 145 775  74 339 297 516 784 542 470 158 237\n",
      " 258 117 544  45 506 326 367 862 421 650 780 401 292 198 375  87  91  58\n",
      " 560 479 597 236 299 466 433 105 593 173 359 586 356 353 287 168 677 291\n",
      " 399 283  35 235 140 879  86 411  81 160 797 315 393 106 774 857 212 662\n",
      " 128 703 815 272 616 269 643 344  23 150 768 690 726 289 318 219 325 714\n",
      " 626 322 390 184 270 391 524 369   9 170 793 370 415 363 324  73 125 458\n",
      " 256  92 251 174 196 821 320  22  70 122 354 118 275 302 762 100  28  36\n",
      "  56 679  12 176 265   7 252 240 305  50  55 794 144 750 246 350  16 175\n",
      "  68 640 397 195 121 371 142 681 423 222  85 227  49 112 348 547 180 260\n",
      " 617 406 716 731  98 823 136 352 169 385 165 480 733  76 327 143 228 273\n",
      " 137 858  37 274 233  33 464 349  38 119 818 340 301  30 108  95 171 739\n",
      " 200 334 394 116 819 247 613 460 130 255 294 323  93 290 296  75 188 114\n",
      " 330 454 194 293 103  52  79  10  94 386  60 319  15 135 193  25  71  39\n",
      "  77 109 248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (410, 43200) (410,)\n",
      "trainset after (420, 43200) (420,)\n",
      "updated train set: (420, 43200) (420,) unique(labels): [110 310] [0 1]\n",
      "val set: (875, 43200) (875,)\n",
      "\n",
      "Train set: (420, 43200) y: (420,)\n",
      "Val   set: (875, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 42\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 1.893 s \n",
      "\n",
      "Accuracy rate for 95.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (875,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (875, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[102 513 794 343 661 271 856 473 166 839 830 627 863 242 420 769 555 559\n",
      " 472 503 203 185 547 491 711 479 832 440 501 842 518 284 751 685 341 311\n",
      " 466 650 206 534 482 366 682 753 823 694 722 641 812 715 864 386 490 874\n",
      " 406 656 771 597 829 612 737 676 189 644 833 586 606 619 574 127 223 871\n",
      " 702 814 516  26 837 304 667 588 459 757 295 595 816 696 616 844 809  82\n",
      "  46 333 527 467 328 761 215 498   8 321 683 847 604 723 796 622 691 536\n",
      " 857 553 151 422 439 733 372 532  90 446 238 224 825 262 689 346 563 530\n",
      " 205 509 764 535 476 569 714 662 444 713 533 579 628 554 229 461 226 818\n",
      " 834 451 828 651 669 453 332 541 799 201 438 824  17 104 654 300 693 607\n",
      " 674 681 213 562 680 179 152 599 666 411 731  47 523 558  27 510 245 687\n",
      " 211 790 266 670  41 457 164 819 755 774 566 430 450 511 851  96 484 678\n",
      " 424 582 351 729  83 758 843 870 209 546 362 671 736 512 496   5 660 788\n",
      " 286  72 762 822  20  62 575 442 697 780 192 387 835 423 798 618 657 629\n",
      "  67  63 652 688 648 805 578 244  40 220 655 564 642 573 632 791 778 241\n",
      " 752 492 162 385 115 221 469 276 471 647 159 508 620 717 489 232 264 589\n",
      " 827 487 779 552 765  18 855  61 403 494 625 565  97 770 598 550 787 132\n",
      " 528 865 538 163 101 342  59 665 139 746 298  19 800 860  80 556 768 738\n",
      " 638 831 633 846 161  21 741 743 382 432 231 488 664 643 249 407 845 475\n",
      " 445 507 190 544 709 470 838 138  29 381 605 199 331 314 455 568 124 448\n",
      " 214 414 434 635 141 460  89 634 759 793 465 777 506 549 659  66 548  13\n",
      " 285 561 394 852 497 197 502 259 609 443 474 695 250 868 797 621 335 803\n",
      " 379 517 480 436 216 378 355 415 505 308 826 313 706 364  42  11 253 692\n",
      " 521  69 841 543 756 781 861  32   6 572 529 316 608 663 463 281 775 454\n",
      " 679 358 784 360 820 429 580 412 703 707 374 426 808 225 744 782 234 307\n",
      " 153 288 640 129  53 499 359 111 591 684 705 730 254 393 821 120 596 867\n",
      " 840  43 172 750 611 230 398  65 817 157 690 336 283 416 178 804 278 615\n",
      " 182 365 243 567 602 763  88 167   1  24 872 735 257 718 653 740  57 581\n",
      " 806 493 210 519 380 449 699 126 410 310 525 317 267 593 485 329 773 853\n",
      " 551 218 263 495 309 357 303 866 202 802 282  84  45  51 428 148 570 268\n",
      " 858 716 306 110 337 361  44 280 531 576   3 435 123  14 377 748 749 701\n",
      " 594 371   2 873 191 795 672 417  34 338 747 728 147 277 592 418 603 862\n",
      " 187 630 500 587 636 721 133 347 400 585 526 261 712 725 704 155 792 481\n",
      " 848 131 584 560 433 437 239 373 113 154 299 631 427 186 520 537 390  54\n",
      " 375 326 402 600 447 425 401 235 524 354 160 145 649 174  86 542  23 405\n",
      " 177 408 859 107 668  91 739 367 745 363 128 353 207 557 720 150 312   4\n",
      " 100 219 464  78 270 204 624 767  31 146 483 279 339 134 836 356 419   0\n",
      "  92 287 727 149 168 396 345 577 208 515 626 658 719 645 849 601 700 441\n",
      " 431 617  28 217 302 376 504 514 772 486 236 181 315 539  58  35 118 156\n",
      " 776 869 468 590 389 522 409 646 397 783 297 613 571 369 292  99  48 158\n",
      " 813 801 117 183 815 677 623 399 734  36 198 760 754  64 708 322  87 184\n",
      " 673   9 212 854 540 639 291  74 325 456 614 637 106 320 269 256 421 344\n",
      " 289  12  81 258 195 807 477 170 413 698 122 251 789 246 686 105  56  70\n",
      " 272  50 237  22 140 368 388 173 583 252 318 391 785 766 275  98  76 348\n",
      " 675 260 176 142 175 196 165 180 742 125  85  37 240 710   7 305  49  73\n",
      " 404 144 324 370 395 222 811 350 786 478  16 545 228 265 143 334 327 850\n",
      " 108  68 121 119 732 458 352 726 227 169  33  55  38 290 136 273 610 301\n",
      " 233 255 810 112 383  95 392 274 349 137 171 724  93 247 114 200  30  75\n",
      " 340 116 294 130 188 452 330 323 462  60 293 296 319  94  15  52 103 384\n",
      " 193  10  79 194  25  71  39 135 109  77 248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (420, 43200) (420,)\n",
      "trainset after (430, 43200) (430,)\n",
      "updated train set: (430, 43200) (430,) unique(labels): [114 316] [0 1]\n",
      "val set: (865, 43200) (865,)\n",
      "\n",
      "Train set: (430, 43200) y: (430,)\n",
      "Val   set: (865, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 43\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.479 s \n",
      "\n",
      "Accuracy rate for 96.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (865,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (865, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[498 824 512 402 549 822 496  82 126 462 764 301 854 553 638 486 383 292\n",
      " 368 744 621 704   8 213 240 468 861 183 662 644 325 746 650 471 358 203\n",
      " 382 436 801 199 528 474  63 529 660 101 687 151 853 669 635 833 722 598\n",
      " 622 521 541 804 504 708 754 806 675 477 659 463 338 591 600 589 606 568\n",
      " 829 678 150 580 330 610 817 676 510 426 201 440 526 308 808 227 745 847\n",
      " 239 442 686 318 449 485 222 739 446  62 264 547 815 821 651 726 505 810\n",
      " 626 613 507 689 557  19 757 838 715 262  13 825 707 282 552 573 493 416\n",
      " 103 835 187 420 243 834 162 750  27 811 680 297 548 221 788 716 682 864\n",
      " 224 211 616 347 479  26 576 706 451 654 283 673 434 601 827 791  46 524\n",
      " 816  18 457 161 818 820 257 131 784 645 826 563  29 236  41 767 783 465\n",
      " 671 207 748 560 410  72 663 229 724 362 690  17 248 209 230 667 138 664\n",
      " 695 542 517 582 491 418 535 455 797 655 281 752 819 527 177 649 599  90\n",
      " 218 674 730 212  43   5 377 814 260 771 530 242 285 572 556 381 435 731\n",
      "  61 569 736 612 842 684 223 648 399  47  20 558 123  80 502 447 646 641\n",
      " 751 333 762 342 219 763 729 204 506 540 152 251 261  42 453 197 532 546\n",
      " 503 163 780 773 329 160  97 637 636 734 188 860 419 761 332 313 846 467\n",
      " 550 487 619 390 360 537 156 653  40 710 629 855 681 850 623 642 483 555\n",
      " 305 438 758 232 559 119  67 273 781 770 354 456 592 843 522 408 489 593\n",
      " 470 658 583  11 501 790 265 688 792 300  53 768 461 772 614 857 378 252\n",
      " 425 195 511 304 755  21 311 430  89 374 515 310 497  66 128 140 176 278\n",
      "   1 567 466 823 339 482 832 484 328 475  88 836 216 488 543 858 407 562\n",
      " 627  96  59 677 657 403 432 406 241 114 696 158 830 375 367 215  83 439\n",
      " 125 450 445 295 200 574  14 743 137 397 672 609 441 628 538 351 492 170\n",
      " 544 694 634 214  32 303 774 615 685 490 602 396 469 632 786  69 398 795\n",
      " 766 656 837 789  84 566 742 698 412 603 749 733 585 275 444 523 812 206\n",
      " 259 605 513 756 796 596 154 175 110 208 702 122  65 309 500 800 545 277\n",
      " 741 459 255 280 343 525 373 587 699 424 147  34 777   6 279 228 132 480\n",
      "   4 357 144 794 798 575 184 130 190  51 247 728 389 165 146 570 518 831\n",
      " 844 740 561 848 581 411 307  24 647 180  99 353 495 586 590 723 618 519\n",
      " 711  54 237 376 564 520 112 428 862 839 851 775 274 700 554 423 737 476\n",
      " 414 341 809 334 785 813 155 335 404 431 189 624 718 714 106 422 205   3\n",
      " 856 443 494 370 145 401 185 148 697 709 705 153 852 611 109 625 578 429\n",
      " 372 202 721 787 597 116  48 661 720 413 371 196 531 314 692 579 356 849\n",
      " 863 392 326 369 630  44 294 514 405  57 683 276 665  78 352 643 349  87\n",
      " 394 533  58 565 433 427  35 776 760 127 478 738   2 133 421 571 104  64\n",
      " 296 828 159 594   9 355 336 415   0 289 181 256 361 588 172  74 769 179\n",
      " 595 234 460 233 385 712 266 640  86 620 437  91 393 859 765 386 536 340\n",
      " 288 306 782 323 732 464 481 139 166 149 363 652  45 509 387 693 713  31\n",
      " 249 607 105 845 365 395 157 508 499 551 701  56 516 805 793 534 268 584\n",
      " 633 171  92 350 639 286  28 315 235 840 577 182 210 348 727 272 753  23\n",
      " 121 321 284  68 472 364 263  85 124 117 142 168 299  12  50 317 322 409\n",
      " 666 194   7 344 799 631 267  22 384 452 747  76 143 366 617 250  36 691\n",
      " 359 269 807 173 312 254 346 244 100 679 258 238  81 759 174 608 670 178\n",
      " 473 302 391 193  73 703  16 778 111 319 141  98 225 226  49 164 135 668\n",
      "  55 120  70 400 417 735 779 539 379 118 220  30 217 231 270  95 388 167\n",
      " 803 271 719 169 324  37 841 115 298  33 245 253 717 725 337 604 136 287\n",
      " 129 198 345 107 454 293 291 331  75  38 191 458  93  52  60 448 802 102\n",
      " 113 320  79 290  94 380 192  10 327 186 134 316  25  71  15  39 108  77\n",
      " 246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (430, 43200) (430,)\n",
      "trainset after (440, 43200) (440,)\n",
      "updated train set: (440, 43200) (440,) unique(labels): [117 323] [0 1]\n",
      "val set: (855, 43200) (855,)\n",
      "\n",
      "Train set: (440, 43200) y: (440,)\n",
      "Val   set: (855, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 44\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.529 s \n",
      "\n",
      "Accuracy rate for 96.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  7 193]]\n",
      "--------------------------------\n",
      "val predicted: (855,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "probabilities: (855, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[613 237 545 746 756 661 238 696 504   8 636 181 844 598 642 211 433 823\n",
      "  27 148 381 793 583 366 514 520 652 197 667 185  63 851 482 306 738 207\n",
      " 413 560 299 679 534 467 670 736 459 201 651 798 590 519 473 800 423  20\n",
      " 809 439 630 572 700 470 199 843 464 811 614 665 316 655 452 522 592 521\n",
      "  47 581 568 837 328 654  66 437 443 290 812  26 718 813 540 731  46 356\n",
      " 336 627 676 481 565 360 149 668 498 555 219 796 678 681 380 674 742 129\n",
      " 501 415  61 454 643 780 698 699 549  89 281 528 819 602 707 240  18 279\n",
      " 708  17 737 160 446 510 759 489 417 432 323 836 815 295 517 802 807 444\n",
      " 828 161 773 749 234 637  13 743 585 608 810 775 641  62 458 803 227 714\n",
      " 832 574 722 340 740 783 824 541 222 728 825 656 744 535 525 228 499 258\n",
      " 193  24 478 593 687 210 754 271 647 337 355 159   5 331 646 118 548 260\n",
      " 605 475 672  87  29 854 666 782 262 640 561 175 716 552 209 629  72 168\n",
      " 621 100 202 659 564 539 850  69 615 102  88  41 205 584 298 345  19 448\n",
      " 217 817 311 537 673 225 663  43 533 431 808 721 483 435 379 450 747 606\n",
      "  40 461 764 135 416 397 591 327 158 604 496 682 136 544 669 776 500 763\n",
      " 650 750 550 638 840 309 479 487 352 628 404 618 833 249 109 735  96 566\n",
      " 255 358 466 216 113 372  95 634 820 523 330 186 436 542 293 575 645  21\n",
      "  11 806 495 246 816 407 505  32 515 375 530 497 419 784 710 758 649 702\n",
      " 442 822  59 848 241 594 626  42 485  82 453 221 303 422 551 726 547 156\n",
      "  67 508 220 723 559 465 847 376 611 772 349 471 755 126 765 138 283 814\n",
      " 429 150 690 332 492 845 347 188 789 778 276 163 633 619 195 760 518 531\n",
      " 462 484 688 388 130 677 826  65 447 753  53 387 463 405 326 562 607 762\n",
      " 280   6 173 733 648 273 680 396 253 694 601  80 368 781 476 259 766 456\n",
      " 488 183 480 230 620 427 239 804 278 308 302 567 790 511 409 839 154 373\n",
      " 305 536 512 394 827 354 852 411 403 124 486 145 250 490 664   1 178 425\n",
      " 597 438 365 732 577 801 703 235 686 353 408  34 441 554 805 553  14 538\n",
      " 108 212  23 245 715 125 257 198 838 624 767 494  83 400 734 692 748 206\n",
      " 472 122 153 691 513 428 741 301 821 263 595 578 516 333 792 421 491 395\n",
      "   3  51 786 834 639 391 697  57 610 111   4 787 274 616 582 440 524 829\n",
      " 573 579 152 841 622 272 653 588 214 402 426 571 788 371 174 226 558 392\n",
      " 725 350 689 420   9 635 348  45 546 842 570 374 194 399 729 474  86  35\n",
      " 297 342  78 720 769 277 657 142 275 603 684 701  90 557 105 367 713 128\n",
      " 351 264 506 617 410  31 144  54 155 556 675  64 341 706 589 359 846 424\n",
      " 779 418 507 121 401 187 370 203   0 757 704 526   2 587 143 752  98 151\n",
      " 724 430 164 147 768 853 761 412 324 339 304  76 284 644 543  58 266 213\n",
      " 390 818 312 777 157 712 586 363 632 321 162 131 529 177 393 477 204 383\n",
      " 115 849 457 170  85 286  28 730 338 182 248 460 200 256  99 369 502  56\n",
      " 166 705 334 503 140 179  91 171 384 116 576  48 797  44 774  74 232 693\n",
      " 580 835 631 292  55 231  81 361 685 563 434 527 310 469 287 103 307 146\n",
      " 317 180 254 208 569 612 382 344 468 247 137 493 313 719 625 785 104 745\n",
      " 233 282 599  70 830  12 658 320 509 357 362 739  50 751 315 120 406 294\n",
      " 139 623 172   7 252 364 449 683  22 791 609  16 242 385 671  73 662 270\n",
      " 319 119 218 215 799 265 600 389 223 695  36  68  84 191 414 251  49 123\n",
      " 770 300 236 532 267 192 771 224 261 346 243 167 727 165 660  37 117 141\n",
      " 322 795 711  33 169 269 229 386  94 176 398 285 134 831  75 377 717  30\n",
      "  38 112 596 106  97 343 451 794 289 196 114 110 296 133 455 325  92  10\n",
      " 335 291 445 329 318 268 709  60 127 190  79  52 378 184 314  15 288  25\n",
      " 189 101  93 132  71  39  77 244 107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (440, 43200) (440,)\n",
      "trainset after (450, 43200) (450,)\n",
      "updated train set: (450, 43200) (450,) unique(labels): [120 330] [0 1]\n",
      "val set: (845, 43200) (845,)\n",
      "\n",
      "Train set: (450, 43200) y: (450,)\n",
      "Val   set: (845, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 45\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.561 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       200\n",
      "           1       0.95      0.96      0.96       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[190  10]\n",
      " [  8 192]]\n",
      "--------------------------------\n",
      "val predicted: (845,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (845, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[834 148 479 180 630 420 578 636 210 645 624 841 648 658 147 621 353 692\n",
      " 430 510 663  46 710 567 800 593 461 287 783 730 470 560 378  26 377 827\n",
      " 660 672 516 802 813 646 637 434 788 674 799 464 555 196 585 803 790 467\n",
      " 440 296 159 576 456 184 363 200 786  12 536 436 608 597 587 518  62  17\n",
      " 671 276 661 728 216 224 699 537 515 723 495 394 714 325 815 101 530 706\n",
      " 320 700 793 517 667 734 809 631 410 749 128 635 198 805 333 603 218 252\n",
      " 833 690 443 478 649 732 640 498  25 455 486 257 736 328 429 513 237 278\n",
      " 404 451 569 814 524 659 449 472 729 221  61 506 412 544 233 818 259 458\n",
      " 563 792 496 770 580 691 292 650  88 531  42 826 226 219 766 588 600 773\n",
      " 174 414 669 765  16 735  71 441 797 529  99 720 740 837 753 556 550 801\n",
      " 708 206 277 822 599 521 840  19 246 559 342 135 772 355 547   5 313 433\n",
      " 303 763 798 807 586 665  28  86 268 535 204 357  95 653  52 540 641 445\n",
      " 844 680  60  65 158 754 796  45 117 741 447 745 493 634 644 208 324 238\n",
      " 125 476 428 579 806 256 370  41 632 482 639 656 295 327 612 519 484 254\n",
      " 810 243 185 337 538 463 450 209 229 255 468 475 308 675 419 192 615 570\n",
      " 713 533 545 622 755 543 804 481  87  94 494 823 779 554  40 157 504 762\n",
      " 376 715  79 628 609 334 774 160 480 606 694 108 546 432 460 830 738  66\n",
      " 497 201 247 401 323 718 134 526 623 601 835  18 590 373 561 838 260 459\n",
      " 812 167 372 501 492 589 683 485 215 511 666  20 670 439 620 752  31 756\n",
      " 121 112 768 225 280 750 299 627  58  39  23 514 532 172 557 727 681 227\n",
      " 369 744 643 613 123 129 306 618 748 673 746 527 220 424   1 726 534 137\n",
      " 426 842 273 402 657 211 642 614 489 173  81 444 794 305 300 435 385 725\n",
      " 397 602 149 817 413 400 542 194 811 596 662 141 462 574 346 679  13 771\n",
      " 483 816 393 236 477 512   3  68  50 352 205 687 453 780 562  82 162 234\n",
      " 270 272 302  10 418 695 153 549 572 502  64 819 362 422 275 592 828 120\n",
      " 739 438 491 507 508 757 384 290 605 702 329 791 831  33 733 193 509 406\n",
      " 349 330 469 213 685 416 274 124 344 573 368 548 151 577 488 298 598 836\n",
      " 568 778 712 689 271 777 182 553 365 417 717 647 269 155 405 203 684 177\n",
      "  89 610 782 389 114 724 127 348 371 776 611 437 408 242   6 503 795 541\n",
      " 471 566 583 144 396 633 682 552 581 181 629 832 261 769 824 392 843 187\n",
      " 391 565 616 551 473 110 197 767 107 829 721   4 398   0  57 407 758 104\n",
      " 584 743 250  47 668 707  85 677 347 423 759 705 421 698 487 152 350 336\n",
      "   8 522  77  53 651 520 764 399 338  34 364  97 212 415 704 304 808 693\n",
      " 425 156  22 747 457 291 380 145 130 499 427 409 751 387 143 474 716 366\n",
      " 142 722 351 163 539 523 321 625 150 388 582 331 638 604 294 136 335 686\n",
      " 500 356  56 176 230 454 775 839 345 711 571 619 202 381 309 283 564 199\n",
      "  90 525  27 575 186   2  30 102  84 558 787 339  55 367 245 678 626 431\n",
      " 696 169 284 289 825 318 170 697  44 301 607 676 115 232  63 251 594 165\n",
      " 239 146 103 505 360  98 179  75 742 446 652 390 737 231 178 490 244 664\n",
      " 655 465  43 312 314 820 119 139 279 154 253 310 358 263 359  21 731 168\n",
      " 161  73 382 281 207 466 617 341 789   7  80 190 122 171 403 267  67 262\n",
      "  83 316 223 354 595  11 760 411  35  49 307 317 654 140 138 235 386 528\n",
      " 118 343 719  72 249 688 781 361 264 379 761 191 297 383 703  54 214 258\n",
      "  69 116 217  48 175 248  93 240 228 821 374 266 166 113 222 591 164 709\n",
      " 109  32 395  36 340  15 701 785 133 784 452 105  96 442 286 195  74  29\n",
      "  91 293 448 132 319 265 332 288 282 126 326 111  37 322  59  51 183 100\n",
      " 189  78   9 188 285 375 311  24 131 315  70  14  92  38  76 241 106]\n",
      "trainset before (450, 43200) (450,)\n",
      "trainset after (460, 43200) (460,)\n",
      "updated train set: (460, 43200) (460,) unique(labels): [123 337] [0 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val set: (835, 43200) (835,)\n",
      "\n",
      "Train set: (460, 43200) y: (460,)\n",
      "Val   set: (835, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 46\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.332 s \n",
      "\n",
      "Accuracy rate for 94.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       200\n",
      "           1       0.94      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[188  12]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "val predicted: (835,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (835, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[681 618 774 430 663 639 628 426 524 293 579 683 701 407 827 194 602 779\n",
      " 445 432 550 777 804 350 615 654 640 790 531 463 374 725 457 360 530 721\n",
      " 198  12 637 818 182 831 375 719 284 505 571 474 511 597 651 273 744 682\n",
      " 671 466 800 330  46 781 409 690 792 490 581 196 824 512 665 594 697 215\n",
      "  28 624 796 436 532 587 526 322 720 691 508 784 460 545 510 562 425 658\n",
      " 493 501 783 813 660  16 652 213 555 629  99 519 452 218 300 558 806 754\n",
      " 439 325 793 727  26 756 794 649 513 662 411 254 135 525 551 740 317 158\n",
      " 644 249 736 641 289   5 591 173  45 159 429 542 761 128 221  71 552 705\n",
      " 798 401 714 757 759  17 699 791 223 731 479 256 789 468 574 235 190 788\n",
      " 726 275 234 481 101  61 723 650 514 451 647 354 521 265  40 805 156 564\n",
      " 230 625 147  19 447 252 204 535  62 339 274 739 454 539 633 506 216  42\n",
      " 582 609 803 809 632 834 801 743  41 202 580 797 516  65 817 787 310 540\n",
      " 437 224 492 253 117 554  79 711 491 627 441 459 830 593 745 573 528 821\n",
      " 533 370 603 157 763 661 244 685 475 583 391  52 443 206 476 226 538 398\n",
      " 765 732 334  18 303  87  25 488 305 134 631 764 616 199 456 656 240 477\n",
      " 472 373 549 828 270 612 324 217 428 770 424 166 666 617 489 367 422 584\n",
      "  60 565 331 208 704 471 251 795 480 243 435 753 125 487 541 352 606 458\n",
      " 729 148 509  86  66 207 657 292  39  68  95  94 413 321 183  10 416 556\n",
      " 369 825 807  88 410 718 814 672 586 747 496 222 212 415 446 607 686 123\n",
      " 636 746 465 499 622 595 608 706 709 529 277 233  31 112 833 599 464 600\n",
      " 440 678 724 808 152 614  20 449 349 572 592 382 785 484 267 527 231 473\n",
      " 359 486 741 394 366 434 664 417 455 137 171 121 420 172 621 388 390 296\n",
      " 192 653 431 302 596 737 748 129 693 822  81 634 638 503 544 676 735 716\n",
      " 108 403 730 762  58 176 397 478 297 648 674 717 668 295 418   1 522 567\n",
      " 381 399 537 557 771 257  64 635  82 320 502  23 258 185 670 191 703  50\n",
      " 802 482 768 120 715 203 343  33 782 698 287 684 590 675 154 483 680 272\n",
      " 247 815 453 497 561 210 144 299 271 346 161 414 568 773 577 195 507 368\n",
      " 326 819 150 341   3 563 269 515 708 760 405 404 826 786 239 412 543 569\n",
      " 104 180 810 610 402 605 127 141  77 832 604 673 548 707 767 389  13 820\n",
      " 712  97 467 347 578 179 559 433 469 517 575 570 769 626 738 348   4  22\n",
      " 110 504 560 536 396 386 266 114 365 659 268  57   6 362 755 546 758 689\n",
      "  85 395 209 327 423 696 823  56 124 335 829 201 613 498  89 184 749 393\n",
      " 345  34 151 107 494 197 143  53 361  47 598 333 130   2 695 750 547 566\n",
      " 421 623 642 742 734 364 318 667 145 406 332 344 799 288 162 328  30 780\n",
      " 301   0  43 286 688 576 146 200 442 630 534 385 470   8 142 377 155 450\n",
      " 778 619 363 315 280 353 227  63 177 419 149 306 276 384 387 495 175 713\n",
      " 520 677 588 248  98 620 298 378 702 518  21 311 136 168 646 102 611  27\n",
      "  90 205 357  84 816 342 427 242 291 103  55 728 379 309 643 462 669 241\n",
      " 228 164 169 553 229 811 119  80 281 722 687 733  73 752 153 766 485 500\n",
      " 601 655 400 351 260 356 336   7  44 188  49 307 408 259 589  67 304 314\n",
      " 461 278 118 115 140 178  54  11  75 710 122 376 340 250 160 139 236 358\n",
      " 167 523 313 264  83 246 338 694 138 355  35 751 214  69 170  72 772  48\n",
      " 211 255 245 189 776 383 645 261 232 219 294 812 380 174 679 116 585  15\n",
      " 371 220 444 700  93 438 392 237 225 163 263  36  74 165 133  29 105 109\n",
      " 290 448 337  96 193 283 132  32 262 113 775 285 279 329 692  91 316 323\n",
      "  37 126 111  59 181 319  51 100  24  70 372 312  14  78 308 131 187   9\n",
      " 282 186  92  38  76 238 106]\n",
      "trainset before (460, 43200) (460,)\n",
      "trainset after (470, 43200) (470,)\n",
      "updated train set: (470, 43200) (470,) unique(labels): [124 346] [0 1]\n",
      "val set: (825, 43200) (825,)\n",
      "\n",
      "Train set: (470, 43200) y: (470,)\n",
      "Val   set: (825, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Iteration: 47\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.382 s \n",
      "\n",
      "Accuracy rate for 95.500000 \n",
      "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       200\n",
      "           1       0.96      0.95      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.96      0.95      0.95       400\n",
      "weighted avg       0.96      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [ 10 190]]\n",
      "--------------------------------\n",
      "val predicted: (825,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (825, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[780 526 522 502 642 674 508 460 433 429 821 373 808 349 273 644 647 769\n",
      " 406 598 329 817 771 444 716 284 783 194 558 712 657 527 692 784 794 631\n",
      " 551  12 449 513 767 577 454 218 463  16 521 613 575 710 487 645 718 681\n",
      " 424  46 731 590 359 611 583 782 321 781 714 374 752 790 101 457 623 216\n",
      " 799 507 546 249 510 682 814 471 196 773 135 410 509 538 528 505 705 593\n",
      " 182 690 275 803 198 626 711 427 589   5 655 448 541 696 619 498 651 747\n",
      " 147 485 400 158 673 688 717 518 795 748 695 408 128 634 576 663 796 567\n",
      " 256 442 478 221 316 493 451 774 786 516 436 633 465  45 610  26 554 421\n",
      " 289 254 804 265  28 234 434 299 820 547 390  17 653  61 818  25 552 490\n",
      " 587  42 223 622 778 529  71  99 824 569 637 791 702 745 338 535 324 761\n",
      " 409 486 736 709 534 215 779 333 722 723 456 489 353 643 274 545 157  52\n",
      " 173 366 627 578 224 570 756  79  66 730 550 488 548 496 797 788 735 235\n",
      " 499 511 204 251 654 531 620 426  62 612 230 159 474 732 469 244 481 793\n",
      " 423 777 443 807 438 432 455 468 117 253  19 787 476 243 240 658 536 605\n",
      " 207 720 676 737 727 503 440 292 617  60 561 202 755 222 754  87 640 213\n",
      " 419 252 183 811  41 734 472 649 199 473 208 453 650  86 629 323 206  65\n",
      " 172  95 302 217 602  39 579 750 369 560 166  40 372 684 599 190  64 470\n",
      " 112 580 524 226 537 156 437 351  18 744 415 134 148 625 785 700 608 368\n",
      " 358 304 596 330  20 389  88 108 484  31 753 707 393 641 591 815 506 231\n",
      " 121 697 277 475 728 519 477 320 595 212 319 402 404 656 664 670 397 270\n",
      " 588 257 604 152   1 630 309 666 348 267 646 523 398 123 417 171 738 822\n",
      " 798 387 176 296 563 582 533 809 564 616  50  58 125  10 342 137 726 483\n",
      " 677 446 632 233 539 381 628 287 660 775 662 239  68 568 823 428 466 462\n",
      " 479 592 272 367 301 192  23 154 461 412 295 414   6 739 452 540 185 586\n",
      " 294 721 201 759 708 792 504 365 494 396 525 120 210 431 772 512 500 129\n",
      " 603 413  94 812  33 269 813 401 532 699 385  82 203 388 776 191 271 687\n",
      " 411 565 715 416 810 380  81 209 706 247 668 141  85 694   3 392 298 150\n",
      " 110 805 326 573 340 450  89 130 601 672 703 501 557 544  13 127 758 266\n",
      " 667 114 760 403 749 179 345 553 762 689   4 104 364 543 675 430 459 764\n",
      " 268 464 161 144 334 361  47 606  57 480 145 816  77 665 258 698  53 741\n",
      " 325 180 124   0 305 422 618 300 621 600 347 405  97 800 733 559 383 542\n",
      " 725 635 195 327 420 394 495 332 280 789 107 556 571 555 310   2 362 572\n",
      " 514 751  22 363 614 574 162 341 317 360 594 352 184 395 729 343  27 652\n",
      " 155  43 530 482 680 314 746 197 686 492  30 142 819 344 562 659  34 200\n",
      " 151 276 177 291 102  84 248  56 376 346 669 491 517 286 806 679 740 168\n",
      " 418  21 143 384 693  80 288 331   8 566 175 724 146 149 661 678  63 103\n",
      " 624 229 636 549 227 281 615 609 260 136 122 297 425 447  90 704 378 386\n",
      "  55 399 768 228 757 597 355  83 458 467 241 167 377 439 139 308 242 169\n",
      " 639 205 607 306   7 515 119 350 719 164 356  73 584 188 648 801 232 354\n",
      " 713 742 407 118 140 259  44 335 770 178 303 115  67 497  98 211 236 337\n",
      " 264 312 743  35 250 153  75  69 701 261 339 379 585 375 763 671  54 160\n",
      " 214 138 189  49 220 313 293  11 170 638  48 520 278 219 109 382  72 255\n",
      "  93 391 225 245  36 263  15 246 685 116 581 174 370 766 445 357 113 262\n",
      "  74 165  96 691 802 328 237  32 283 336 163 132  29 133 193 285 105 441\n",
      " 290 683 322  91 435  37 279 765 126 100 315 181  59 318 371  14  51 307\n",
      " 187 311 131 111 282  24 186  70  78   9  92  38  76 238 106]\n",
      "trainset before (470, 43200) (470,)\n",
      "trainset after (480, 43200) (480,)\n",
      "updated train set: (480, 43200) (480,) unique(labels): [124 356] [0 1]\n",
      "val set: (815, 43200) (815,)\n",
      "\n",
      "Train set: (480, 43200) y: (480,)\n",
      "Val   set: (815, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 48\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.439 s \n",
      "\n",
      "Accuracy rate for 95.000000 \n",
      "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       200\n",
      "           1       0.95      0.94      0.95       200\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[191   9]\n",
      " [ 11 189]]\n",
      "--------------------------------\n",
      "val predicted: (815,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "probabilities: (815, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[349 373 760 639 591 784 701 570 194 452 703 284 672 707 551 539 807 329\n",
      " 359 636 649 798 583 772  12 758 811 637 780 484 218 520 604 460 442 647\n",
      " 198 774 544 468 455 804 406 576 505 101 568 273 762 374 709 586 764 447\n",
      " 434 504  16 196 521 722 440  28 495 508 683 516 501 256 773 321 743 624\n",
      " 789 681 424 475 449 705 776  17 786 739 560 503 679 619 673   5 182 612\n",
      " 221 645 643 582 696 275 665  46 655 135 616 462 793 531 687 427 128 765\n",
      " 540 606 289 770 686 534 408 213 626 738 769 513 726 771 446 316 785 324\n",
      " 810 580 708  99 814 410 147 215 511 249 338 635 487 528 353 485  71 714\n",
      " 547 299 630 235 627 569 752 224 254 713 506 702  52 216  26 615 524 620\n",
      " 486 158 173 778 693 595 390 718 441 400 234 483 206 490 768  61 482 783\n",
      " 527 204 274 436  45 230 470  25 563 223 736 409 727 202 522 499 646 473\n",
      " 541 207  88 243 265 432  87 777 333 633 252 543 641 571 451  79 794 605\n",
      " 728 787 426  62 808 366 421 538 159 471 603 747 529 240 372 431 369  42\n",
      " 454  18 650 613 700 553 423 545 721 397 711 493 610 226  19 667  41 166\n",
      " 419 801 466 572 292 438 148 691 467  86 725 589  40 723 781 609  66 244\n",
      " 741 253 746 562 478 592 634 190 797 745 212 465 156 598 323 222 554 648\n",
      " 435 735 134 157 573 481 217  39   1 199 719 302 530 775 622 453 368 805\n",
      "  94 389 277 309 474 330 601 172 642 304 618 502 117 469 208 717  60 270\n",
      " 251 496 688   6 533 320 744 518 296 402 675 358  65 351  31 295 381  95\n",
      " 584 257 575  20 137 393 459 319 556 176 287 517 183 152 729 557 472 597\n",
      " 652 581 766 123 698 623 125  10 267 342 656 750 121 112 782 662 596 398\n",
      " 491 654 588 514  50 658 621 444 526 185 788 625 561 480 387 415 413 428\n",
      " 231 638 239  64 813 690 430 414 712 108 668 585 348 566 579 404 171 450\n",
      " 272 301 458 685 396 367  68 129 417 120 558 500 294 740 412 258 365 401\n",
      " 271 416 476 154 380 749 730 706 192 233 150 799  33 659 195 751 537  81\n",
      " 298 203 532 463 201 191 699  58 763 497  23 812 519  13 269 694  82 361\n",
      " 507 803 678 498 802 767 180 660 247 210 664 345 448 666 755 104 392 266\n",
      " 179 411 209  77 800 403 535 806 141 110 546 753 130 385  85 429 162 144\n",
      " 525 388 127 550 461 657 680 326 732 364 340 697 477 344  53 552   4 593\n",
      " 334 352 599 671 114 325 795  47 779  97 327 197 614 689 536 363   0 720\n",
      " 347 594 161 549 145 317 143 790 268 492 564 422 305 383 611 628  57 394\n",
      " 509   3   2 567 809 184 395 360 724 742 288 124 200 716  34  56 151 405\n",
      " 548 479 651  22 644 362 737 677 314 229  89 731 489 420 310 457 343 332\n",
      " 155 607 346 670 300 280 512 107 341 542 523 565 488 331 661 286 608 555\n",
      " 418 653 684 146  27  63  30 587 796 629 102 559 168 425 617 445  84  43\n",
      " 177 759 142 248 602 227  80 276 378  21 386 376 695 205 291 464 748 297\n",
      "  90 590  55 149 384 715 122 175 281 103   8 260 228 510  83 136  73 377\n",
      " 437   7 456 354 632 399 669 710 577 355 167 242 640 791 356 704  98 350\n",
      " 734 308 178 153 169 119 241 188 306 164 600 139 494 692 733 407 259 303\n",
      "  67 339  35 761 261 754 264 140 115 236  48 375 313  69  44 232  11 118\n",
      " 335 663 391  75 250  49 255 312 578 214 631 293 278  72 245 219 757 379\n",
      " 246 337 170 211 138 160 382 515 174 357  54 225 676 116  93 220  15 189\n",
      " 132 574 370 263 163 792 109  36 682 439 237 674 336 105 443 113  32  74\n",
      " 165 133 262  29 290 328  96 285 283 193 279 126 433  91 322 315  37 756\n",
      "  51 181  59 371 100 131 318 111 307 282  78 311  70 187 186  24  14   9\n",
      "  92  38 238  76 106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset before (480, 43200) (480,)\n",
      "trainset after (490, 43200) (490,)\n",
      "updated train set: (490, 43200) (490,) unique(labels): [127 363] [0 1]\n",
      "val set: (805, 43200) (805,)\n",
      "\n",
      "Train set: (490, 43200) y: (490,)\n",
      "Val   set: (805, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 49\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.494 s \n",
      "\n",
      "Accuracy rate for 94.250000 \n",
      "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       200\n",
      "           1       0.94      0.94      0.94       200\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  11]\n",
      " [ 12 188]]\n",
      "--------------------------------\n",
      "val predicted: (805,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "probabilities: (805, 2) \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[763 699 638 516  46 640 642 437 357 676 405 535 750 631 456 530 728 320\n",
      " 403 424 765 536  16 783 480 665  12  52 630 497 540 439 564 701 730 788\n",
      " 695 182 797 272 507 764 801 648 328 753 755  25 283 451  42 500 501 794\n",
      " 464  62 674 212 735 771 618 147 288 431 620 598 444 197  28 581 517 543\n",
      "  17 255 371 203 471 483 159 195 578 666 700 710 606 689   5 512 636 756\n",
      " 504 718 714 491 421 658 173 234 509 776 639 446 697 744 274 575 614 703\n",
      "  61 706 719 800  99 600 787 443 158  71 524 101 705 458  86 423 323  45\n",
      " 680 547 629 624 767 214  26 610 737 438  79 779 613 731 769 571 499 566\n",
      " 407 804 565 527 609 220 523 469 567 556 760  60 478 686 502 586 298 621\n",
      " 463 762 558 387 418 253 248 761 479 222 534 577 465 242  19 495 135 713\n",
      "  31 672 428 481 337 520 229  41 603 223 251 529 694 627 233  88 591 332\n",
      " 645  40 632 774 679 733 482 406 429 798 450 217 409 117 157 597 315 291\n",
      " 559 432 592 684 351 539 553 514 397 467 784 349 225 759 156 715 435 128\n",
      " 273 410 206 775 518 525 537 449 486 607 319 720 589 541 595 462 433 207\n",
      " 205 252 239 772 370 461 322 303 612 791 498 643 364 660 668 634 768 329\n",
      " 599 717 216 413 394 489 466 604 635  64 243 477 318 264 568 420 773  94\n",
      " 550  18 201 215 628 172 510 739 123 738 448 693  87 584 470 617 308 230\n",
      " 166 474 148 341 301  10 503 777 526 641 615 579 190 198 414 286 221   6\n",
      " 766 742   1 616 649 590 441 367 386 795 134 269  66 619 583 562 711 412\n",
      "  20 129  95 790 492 681 238 652  65 416 211 344 698 297 493 183 741 112\n",
      " 549 295 709 721 691 125 727 515  68 366 570 661 487 754 427 657 455 294\n",
      " 736 154 232 365 447 650 276 722 468 445 378 513 655 522 408 653 390 552\n",
      " 152 246 557 692 789 108 803 704 384  23 339 496 257 425  82 574 758 792\n",
      " 651 580 663 161 256 137 325 356 785 546 647 472  39 347 745 542 359 266\n",
      " 363 401 476 120 802 268 411 121 104 554 192 757 250 683 399 185 454 682\n",
      " 793  50 659 778 293 459 191 576 389 712 732 141  58 180 202 194 271 770\n",
      "  81 528 743 377 395 270 400 457 176 494 716 150 300 533   0 473 671 200\n",
      " 664 678 171 110 393 687 611 708  85 747 124 602 382   3  33 392 690 130\n",
      " 796 402 343 505 267 521 622 608 453 724 426 316 398 265 593 673 350 508\n",
      " 340 345 644 799 548 588 531 587 419 324 196 346 381 144  30  53 209 734\n",
      " 582 385 654 519  77 417 544 442 488 545  13 151 637 127 560 532 605  22\n",
      " 179  89 555 670  97 208 114 601  57 404  27 391 362 780 309 360 563 326\n",
      "  34 729 199  90 342 751 358 290 723 296 626 475 380 143 561 484 184 287\n",
      " 279  47  56 623 146 361 333 139 702 313 551 460 228 786 434 162 485   4\n",
      " 646 145 538 696 749 299   8 177 204 331 155 304 781 122 677 136 415  55\n",
      "  63 596 241 585  84  98 506 422 330 142 175 259  21 285 572 752 169 707\n",
      " 685 107 740 102 594  80 373 226 119 280 726 354 168 436 375 662 149 688\n",
      " 153 656 247  75 490 307   2  54 334 383 452  73 633 348 178 746 240 396\n",
      " 164 277 353 103   7 115 118 188 352 275 305  44  43 227  49 210 336 249\n",
      " 725  83 374 573  67 213 258 235 167 669  11 312  15 302 569 140 160  35\n",
      " 263 138 244 675 116 219 372 511 379  48 311 782 376  72 625 170 260 338\n",
      " 355 218 254  69 245 236  36 231 189 368 335 430 292 388 105 174  74 165\n",
      "  29 262 440 667 224  32  93 163 133 289 113 748 132 193 109 284  91 327\n",
      " 278  37 321 261 126  96 314 282 317  59  51 187 369 100 181 306 111  78\n",
      " 281 310   9  24 131  70  14 186  92  38  76 237 106]\n",
      "trainset before (490, 43200) (490,)\n",
      "trainset after (500, 43200) (500,)\n",
      "updated train set: (500, 43200) (500,) unique(labels): [129 371] [0 1]\n",
      "val set: (795, 43200) (795,)\n",
      "\n",
      "Train set: (500, 43200) y: (500,)\n",
      "Val   set: (795, 43200)\n",
      "Test  set: (400, 43200)\n",
      "training multinomial logistic regression\n",
      "--------------------------------\n",
      "Iteration: 50\n",
      "--------------------------------\n",
      "y-test set: (400,)\n",
      "Example run in 2.326 s \n",
      "\n",
      "Accuracy rate for 95.750000 \n",
      "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "          random_state=None, solver='saga', tol=0.1, verbose=0,\n",
      "          warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       200\n",
      "           1       0.96      0.95      0.96       200\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[192   8]\n",
      " [  9 191]]\n",
      "--------------------------------\n",
      "final active learning accuracies [68.75, 79.0, 93.25, 97.25, 97.0, 97.0, 96.75, 95.0, 95.5, 95.5, 96.75, 96.25, 97.0, 96.0, 97.5, 97.25, 97.0, 97.0, 97.75, 96.75, 96.75, 98.0, 97.5, 97.75, 97.5, 97.5, 96.25, 98.0, 97.0, 97.25, 96.5, 96.25, 97.0, 96.0, 96.25, 96.0, 96.0, 96.0, 97.0, 96.25, 96.0, 95.75, 96.0, 96.25, 95.5, 94.75, 95.5, 95.0, 94.25, 95.75]\n",
      "\n",
      "---------------------------- FINISHED ---------------------------\n",
      "\n",
      "{'HOG': {'LogModel': {'RandomSelection': {'10': [[76.75, 84.75, 88.5, 94.25, 95.5, 95.75, 96.5, 96.25, 94.75, 95.5, 94.75, 96.25, 95.5, 95.5, 95.5, 95.25, 95.75, 96.0, 95.25, 95.5, 95.0, 95.0, 95.0, 94.75, 94.75, 94.5, 93.75, 95.0, 93.25, 93.0, 93.5, 93.0, 92.5, 93.75, 93.5, 93.75, 93.75, 92.75, 93.0, 93.25, 92.5, 92.75, 93.5, 93.25, 93.25, 93.25, 93.25, 92.75, 93.25, 92.75]]}, 'MarginSamplingSelection': {'10': [[64.25, 83.25, 72.5, 98.25, 97.75, 97.5, 96.75, 97.75, 96.0, 98.0, 97.75, 99.0, 98.0, 97.5, 98.5, 97.5, 98.25, 98.0, 98.0, 97.25, 97.5, 97.0, 97.25, 97.25, 97.5, 97.5, 97.0, 97.5, 97.0, 96.75, 96.25, 97.5, 97.25, 97.25, 96.5, 97.0, 96.25, 96.25, 96.5, 96.0, 95.75, 96.5, 96.25, 96.5, 95.5, 95.0, 96.0, 95.5, 95.0, 95.25]]}, 'EntropySelection': {'10': [[68.75, 79.0, 93.25, 97.25, 97.0, 97.0, 96.75, 95.0, 95.5, 95.5, 96.75, 96.25, 97.0, 96.0, 97.5, 97.25, 97.0, 97.0, 97.75, 96.75, 96.75, 98.0, 97.5, 97.75, 97.5, 97.5, 96.25, 98.0, 97.0, 97.25, 96.5, 96.25, 97.0, 96.0, 96.25, 96.0, 96.0, 96.0, 97.0, 96.25, 96.0, 95.75, 96.0, 96.25, 95.5, 94.75, 95.5, 95.0, 94.25, 95.75]]}}}}\n",
      "1\n",
      "{'HOG': {'LogModel': {'EntropySelection': {'10': [[68.75, 79.0, 93.25, 97.25, 97.0, 97.0, 96.75, 95.0, 95.5, 95.5, 96.75, 96.25, 97.0, 96.0, 97.5, 97.25, 97.0, 97.0, 97.75, 96.75, 96.75, 98.0, 97.5, 97.75, 97.5, 97.5, 96.25, 98.0, 97.0, 97.25, 96.5, 96.25, 97.0, 96.0, 96.25, 96.0, 96.0, 96.0, 97.0, 96.25, 96.0, 95.75, 96.0, 96.25, 95.5, 94.75, 95.5, 95.0, 94.25, 95.75]]}, 'MarginSamplingSelection': {'10': [[64.25, 83.25, 72.5, 98.25, 97.75, 97.5, 96.75, 97.75, 96.0, 98.0, 97.75, 99.0, 98.0, 97.5, 98.5, 97.5, 98.25, 98.0, 98.0, 97.25, 97.5, 97.0, 97.25, 97.25, 97.5, 97.5, 97.0, 97.5, 97.0, 96.75, 96.25, 97.5, 97.25, 97.25, 96.5, 97.0, 96.25, 96.25, 96.5, 96.0, 95.75, 96.5, 96.25, 96.5, 95.5, 95.0, 96.0, 95.5, 95.0, 95.25]]}, 'RandomSelection': {'10': [[76.75, 84.75, 88.5, 94.25, 95.5, 95.75, 96.5, 96.25, 94.75, 95.5, 94.75, 96.25, 95.5, 95.5, 95.5, 95.25, 95.75, 96.0, 95.25, 95.5, 95.0, 95.0, 95.0, 94.75, 94.75, 94.5, 93.75, 95.0, 93.25, 93.0, 93.5, 93.0, 92.5, 93.75, 93.5, 93.75, 93.75, 92.75, 93.0, 93.25, 92.5, 92.75, 93.5, 93.25, 93.25, 93.25, 93.25, 92.75, 93.25, 92.75]]}}}}\n",
      "Time used: 491.34950299999764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    " \n",
    "start = time.clock()\n",
    " \n",
    "#get the dataset here\n",
    "\n",
    "\n",
    "  \n",
    "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
    "    algos_temp = []\n",
    "    print ('stopping at:', max_queried)\n",
    "    count = 0\n",
    "    for choice in choices:\n",
    "        if choice not in d:\n",
    "            d[choice] = {}\n",
    "            (X_train_full,y_train_full,X_test,y_test)=getdataset(choice)\n",
    "        for model_object in models:\n",
    "            if model_object.__name__ not in d[choice]:\n",
    "                d[choice][model_object.__name__] = {}\n",
    "            for selection_function in selection_functions:\n",
    "                if selection_function.__name__ not in d[choice][model_object.__name__]:\n",
    "                    d[choice][model_object.__name__][selection_function.__name__] = {}\n",
    "\n",
    "                for k in Ks:\n",
    "                    d[choice][model_object.__name__][selection_function.__name__][str(k)] = []           \n",
    "\n",
    "                    for i in range(0, repeats):\n",
    "                        count+=1\n",
    "                        if count >= contfrom:\n",
    "                            print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
    "                            alg = TheAlgorithm(k, \n",
    "                                               model_object, \n",
    "                                               selection_function\n",
    "                                               )\n",
    "                            alg.run(X_train_full, y_train_full, X_test, y_test)\n",
    "                            d[choice][model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
    "                            if count % 5 == 0:\n",
    "                                print(json.dumps(d, indent=2, sort_keys=True))\n",
    "                            print ()\n",
    "                            print ('---------------------------- FINISHED ---------------------------')\n",
    "                            print ()\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "d = {}\n",
    "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
    "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
    "# print(json.dumps(d, indent=2, sort_keys=True))\n",
    "\n",
    "d = experiment(d, models, selection_functions, Ks, 1, stopped_at+1)\n",
    "print (d)\n",
    "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
    "print('1')\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elapsed = (time.clock() - start)\n",
    "print(\"Time used:\",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Raw': {'SvmModel': {'RandomSelection': {'10': [[59.5, 67.25, 64.0, 67.75, 70.75, 73.75, 74.5, 72.75, 69.75, 71.25, 72.5, 71.5, 72.75, 72.75, 73.0, 73.25, 73.25, 72.75, 73.25, 73.75, 74.75, 73.75, 73.5, 72.25, 72.25, 72.5, 73.25, 73.75, 74.0, 74.5, 74.25, 75.25, 76.0, 76.25, 76.25, 76.25, 75.5, 75.5, 75.5, 75.75, 76.0, 75.75, 76.0, 76.25, 75.75, 75.25, 75.0, 75.25, 75.5, 75.25]]}, 'MarginSamplingSelection': {'10': [[75.5, 74.75, 81.5, 78.5, 78.25, 80.0, 83.25, 85.25, 83.0, 85.5, 81.0, 84.75, 83.0, 83.75, 85.5, 84.5, 83.75, 83.25, 83.5, 83.25, 83.5, 84.25, 84.0, 84.0, 84.0, 85.25, 85.5, 85.75, 85.0, 85.0, 85.0, 85.0, 84.75, 84.75, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.5, 84.75, 84.75, 84.75]]}, 'EntropySelection': {'10': [[61.75000000000001, 75.25, 73.25, 72.0, 74.5, 77.25, 80.25, 81.25, 81.5, 80.5, 81.0, 81.25, 80.0, 85.75, 86.75, 87.0, 87.25, 86.75, 86.75, 86.0, 86.75, 85.5, 85.75, 86.0, 85.5, 86.25, 85.75, 85.75, 86.0, 85.5, 85.0, 85.0, 84.75, 84.5, 84.5, 84.5, 84.5, 84.75, 84.5, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75, 84.75]]}}, 'RfModel': {'RandomSelection': {'10': [[74.25, 67.0, 66.0, 62.25000000000001, 69.5, 73.0, 72.5, 72.0, 72.5, 74.25, 72.25, 71.5, 71.0, 73.5, 73.0, 74.5, 74.25, 72.75, 74.0, 73.75, 73.0, 72.5, 73.75, 71.5, 71.75, 71.0, 72.25, 72.25, 71.25, 71.25, 72.25, 71.5, 71.25, 71.75, 72.25, 70.5, 70.0, 71.75, 71.5, 70.75, 71.5, 71.0, 71.25, 73.25, 72.25, 72.5, 72.0, 72.0, 73.5, 73.25]]}, 'MarginSamplingSelection': {'10': [[67.25, 61.5, 69.75, 72.0, 73.0, 75.0, 72.75, 74.25, 75.25, 78.0, 80.25, 79.25, 81.75, 81.25, 80.25, 83.0, 84.5, 83.5, 82.25, 80.25, 80.75, 81.25, 81.75, 84.0, 79.5, 79.5, 81.25, 78.5, 79.75, 79.5, 79.0, 78.0, 78.75, 78.25, 77.75, 78.0, 78.0, 77.5, 77.75, 76.5, 78.0, 76.5, 77.75, 75.25, 76.0, 77.25, 75.75, 76.5, 76.75, 77.0]]}, 'EntropySelection': {'10': [[50.24999999999999, 67.5, 69.0, 65.25, 72.75, 75.5, 74.5, 77.75, 77.25, 76.0, 74.75, 76.5, 77.5, 79.0, 81.5, 77.75, 81.25, 80.75, 79.75, 79.25, 79.25, 83.0, 78.75, 80.5, 79.5, 80.25, 78.25, 79.75, 78.25, 78.25, 79.0, 78.5, 78.5, 78.0, 77.75, 78.5, 77.25, 76.25, 77.25, 76.75, 77.0, 77.0, 76.75, 76.75, 76.75, 76.75, 75.75, 76.5, 76.75, 76.5]]}}, 'LogModel': {'RandomSelection': {'10': [[62.0, 69.0, 71.75, 73.75, 74.75, 74.75, 74.5, 69.5, 72.0, 71.75, 73.75, 72.0, 71.0, 69.75, 68.5, 67.5, 72.0, 72.25, 71.75, 72.75, 72.5, 73.75, 73.5, 72.5, 73.75, 73.25, 74.5, 71.75, 72.5, 72.25, 71.75, 71.5, 72.0, 70.5, 72.0, 71.5, 70.5, 71.0, 71.5, 71.0, 72.0, 70.5, 71.0, 71.5, 72.25, 71.75, 72.75, 71.5, 71.75, 71.0]]}, 'MarginSamplingSelection': {'10': [[64.25, 77.25, 78.0, 77.25, 78.75, 75.5, 79.25, 79.0, 79.75, 76.0, 77.0, 79.0, 79.75, 79.5, 79.25, 81.0, 82.0, 79.75, 80.25, 83.0, 84.0, 82.75, 76.0, 75.75, 83.75, 79.5, 82.0, 75.25, 81.5, 69.75, 81.5, 74.75, 71.5, 67.5, 61.25000000000001, 78.0, 79.75, 66.25, 65.0, 77.0, 77.5, 78.0, 79.75, 80.0, 77.75, 78.5, 79.5, 79.25, 63.74999999999999, 78.25]]}, 'EntropySelection': {'10': [[51.0, 51.0, 56.25, 60.25, 54.25, 67.25, 68.75, 73.0, 72.25, 77.25, 79.0, 80.0, 82.0, 80.25, 78.75, 79.25, 80.5, 78.75, 80.25, 78.0, 82.0, 80.5, 78.25, 81.0, 80.5, 82.0, 80.75, 79.25, 78.75, 82.5, 81.0, 81.25, 82.5, 80.5, 81.25, 80.5, 81.0, 79.25, 80.0, 80.5, 80.25, 80.25, 76.5, 80.5, 80.0, 81.0, 81.0, 80.75, 81.0, 83.5]]}}}, 'Pretrained': {'SvmModel': {'RandomSelection': {'10': [[85.25, 93.0, 95.0, 93.0, 94.0, 96.25, 96.5, 97.5, 97.75, 97.5, 97.25, 97.75, 97.0, 97.25, 97.0, 97.75, 98.75, 99.25, 98.5, 98.75, 98.5, 99.5, 99.75, 99.75, 99.5, 99.5, 99.5, 99.5, 99.5, 99.75, 99.75, 99.75, 99.75, 99.5, 99.5, 99.5, 99.25, 99.25, 99.0, 99.0, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.25, 99.25, 99.5, 99.25]]}, 'MarginSamplingSelection': {'10': [[77.25, 80.0, 81.5, 78.75, 83.75, 85.25, 87.5, 87.0, 99.25, 99.5, 100.0, 100.0, 99.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]]}, 'EntropySelection': {'10': [[82.75, 62.0, 95.25, 98.5, 97.0, 97.25, 98.0, 97.25, 97.5, 97.5, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75, 97.75]]}}, 'RfModel': {'RandomSelection': {'10': [[98.5, 99.5, 97.75, 98.25, 99.25, 99.5, 98.75, 99.5, 99.0, 99.25, 99.25, 99.25, 99.0, 99.25, 98.5, 98.75, 99.0, 99.75, 99.0, 99.75, 99.5, 99.0, 98.75, 99.75, 99.5, 98.75, 99.5, 98.75, 99.25, 99.75, 99.5, 99.5, 100.0, 99.5, 99.75, 99.75, 100.0, 99.0, 100.0, 99.75, 100.0, 99.5, 99.75, 99.75, 99.75, 100.0, 100.0, 100.0, 100.0, 99.75]]}, 'MarginSamplingSelection': {'10': [[91.5, 84.25, 99.75, 94.5, 99.75, 99.75, 99.75, 100.0, 100.0, 100.0, 100.0, 99.75, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.75, 100.0, 100.0, 100.0, 100.0, 99.75, 100.0, 100.0, 99.75, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]]}, 'EntropySelection': {'10': [[79.25, 94.25, 99.75, 96.5, 99.75, 99.5, 99.75, 99.75, 99.75, 100.0, 99.75, 99.75, 99.75, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.75, 100.0, 100.0, 100.0, 99.75, 100.0, 100.0, 100.0, 100.0, 100.0, 99.75, 100.0, 99.75, 100.0, 100.0, 100.0, 100.0, 100.0, 99.75]]}}, 'LogModel': {'RandomSelection': {'10': [[95.5, 99.75, 99.75, 99.75, 99.75, 99.75, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.5, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.25, 99.75, 99.25, 99.5, 99.75, 99.5, 99.25, 99.25, 99.25, 99.75, 99.25, 99.25, 99.25, 99.25, 99.0, 99.25, 99.25, 99.0, 98.75, 98.5, 98.75, 98.5]]}, 'MarginSamplingSelection': {'10': [[82.25, 93.0, 96.25, 99.25, 100.0, 100.0, 100.0, 99.75, 99.75, 99.5, 100.0, 99.25, 99.5, 99.5, 100.0, 99.75, 100.0, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.75, 99.25, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.0, 99.25, 99.25, 99.0, 99.0, 99.0, 99.25, 99.5, 99.25, 99.5, 99.5, 99.25, 99.0, 98.75]]}, 'EntropySelection': {'10': [[89.75, 98.5, 99.75, 99.5, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.75, 99.5, 99.5, 99.25, 99.5, 99.5, 99.5, 99.5, 99.25, 99.0, 99.0, 99.25, 99.75, 99.75, 99.25, 99.75, 99.25, 99.5, 99.5, 99.5, 99.5, 99.75, 99.5, 99.5, 99.75, 99.75, 99.75]]}}}, 'HOG': {'SvmModel': {'RandomSelection': {'10': [[64.5, 89.5, 89.0, 95.5, 95.75, 96.75, 96.5, 98.25, 98.25, 98.75, 99.0, 99.25, 99.25, 99.5, 99.5, 99.5, 99.5, 99.25, 99.5, 99.0, 99.25, 99.25, 99.25, 99.25, 99.5, 99.75, 99.75, 99.75, 99.75, 99.5, 99.5, 99.25, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.25, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 98.75, 99.0, 98.75, 99.0]]}, 'MarginSamplingSelection': {'10': [[91.5, 82.5, 89.75, 88.5, 88.5, 82.0, 74.5, 77.25, 97.0, 98.75, 96.75, 98.25, 99.0, 98.25, 98.5, 99.0, 98.25, 98.5, 98.75, 99.0, 99.0, 99.25, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5]]}, 'EntropySelection': {'10': [[59.25, 82.0, 92.5, 95.75, 96.75, 94.0, 99.25, 99.0, 98.5, 98.5, 99.0, 98.75, 98.75, 99.0, 99.0, 99.0, 99.0, 99.0, 99.25, 99.25, 99.25, 99.25, 99.5, 99.5, 99.5, 99.25, 99.25, 99.25, 99.25, 99.25, 99.25, 99.25, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.5, 99.25, 99.25, 99.25]]}}, 'RfModel': {'RandomSelection': {'10': [[50.0, 68.25, 89.0, 92.0, 93.0, 92.0, 91.25, 91.0, 90.5, 90.25, 90.75, 90.5, 90.25, 89.75, 92.5, 91.5, 93.0, 90.75, 92.5, 92.25, 91.75, 93.0, 93.0, 92.75, 93.25, 92.25, 92.5, 92.25, 92.5, 93.25, 93.5, 93.5, 94.5, 94.25, 92.75, 94.5, 94.25, 94.25, 94.0, 94.0, 95.0, 94.5, 94.5, 94.5, 94.75, 94.25, 94.5, 95.0, 94.5, 94.5]]}, 'MarginSamplingSelection': {'10': [[68.25, 57.25, 85.5, 78.5, 98.0, 98.75, 93.25, 98.25, 95.5, 98.5, 97.75, 98.25, 97.25, 99.5, 97.5, 97.5, 98.75, 97.75, 99.25, 98.0, 98.75, 98.75, 98.0, 98.0, 97.75, 98.25, 98.75, 98.75, 99.25, 98.25, 99.0, 99.0, 98.0, 98.5, 98.5, 98.75, 97.75, 98.5, 97.75, 97.75, 98.0, 98.5, 97.5, 97.5, 97.5, 98.0, 98.0, 97.25, 97.75, 97.25]]}, 'EntropySelection': {'10': [[84.75, 63.0, 94.75, 87.25, 97.75, 98.0, 94.25, 98.5, 96.0, 98.0, 97.25, 98.25, 98.0, 97.0, 99.0, 97.25, 98.0, 98.25, 98.25, 98.75, 97.75, 98.0, 98.25, 98.75, 98.75, 98.0, 97.75, 99.0, 98.75, 98.25, 98.25, 98.5, 98.0, 97.75, 98.25, 98.5, 98.25, 98.25, 98.25, 98.5, 98.5, 98.5, 98.0, 98.5, 97.75, 97.25, 97.5, 97.75, 97.25, 97.75]]}}, 'LogModel': {'RandomSelection': {'10': []}}}}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane & car HOG LogModel: The area below this line is: 46426.25\n",
      "Airplane & car HOG LogModel: The area below this line is: 47458.75\n",
      "Airplane & car HOG LogModel: The area below this line is: 47413.75\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats, lowest_y):  \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'algorithm-upper-bound')\n",
    "    for choice_str in choices_str:\n",
    "        for model_object in models:\n",
    "          for selection_function in selection_functions:\n",
    "            for idx, k in enumerate(Ks):\n",
    "                x = np.arange(float(Ks[idx]), max_queried + float(Ks[idx]), float(Ks[idx]))            \n",
    "                filename =choice_str+'-'+ model_object + '-' +selection_function + '-' + \"10\"+'.npy'\n",
    "                mean = np.array(dic[choice_str][model_object][selection_function][k][0])\n",
    "                if(save_file==True):\n",
    "                    np.save(filename,mean)\n",
    "                ax.plot(x, mean ,label = model_object + '-' + selection_function + '-' + str(k)+'-'+choice_str)\n",
    "                areas(mean,choice_str,model_object)\n",
    "    ax.legend()\n",
    "    ax.set_xlim([0,500])\n",
    "    ax.set_ylim([0,101])\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "performance_plot(100, \n",
    "                 d,\n",
    "                 models_str,\n",
    "                 selection_functions_str , \n",
    "                 Ks_str, \n",
    "                 1,\n",
    "                 90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Active Learning Tutorial",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
